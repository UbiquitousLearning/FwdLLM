run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=0.001_layer=e.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=0.001_layer=e,0,1,2,3,4,5.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=0.001_layer=e,0,1,2,3,4,5,6,7,8,9,10,11.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=0.0001_layer=e.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=0.0001_layer=e,0,1,2,3,4,5.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=0.0001_layer=e,0,1,2,3,4,5,6,7,8,9,10,11.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=1e-05_layer=e.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=1e-05_layer=e,0,1,2,3,4,5.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled/agnews_forward_lr=1e-05_layer=e,0,1,2,3,4,5,6,7,8,9,10,11.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled_k=100_eval_stap_5/agnews_forward_lr=0.001_layer=e.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled_k=100_eval_stap_5/agnews_forward_lr=0.001_layer=e,0,1,2,3,4,5.log: Directory nonexistent
run_text_classification_sweep.sh: 6: run_text_classification_sweep.sh: cannot create ./log/scaled_k=100_eval_stap_5/agnews_forward_lr=0.001_layer=e,0,1,2,3,4,5,6,7,8,9,10,11.log: Directory nonexistent
36814 2023-02-16,23:36:08.528 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36932 2023-02-16,23:36:11.692 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37035 2023-02-16,23:36:14.663 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)36814 2023-02-16,23:36:17.305 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:17.370 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37152 2023-02-16,23:36:17.851 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37268 2023-02-16,23:36:20.367 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)36932 2023-02-16,23:36:20.464 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36932 2023-02-16,23:36:20.533 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37393 2023-02-16,23:36:23.105 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37035 2023-02-16,23:36:23.342 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37035 2023-02-16,23:36:23.415 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36814 2023-02-16,23:36:24.156 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
36814 2023-02-16,23:36:24.159 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
36814 2023-02-16,23:36:24.159 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e']
36814 2023-02-16,23:36:24.161 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 42530308}
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
36814 2023-02-16,23:36:26.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36814 2023-02-16,23:36:26.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36814 2023-02-16,23:36:26.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36814 2023-02-16,23:36:26.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:36:26.454 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:36:26.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37152 2023-02-16,23:36:26.568 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:26.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37152 2023-02-16,23:36:26.632 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:36:26.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:36:26.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:36:26.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:36:26.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:36:26.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37533 2023-02-16,23:36:26.872 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4,5', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36814 2023-02-16,23:36:26.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
36814 2023-02-16,23:36:26.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
36814 2023-02-16,23:36:26.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36814 2023-02-16,23:36:27.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36814 2023-02-16,23:36:27.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:36:27.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:36:27.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:36:27.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:36:27.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36814 2023-02-16,23:36:27.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
36814 2023-02-16,23:36:27.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
36814 2023-02-16,23:36:27.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36814 2023-02-16,23:36:27.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36814 2023-02-16,23:36:27.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36814 2023-02-16,23:36:27.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36814 2023-02-16,23:36:27.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:36:27.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:36:27.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:36:27.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:36:27.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
36932 2023-02-16,23:36:27.919 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
36932 2023-02-16,23:36:27.922 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
36932 2023-02-16,23:36:27.922 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0']
36932 2023-02-16,23:36:27.923 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 35442436}
36814 2023-02-16,23:36:27.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36814 2023-02-16,23:36:27.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:36:28.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36814 2023-02-16,23:36:28.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:36:28.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36814 2023-02-16,23:36:28.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:36:28.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
36814 2023-02-16,23:36:28.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36814 2023-02-16,23:36:28.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
36814 2023-02-16,23:36:28.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36814 2023-02-16,23:36:28.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36814 2023-02-16,23:36:28.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36814 2023-02-16,23:36:28.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
36814 2023-02-16,23:36:28.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
36814 2023-02-16,23:36:28.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
36814 2023-02-16,23:36:28.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
36814 2023-02-16,23:36:28.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36814 2023-02-16,23:36:28.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
36814 2023-02-16,23:36:28.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37268 2023-02-16,23:36:28.952 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:28.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37268 2023-02-16,23:36:29.021 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:36:29.026 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
36814 2023-02-16,23:36:29.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
36814 2023-02-16,23:36:29.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
36814 2023-02-16,23:36:29.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
37757 2023-02-16,23:36:29.210 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36814 2023-02-16,23:36:29.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
36814 2023-02-16,23:36:29.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
36814 2023-02-16,23:36:29.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
36814 2023-02-16,23:36:29.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36814 2023-02-16,23:36:29.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36814 2023-02-16,23:36:29.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36814 2023-02-16,23:36:29.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
36814 2023-02-16,23:36:29.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
36814 2023-02-16,23:36:29.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
36814 2023-02-16,23:36:29.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36814 2023-02-16,23:36:29.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
36814 2023-02-16,23:36:29.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
36814 2023-02-16,23:36:29.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
36932 2023-02-16,23:36:29.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36814 2023-02-16,23:36:30.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
36932 2023-02-16,23:36:30.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36814 2023-02-16,23:36:30.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
36932 2023-02-16,23:36:30.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36814 2023-02-16,23:36:30.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
36932 2023-02-16,23:36:30.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:36:30.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
36814 2023-02-16,23:36:30.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
36932 2023-02-16,23:36:30.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:36:30.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
36932 2023-02-16,23:36:30.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36814 2023-02-16,23:36:30.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36932 2023-02-16,23:36:30.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36814 2023-02-16,23:36:30.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
36932 2023-02-16,23:36:30.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:36:30.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
36814 2023-02-16,23:36:30.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36932 2023-02-16,23:36:30.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:36:30.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
36932 2023-02-16,23:36:30.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:36:30.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
36932 2023-02-16,23:36:30.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:36:30.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
37035 2023-02-16,23:36:30.733 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37035 2023-02-16,23:36:30.736 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37035 2023-02-16,23:36:30.736 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1']
37035 2023-02-16,23:36:30.738 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 28354564}
36932 2023-02-16,23:36:30.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
36814 2023-02-16,23:36:30.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
36814 2023-02-16,23:36:30.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36932 2023-02-16,23:36:30.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36814 2023-02-16,23:36:30.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
36932 2023-02-16,23:36:30.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
36814 2023-02-16,23:36:30.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36814 2023-02-16,23:36:30.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
36932 2023-02-16,23:36:30.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36814 2023-02-16,23:36:31.040 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
36932 2023-02-16,23:36:31.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36814 2023-02-16,23:36:31.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
36932 2023-02-16,23:36:31.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:36:31.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
36814 2023-02-16,23:36:31.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
36932 2023-02-16,23:36:31.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:36:31.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36932 2023-02-16,23:36:31.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:36:31.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37393 2023-02-16,23:36:31.316 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36932 2023-02-16,23:36:31.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:36:31.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
37393 2023-02-16,23:36:31.356 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:36:31.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
36932 2023-02-16,23:36:31.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36814 2023-02-16,23:36:31.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
36814 2023-02-16,23:36:31.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
36932 2023-02-16,23:36:31.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
36814 2023-02-16,23:36:31.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
36932 2023-02-16,23:36:31.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
36814 2023-02-16,23:36:31.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
36814 2023-02-16,23:36:31.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
36932 2023-02-16,23:36:31.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36814 2023-02-16,23:36:31.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
36932 2023-02-16,23:36:31.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36814 2023-02-16,23:36:31.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
36932 2023-02-16,23:36:31.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36814 2023-02-16,23:36:31.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
36814 2023-02-16,23:36:31.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
36932 2023-02-16,23:36:31.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36814 2023-02-16,23:36:31.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
36932 2023-02-16,23:36:31.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:36:31.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
36932 2023-02-16,23:36:31.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:36:31.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
36814 2023-02-16,23:36:32.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
36932 2023-02-16,23:36:32.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:36:32.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
36932 2023-02-16,23:36:32.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:36:32.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
36932 2023-02-16,23:36:32.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
36814 2023-02-16,23:36:32.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
36932 2023-02-16,23:36:32.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36932 2023-02-16,23:36:32.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:36:32.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
36932 2023-02-16,23:36:32.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36814 2023-02-16,23:36:32.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
36932 2023-02-16,23:36:32.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:36:32.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
36932 2023-02-16,23:36:32.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36814 2023-02-16,23:36:32.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
36932 2023-02-16,23:36:32.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:36:32.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37035 2023-02-16,23:36:32.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36814 2023-02-16,23:36:32.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
37035 2023-02-16,23:36:32.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36932 2023-02-16,23:36:32.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
36814 2023-02-16,23:36:32.782 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37035 2023-02-16,23:36:32.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36932 2023-02-16,23:36:32.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36814 2023-02-16,23:36:32.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37862 2023-02-16,23:36:32.846 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37035 2023-02-16,23:36:32.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:36:32.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
36932 2023-02-16,23:36:32.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37035 2023-02-16,23:36:32.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36932 2023-02-16,23:36:32.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36814 2023-02-16,23:36:32.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37035 2023-02-16,23:36:33.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36932 2023-02-16,23:36:33.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
37035 2023-02-16,23:36:33.067 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36814 2023-02-16,23:36:33.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
36932 2023-02-16,23:36:33.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37035 2023-02-16,23:36:33.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:36:33.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36814 2023-02-16,23:36:33.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
36932 2023-02-16,23:36:33.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37035 2023-02-16,23:36:33.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:36:33.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37035 2023-02-16,23:36:33.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36932 2023-02-16,23:36:33.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
36814 2023-02-16,23:36:33.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
36932 2023-02-16,23:36:33.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37035 2023-02-16,23:36:33.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:36:33.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
36932 2023-02-16,23:36:33.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37035 2023-02-16,23:36:33.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
36814 2023-02-16,23:36:33.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37035 2023-02-16,23:36:33.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
36932 2023-02-16,23:36:33.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36814 2023-02-16,23:36:33.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
36932 2023-02-16,23:36:33.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37035 2023-02-16,23:36:33.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
36814 2023-02-16,23:36:33.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37035 2023-02-16,23:36:33.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36932 2023-02-16,23:36:33.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36814 2023-02-16,23:36:33.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
36814 2023-02-16,23:36:33.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37035 2023-02-16,23:36:33.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36932 2023-02-16,23:36:33.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
36814 2023-02-16,23:36:33.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
36932 2023-02-16,23:36:33.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37035 2023-02-16,23:36:33.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:36:33.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
36932 2023-02-16,23:36:33.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37035 2023-02-16,23:36:33.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:36:33.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
36932 2023-02-16,23:36:33.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37035 2023-02-16,23:36:33.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:36:33.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
36932 2023-02-16,23:36:34.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
36814 2023-02-16,23:36:34.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
37035 2023-02-16,23:36:34.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:36:34.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37035 2023-02-16,23:36:34.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36932 2023-02-16,23:36:34.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
36814 2023-02-16,23:36:34.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37035 2023-02-16,23:36:34.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
36932 2023-02-16,23:36:34.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
36814 2023-02-16,23:36:34.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
36932 2023-02-16,23:36:34.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
37035 2023-02-16,23:36:34.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
36814 2023-02-16,23:36:34.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
36814 2023-02-16,23:36:34.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37035 2023-02-16,23:36:34.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36932 2023-02-16,23:36:34.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36814 2023-02-16,23:36:34.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37035 2023-02-16,23:36:34.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36932 2023-02-16,23:36:34.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36814 2023-02-16,23:36:34.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37035 2023-02-16,23:36:34.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36932 2023-02-16,23:36:34.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36814 2023-02-16,23:36:34.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
36814 2023-02-16,23:36:34.515 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37035 2023-02-16,23:36:34.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36932 2023-02-16,23:36:34.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37035 2023-02-16,23:36:34.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:36:34.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
36932 2023-02-16,23:36:34.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37152 2023-02-16,23:36:34.648 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37152 2023-02-16,23:36:34.650 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37152 2023-02-16,23:36:34.650 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2']
37152 2023-02-16,23:36:34.652 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 21266692}
37035 2023-02-16,23:36:34.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:36:34.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
36932 2023-02-16,23:36:34.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37035 2023-02-16,23:36:34.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36932 2023-02-16,23:36:34.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
37035 2023-02-16,23:36:34.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:36:34.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
36932 2023-02-16,23:36:34.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37035 2023-02-16,23:36:34.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
36814 2023-02-16,23:36:34.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37035 2023-02-16,23:36:34.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36932 2023-02-16,23:36:34.902 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37035 2023-02-16,23:36:34.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:36:34.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37035 2023-02-16,23:36:34.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36932 2023-02-16,23:36:35.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37035 2023-02-16,23:36:35.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:36:35.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37035 2023-02-16,23:36:35.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36932 2023-02-16,23:36:35.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
36814 2023-02-16,23:36:35.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37035 2023-02-16,23:36:35.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36932 2023-02-16,23:36:35.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
36814 2023-02-16,23:36:35.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37035 2023-02-16,23:36:35.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
36814 2023-02-16,23:36:35.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
36932 2023-02-16,23:36:35.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
37035 2023-02-16,23:36:35.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36814 2023-02-16,23:36:35.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37035 2023-02-16,23:36:35.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
36814 2023-02-16,23:36:35.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
36932 2023-02-16,23:36:35.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37983 2023-02-16,23:36:35.374 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37035 2023-02-16,23:36:35.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36814 2023-02-16,23:36:35.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37035 2023-02-16,23:36:35.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36814 2023-02-16,23:36:35.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
36932 2023-02-16,23:36:35.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
37035 2023-02-16,23:36:35.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36814 2023-02-16,23:36:35.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37035 2023-02-16,23:36:35.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
36932 2023-02-16,23:36:35.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37533 2023-02-16,23:36:35.583 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:35.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37035 2023-02-16,23:36:35.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37533 2023-02-16,23:36:35.644 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36932 2023-02-16,23:36:35.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36814 2023-02-16,23:36:35.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37035 2023-02-16,23:36:35.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
36814 2023-02-16,23:36:35.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37035 2023-02-16,23:36:35.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
36932 2023-02-16,23:36:35.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
36814 2023-02-16,23:36:35.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37035 2023-02-16,23:36:35.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36932 2023-02-16,23:36:35.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
36814 2023-02-16,23:36:35.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37035 2023-02-16,23:36:35.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
36932 2023-02-16,23:36:35.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37035 2023-02-16,23:36:35.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36814 2023-02-16,23:36:35.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
36932 2023-02-16,23:36:35.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
37035 2023-02-16,23:36:35.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
36814 2023-02-16,23:36:35.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
36932 2023-02-16,23:36:35.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
37035 2023-02-16,23:36:36.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
36814 2023-02-16,23:36:36.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
36932 2023-02-16,23:36:36.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
36814 2023-02-16,23:36:36.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
36932 2023-02-16,23:36:36.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37035 2023-02-16,23:36:36.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
36814 2023-02-16,23:36:36.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37035 2023-02-16,23:36:36.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
36932 2023-02-16,23:36:36.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36814 2023-02-16,23:36:36.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
36932 2023-02-16,23:36:36.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
37035 2023-02-16,23:36:36.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
36814 2023-02-16,23:36:36.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
36932 2023-02-16,23:36:36.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36814 2023-02-16,23:36:36.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
36932 2023-02-16,23:36:36.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
37035 2023-02-16,23:36:36.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
36814 2023-02-16,23:36:36.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
36932 2023-02-16,23:36:36.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
37035 2023-02-16,23:36:36.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37035 2023-02-16,23:36:36.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
36932 2023-02-16,23:36:36.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
36814 2023-02-16,23:36:36.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
36814 2023-02-16,23:36:36.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
36932 2023-02-16,23:36:36.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
37035 2023-02-16,23:36:36.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36814 2023-02-16,23:36:36.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
36932 2023-02-16,23:36:36.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37035 2023-02-16,23:36:36.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36932 2023-02-16,23:36:36.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36814 2023-02-16,23:36:36.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37035 2023-02-16,23:36:36.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36932 2023-02-16,23:36:36.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
36814 2023-02-16,23:36:36.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37035 2023-02-16,23:36:36.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
36932 2023-02-16,23:36:36.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
36814 2023-02-16,23:36:36.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36932 2023-02-16,23:36:36.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37035 2023-02-16,23:36:36.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
36814 2023-02-16,23:36:36.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37035 2023-02-16,23:36:36.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
36932 2023-02-16,23:36:37.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
36814 2023-02-16,23:36:37.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37035 2023-02-16,23:36:37.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36814 2023-02-16,23:36:37.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
36932 2023-02-16,23:36:37.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
36814 2023-02-16,23:36:37.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37035 2023-02-16,23:36:37.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
36932 2023-02-16,23:36:37.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
36814 2023-02-16,23:36:37.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37035 2023-02-16,23:36:37.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
36814 2023-02-16,23:36:37.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
36932 2023-02-16,23:36:37.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
36814 2023-02-16,23:36:37.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
36932 2023-02-16,23:36:37.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
37035 2023-02-16,23:36:37.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
36814 2023-02-16,23:36:37.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
36932 2023-02-16,23:36:37.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
37035 2023-02-16,23:36:37.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
36814 2023-02-16,23:36:37.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
36932 2023-02-16,23:36:37.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
36814 2023-02-16,23:36:37.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
36932 2023-02-16,23:36:37.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
37035 2023-02-16,23:36:37.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
36814 2023-02-16,23:36:37.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
36932 2023-02-16,23:36:37.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
37035 2023-02-16,23:36:37.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
36814 2023-02-16,23:36:37.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
36932 2023-02-16,23:36:37.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
37035 2023-02-16,23:36:37.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
36814 2023-02-16,23:36:37.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
36932 2023-02-16,23:36:37.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
36814 2023-02-16,23:36:37.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
36932 2023-02-16,23:36:37.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37035 2023-02-16,23:36:37.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
36932 2023-02-16,23:36:37.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
36814 2023-02-16,23:36:37.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
37035 2023-02-16,23:36:37.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
36814 2023-02-16,23:36:37.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
36932 2023-02-16,23:36:37.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37035 2023-02-16,23:36:37.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36814 2023-02-16,23:36:37.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
36932 2023-02-16,23:36:37.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37268 2023-02-16,23:36:37.898 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37268 2023-02-16,23:36:37.900 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37268 2023-02-16,23:36:37.900 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3']
37268 2023-02-16,23:36:37.901 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 14178820}
37035 2023-02-16,23:36:37.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
36932 2023-02-16,23:36:37.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37757 2023-02-16,23:36:37.964 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:37.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
36932 2023-02-16,23:36:38.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37035 2023-02-16,23:36:38.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37757 2023-02-16,23:36:38.046 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:36:38.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
36932 2023-02-16,23:36:38.058 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37035 2023-02-16,23:36:38.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36932 2023-02-16,23:36:38.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
36814 2023-02-16,23:36:38.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37035 2023-02-16,23:36:38.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
36932 2023-02-16,23:36:38.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
36814 2023-02-16,23:36:38.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3944381475448608
37035 2023-02-16,23:36:38.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
36932 2023-02-16,23:36:38.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
36814 2023-02-16,23:36:38.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4107284545898438
37035 2023-02-16,23:36:38.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37152 2023-02-16,23:36:38.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36932 2023-02-16,23:36:38.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
36814 2023-02-16,23:36:38.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.422221064567566
37035 2023-02-16,23:36:38.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
36932 2023-02-16,23:36:38.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37152 2023-02-16,23:36:38.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36814 2023-02-16,23:36:38.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3748679161071777
37035 2023-02-16,23:36:38.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36932 2023-02-16,23:36:38.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37152 2023-02-16,23:36:38.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36814 2023-02-16,23:36:38.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.370774745941162
37035 2023-02-16,23:36:38.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
36932 2023-02-16,23:36:38.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37152 2023-02-16,23:36:38.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:36:38.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.398972988128662
37035 2023-02-16,23:36:38.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36932 2023-02-16,23:36:38.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37035 2023-02-16,23:36:38.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
36814 2023-02-16,23:36:38.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.39992356300354
37152 2023-02-16,23:36:38.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36814 2023-02-16,23:36:38.783 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3837345838546753
36932 2023-02-16,23:36:38.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37152 2023-02-16,23:36:38.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37035 2023-02-16,23:36:38.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38108 2023-02-16,23:36:38.834 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36814 2023-02-16,23:36:38.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3782150745391846
37152 2023-02-16,23:36:38.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36932 2023-02-16,23:36:38.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37035 2023-02-16,23:36:38.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
36932 2023-02-16,23:36:38.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37152 2023-02-16,23:36:38.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
37035 2023-02-16,23:36:38.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
36814 2023-02-16,23:36:38.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4403983354568481
37035 2023-02-16,23:36:39.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37152 2023-02-16,23:36:39.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36932 2023-02-16,23:36:39.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
36814 2023-02-16,23:36:39.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3536986112594604
36932 2023-02-16,23:36:39.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37035 2023-02-16,23:36:39.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36814 2023-02-16,23:36:39.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4236855506896973
37152 2023-02-16,23:36:39.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:36:39.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3924638032913208
36932 2023-02-16,23:36:39.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37035 2023-02-16,23:36:39.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
37152 2023-02-16,23:36:39.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:36:39.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4004521369934082
36932 2023-02-16,23:36:39.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37035 2023-02-16,23:36:39.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
37152 2023-02-16,23:36:39.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37035 2023-02-16,23:36:39.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37152 2023-02-16,23:36:39.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
36814 2023-02-16,23:36:39.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4167895317077637
36932 2023-02-16,23:36:39.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
36814 2023-02-16,23:36:39.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3963053226470947
36932 2023-02-16,23:36:39.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37035 2023-02-16,23:36:39.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37152 2023-02-16,23:36:39.496 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37035 2023-02-16,23:36:39.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
36814 2023-02-16,23:36:39.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3381288051605225
37152 2023-02-16,23:36:39.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36932 2023-02-16,23:36:39.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
36814 2023-02-16,23:36:39.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3634902238845825
37035 2023-02-16,23:36:39.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37152 2023-02-16,23:36:39.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36932 2023-02-16,23:36:39.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37035 2023-02-16,23:36:39.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
37152 2023-02-16,23:36:39.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:36:39.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3456318378448486
36932 2023-02-16,23:36:39.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37035 2023-02-16,23:36:39.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
37152 2023-02-16,23:36:39.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:36:39.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3503947257995605
36932 2023-02-16,23:36:39.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37035 2023-02-16,23:36:39.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
37152 2023-02-16,23:36:39.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:36:39.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3879326581954956
36932 2023-02-16,23:36:39.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
37035 2023-02-16,23:36:39.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
36814 2023-02-16,23:36:40.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3448486328125
37152 2023-02-16,23:36:40.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36932 2023-02-16,23:36:40.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
37035 2023-02-16,23:36:40.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
36814 2023-02-16,23:36:40.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.368371844291687
36932 2023-02-16,23:36:40.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
37393 2023-02-16,23:36:40.097 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37152 2023-02-16,23:36:40.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
37393 2023-02-16,23:36:40.100 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37393 2023-02-16,23:36:40.100 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3', '4']
37393 2023-02-16,23:36:40.102 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 7090948}
37035 2023-02-16,23:36:40.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
36814 2023-02-16,23:36:40.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.412251591682434
36932 2023-02-16,23:36:40.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37152 2023-02-16,23:36:40.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37035 2023-02-16,23:36:40.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
36932 2023-02-16,23:36:40.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37152 2023-02-16,23:36:40.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
36814 2023-02-16,23:36:40.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3543955087661743
37035 2023-02-16,23:36:40.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
37152 2023-02-16,23:36:40.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36932 2023-02-16,23:36:40.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37035 2023-02-16,23:36:40.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
36814 2023-02-16,23:36:40.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3592514991760254
37035 2023-02-16,23:36:40.439 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37152 2023-02-16,23:36:40.440 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36814 2023-02-16,23:36:40.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3841006755828857
36932 2023-02-16,23:36:40.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37035 2023-02-16,23:36:40.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
36814 2023-02-16,23:36:40.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.35696280002594
37152 2023-02-16,23:36:40.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36932 2023-02-16,23:36:40.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37035 2023-02-16,23:36:40.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
36814 2023-02-16,23:36:40.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3670949935913086
37152 2023-02-16,23:36:40.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36932 2023-02-16,23:36:40.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37035 2023-02-16,23:36:40.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
37152 2023-02-16,23:36:40.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36932 2023-02-16,23:36:40.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
36814 2023-02-16,23:36:40.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3612109422683716
37035 2023-02-16,23:36:40.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37152 2023-02-16,23:36:40.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:36:40.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4194444417953491
36932 2023-02-16,23:36:40.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37035 2023-02-16,23:36:40.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37152 2023-02-16,23:36:40.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36932 2023-02-16,23:36:40.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
36814 2023-02-16,23:36:40.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.431254506111145
37035 2023-02-16,23:36:40.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37152 2023-02-16,23:36:40.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:36:40.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3950846195220947
36932 2023-02-16,23:36:40.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37035 2023-02-16,23:36:41.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37152 2023-02-16,23:36:41.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
36814 2023-02-16,23:36:41.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3708620071411133
36932 2023-02-16,23:36:41.035 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37035 2023-02-16,23:36:41.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
37152 2023-02-16,23:36:41.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36932 2023-02-16,23:36:41.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
36814 2023-02-16,23:36:41.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4281599521636963
37035 2023-02-16,23:36:41.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
37152 2023-02-16,23:36:41.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36932 2023-02-16,23:36:41.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
36814 2023-02-16,23:36:41.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4242079257965088
37035 2023-02-16,23:36:41.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37152 2023-02-16,23:36:41.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36932 2023-02-16,23:36:41.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
36814 2023-02-16,23:36:41.279 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.395251750946045
37035 2023-02-16,23:36:41.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37152 2023-02-16,23:36:41.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:36:41.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.411716103553772
36932 2023-02-16,23:36:41.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37152 2023-02-16,23:36:41.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
37035 2023-02-16,23:36:41.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
36814 2023-02-16,23:36:41.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3711867332458496
36932 2023-02-16,23:36:41.452 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37035 2023-02-16,23:36:41.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37152 2023-02-16,23:36:41.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:36:41.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3754464387893677
36932 2023-02-16,23:36:41.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37152 2023-02-16,23:36:41.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37035 2023-02-16,23:36:41.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
36814 2023-02-16,23:36:41.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.389987826347351
36932 2023-02-16,23:36:41.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37035 2023-02-16,23:36:41.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37152 2023-02-16,23:36:41.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37862 2023-02-16,23:36:41.661 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:41.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3862546682357788
37035 2023-02-16,23:36:41.712 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37152 2023-02-16,23:36:41.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
36932 2023-02-16,23:36:41.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37862 2023-02-16,23:36:41.720 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37035 2023-02-16,23:36:41.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37152 2023-02-16,23:36:41.795 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36814 2023-02-16,23:36:41.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4014194011688232
36932 2023-02-16,23:36:41.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
38255 2023-02-16,23:36:41.801 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37035 2023-02-16,23:36:41.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37152 2023-02-16,23:36:41.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36814 2023-02-16,23:36:41.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4071680307388306
36932 2023-02-16,23:36:41.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37035 2023-02-16,23:36:41.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37152 2023-02-16,23:36:41.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36814 2023-02-16,23:36:41.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3979439735412598
36932 2023-02-16,23:36:41.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37152 2023-02-16,23:36:42.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37035 2023-02-16,23:36:42.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
36932 2023-02-16,23:36:42.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
36814 2023-02-16,23:36:42.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4266529083251953
37152 2023-02-16,23:36:42.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37035 2023-02-16,23:36:42.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
36814 2023-02-16,23:36:42.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.438494324684143
36932 2023-02-16,23:36:42.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37152 2023-02-16,23:36:42.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37035 2023-02-16,23:36:42.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
36814 2023-02-16,23:36:42.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4179333448410034
36932 2023-02-16,23:36:42.219 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37152 2023-02-16,23:36:42.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37035 2023-02-16,23:36:42.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
36814 2023-02-16,23:36:42.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4268027544021606
36932 2023-02-16,23:36:42.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37152 2023-02-16,23:36:42.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37035 2023-02-16,23:36:42.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
36814 2023-02-16,23:36:42.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3947529792785645
36932 2023-02-16,23:36:42.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37035 2023-02-16,23:36:42.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37152 2023-02-16,23:36:42.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
36932 2023-02-16,23:36:42.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
36814 2023-02-16,23:36:42.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.402390480041504
37035 2023-02-16,23:36:42.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
36932 2023-02-16,23:36:42.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
36814 2023-02-16,23:36:42.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4418705701828003
37152 2023-02-16,23:36:42.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
37035 2023-02-16,23:36:42.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
36814 2023-02-16,23:36:42.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3776497840881348
36932 2023-02-16,23:36:42.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37152 2023-02-16,23:36:42.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37268 2023-02-16,23:36:42.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37035 2023-02-16,23:36:42.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
36814 2023-02-16,23:36:42.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4071439504623413
36932 2023-02-16,23:36:42.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37152 2023-02-16,23:36:42.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37035 2023-02-16,23:36:42.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
36932 2023-02-16,23:36:42.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
36814 2023-02-16,23:36:42.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3665921688079834
37152 2023-02-16,23:36:42.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37268 2023-02-16,23:36:42.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37035 2023-02-16,23:36:42.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
36932 2023-02-16,23:36:42.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
36814 2023-02-16,23:36:42.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.36996328830719
37152 2023-02-16,23:36:42.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37268 2023-02-16,23:36:42.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37035 2023-02-16,23:36:43.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37152 2023-02-16,23:36:43.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
36814 2023-02-16,23:36:43.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3973619937896729
36932 2023-02-16,23:36:43.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
37268 2023-02-16,23:36:43.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37035 2023-02-16,23:36:43.101 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37152 2023-02-16,23:36:43.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
36814 2023-02-16,23:36:43.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3801718950271606
36932 2023-02-16,23:36:43.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
37035 2023-02-16,23:36:43.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
36814 2023-02-16,23:36:43.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3659502267837524
37152 2023-02-16,23:36:43.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37268 2023-02-16,23:36:43.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36932 2023-02-16,23:36:43.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
37035 2023-02-16,23:36:43.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37152 2023-02-16,23:36:43.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
36814 2023-02-16,23:36:43.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3579378128051758
37268 2023-02-16,23:36:43.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36932 2023-02-16,23:36:43.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37152 2023-02-16,23:36:43.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36814 2023-02-16,23:36:43.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3542557954788208
37035 2023-02-16,23:36:43.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37268 2023-02-16,23:36:43.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36932 2023-02-16,23:36:43.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
37152 2023-02-16,23:36:43.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36814 2023-02-16,23:36:43.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4305098056793213
37035 2023-02-16,23:36:43.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
36932 2023-02-16,23:36:43.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
37268 2023-02-16,23:36:43.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37983 2023-02-16,23:36:43.540 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37152 2023-02-16,23:36:43.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36814 2023-02-16,23:36:43.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4318424463272095
37983 2023-02-16,23:36:43.601 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37035 2023-02-16,23:36:43.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
36932 2023-02-16,23:36:43.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37268 2023-02-16,23:36:43.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:36:43.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3837517499923706
37035 2023-02-16,23:36:43.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37152 2023-02-16,23:36:43.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37268 2023-02-16,23:36:43.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36932 2023-02-16,23:36:43.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
36814 2023-02-16,23:36:43.799 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3724877834320068
37035 2023-02-16,23:36:43.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37152 2023-02-16,23:36:43.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37268 2023-02-16,23:36:43.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36932 2023-02-16,23:36:43.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
36814 2023-02-16,23:36:43.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.3986988067626953
37035 2023-02-16,23:36:43.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37152 2023-02-16,23:36:43.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37268 2023-02-16,23:36:43.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
36932 2023-02-16,23:36:43.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
36814 2023-02-16,23:36:43.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4023280143737793
37035 2023-02-16,23:36:43.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37152 2023-02-16,23:36:44.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
37268 2023-02-16,23:36:44.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
36932 2023-02-16,23:36:44.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
36814 2023-02-16,23:36:44.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3811001777648926
37035 2023-02-16,23:36:44.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37152 2023-02-16,23:36:44.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37268 2023-02-16,23:36:44.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
36932 2023-02-16,23:36:44.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
36814 2023-02-16,23:36:44.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3922683000564575
37035 2023-02-16,23:36:44.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37152 2023-02-16,23:36:44.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37268 2023-02-16,23:36:44.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36932 2023-02-16,23:36:44.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
37035 2023-02-16,23:36:44.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
36814 2023-02-16,23:36:44.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3976486921310425
37152 2023-02-16,23:36:44.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37268 2023-02-16,23:36:44.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36932 2023-02-16,23:36:44.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37035 2023-02-16,23:36:44.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
36814 2023-02-16,23:36:44.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4327387809753418
37035 2023-02-16,23:36:44.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37152 2023-02-16,23:36:44.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
36814 2023-02-16,23:36:44.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4009599685668945
36932 2023-02-16,23:36:44.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37268 2023-02-16,23:36:44.452 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37035 2023-02-16,23:36:44.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
36814 2023-02-16,23:36:44.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3849284648895264
36932 2023-02-16,23:36:44.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
37152 2023-02-16,23:36:44.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
37268 2023-02-16,23:36:44.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
38362 2023-02-16,23:36:44.563 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36814 2023-02-16,23:36:44.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4448908567428589
36932 2023-02-16,23:36:44.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37152 2023-02-16,23:36:44.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
37268 2023-02-16,23:36:44.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
37035 2023-02-16,23:36:44.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37035 2023-02-16,23:36:44.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
36814 2023-02-16,23:36:44.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4099314212799072
36932 2023-02-16,23:36:44.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37152 2023-02-16,23:36:44.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37268 2023-02-16,23:36:44.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:36:44.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4305857419967651
37035 2023-02-16,23:36:44.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37152 2023-02-16,23:36:44.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
37268 2023-02-16,23:36:44.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36932 2023-02-16,23:36:44.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
36814 2023-02-16,23:36:44.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.397027850151062
37035 2023-02-16,23:36:44.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
36932 2023-02-16,23:36:44.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37152 2023-02-16,23:36:44.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
37268 2023-02-16,23:36:44.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
36814 2023-02-16,23:36:45.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4524929523468018
37035 2023-02-16,23:36:45.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
36932 2023-02-16,23:36:45.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37152 2023-02-16,23:36:45.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
37268 2023-02-16,23:36:45.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37035 2023-02-16,23:36:45.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
36814 2023-02-16,23:36:45.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4376956224441528
36932 2023-02-16,23:36:45.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37152 2023-02-16,23:36:45.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
37268 2023-02-16,23:36:45.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
37035 2023-02-16,23:36:45.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
36814 2023-02-16,23:36:45.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4008244276046753
36932 2023-02-16,23:36:45.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
37152 2023-02-16,23:36:45.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37268 2023-02-16,23:36:45.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37035 2023-02-16,23:36:45.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
36814 2023-02-16,23:36:45.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3915315866470337
36932 2023-02-16,23:36:45.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37152 2023-02-16,23:36:45.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37268 2023-02-16,23:36:45.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36814 2023-02-16,23:36:45.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4074761867523193
37035 2023-02-16,23:36:45.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37268 2023-02-16,23:36:45.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37152 2023-02-16,23:36:45.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
36932 2023-02-16,23:36:45.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
36814 2023-02-16,23:36:45.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4124164581298828
37035 2023-02-16,23:36:45.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37152 2023-02-16,23:36:45.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
36932 2023-02-16,23:36:45.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37268 2023-02-16,23:36:45.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:36:45.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3759716749191284
37035 2023-02-16,23:36:45.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37152 2023-02-16,23:36:45.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
36932 2023-02-16,23:36:45.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37393 2023-02-16,23:36:45.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37268 2023-02-16,23:36:45.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:36:45.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890239000320435
37035 2023-02-16,23:36:45.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37152 2023-02-16,23:36:45.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
36932 2023-02-16,23:36:45.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37268 2023-02-16,23:36:45.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:36:45.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3732397556304932
37035 2023-02-16,23:36:45.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37152 2023-02-16,23:36:45.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36932 2023-02-16,23:36:45.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37393 2023-02-16,23:36:45.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36814 2023-02-16,23:36:45.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.394612431526184
37035 2023-02-16,23:36:45.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37268 2023-02-16,23:36:45.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
37152 2023-02-16,23:36:45.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
37393 2023-02-16,23:36:45.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36932 2023-02-16,23:36:46.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37268 2023-02-16,23:36:46.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37393 2023-02-16,23:36:46.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:36:46.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4033585786819458
37035 2023-02-16,23:36:46.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37152 2023-02-16,23:36:46.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36932 2023-02-16,23:36:46.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3942523002624512
37035 2023-02-16,23:36:46.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
37268 2023-02-16,23:36:46.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37393 2023-02-16,23:36:46.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:36:46.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3726000785827637
36932 2023-02-16,23:36:46.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4103646278381348
37152 2023-02-16,23:36:46.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
37035 2023-02-16,23:36:46.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
37268 2023-02-16,23:36:46.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
37393 2023-02-16,23:36:46.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36814 2023-02-16,23:36:46.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3951376676559448
36932 2023-02-16,23:36:46.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4222036600112915
37152 2023-02-16,23:36:46.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
37035 2023-02-16,23:36:46.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
37268 2023-02-16,23:36:46.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
37393 2023-02-16,23:36:46.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36814 2023-02-16,23:36:46.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3602800369262695
36932 2023-02-16,23:36:46.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3743112087249756
37152 2023-02-16,23:36:46.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37035 2023-02-16,23:36:46.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37268 2023-02-16,23:36:46.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:36:46.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.386864185333252
37152 2023-02-16,23:36:46.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
37393 2023-02-16,23:36:46.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36932 2023-02-16,23:36:46.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3705263137817383
37268 2023-02-16,23:36:46.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
37393 2023-02-16,23:36:46.712 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37035 2023-02-16,23:36:46.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
36814 2023-02-16,23:36:46.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.417733073234558
36932 2023-02-16,23:36:46.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3986607789993286
37152 2023-02-16,23:36:46.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37268 2023-02-16,23:36:46.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
37393 2023-02-16,23:36:46.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
37035 2023-02-16,23:36:46.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
37152 2023-02-16,23:36:46.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36814 2023-02-16,23:36:46.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4184141159057617
36932 2023-02-16,23:36:46.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4015400409698486
37393 2023-02-16,23:36:46.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
37268 2023-02-16,23:36:46.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37152 2023-02-16,23:36:46.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
36814 2023-02-16,23:36:46.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3540626764297485
36932 2023-02-16,23:36:46.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.385284662246704
37035 2023-02-16,23:36:46.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37393 2023-02-16,23:36:47.057 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37035 2023-02-16,23:36:47.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37268 2023-02-16,23:36:47.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37152 2023-02-16,23:36:47.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
36814 2023-02-16,23:36:47.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4240602254867554
36932 2023-02-16,23:36:47.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3782092332839966
37393 2023-02-16,23:36:47.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37035 2023-02-16,23:36:47.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37268 2023-02-16,23:36:47.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37152 2023-02-16,23:36:47.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
36932 2023-02-16,23:36:47.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4404478073120117
36814 2023-02-16,23:36:47.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3887529373168945
37393 2023-02-16,23:36:47.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37533 2023-02-16,23:36:47.285 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37533 2023-02-16,23:36:47.288 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37533 2023-02-16,23:36:47.288 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3', '4', '5']
37533 2023-02-16,23:36:47.291 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 3076}
37035 2023-02-16,23:36:47.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
37268 2023-02-16,23:36:47.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
37152 2023-02-16,23:36:47.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
36932 2023-02-16,23:36:47.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.357358455657959
36814 2023-02-16,23:36:47.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4011168479919434
37393 2023-02-16,23:36:47.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
37268 2023-02-16,23:36:47.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
37035 2023-02-16,23:36:47.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37152 2023-02-16,23:36:47.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
36814 2023-02-16,23:36:47.445 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4054780006408691
36932 2023-02-16,23:36:47.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4225389957427979
37393 2023-02-16,23:36:47.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)38108 2023-02-16,23:36:47.502 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37268 2023-02-16,23:36:47.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37035 2023-02-16,23:36:47.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37152 2023-02-16,23:36:47.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
36814 2023-02-16,23:36:47.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.431037425994873
36932 2023-02-16,23:36:47.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3918217420578003
38108 2023-02-16,23:36:47.561 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37393 2023-02-16,23:36:47.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37035 2023-02-16,23:36:47.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
37268 2023-02-16,23:36:47.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37152 2023-02-16,23:36:47.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
36814 2023-02-16,23:36:47.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4244861602783203
36932 2023-02-16,23:36:47.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4013596773147583
37393 2023-02-16,23:36:47.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
37268 2023-02-16,23:36:47.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37035 2023-02-16,23:36:47.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
36932 2023-02-16,23:36:47.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4163265228271484
37152 2023-02-16,23:36:47.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
36814 2023-02-16,23:36:47.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3689879179000854
37393 2023-02-16,23:36:47.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
37268 2023-02-16,23:36:47.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37035 2023-02-16,23:36:47.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37152 2023-02-16,23:36:47.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
36932 2023-02-16,23:36:47.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3957911729812622
37393 2023-02-16,23:36:47.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:36:47.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3909475803375244
37268 2023-02-16,23:36:47.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37035 2023-02-16,23:36:48.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
37152 2023-02-16,23:36:48.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
37393 2023-02-16,23:36:48.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36932 2023-02-16,23:36:48.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372193574905396
36814 2023-02-16,23:36:48.033 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.38657808303833
37268 2023-02-16,23:36:48.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37035 2023-02-16,23:36:48.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37393 2023-02-16,23:36:48.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37152 2023-02-16,23:36:48.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
36814 2023-02-16,23:36:48.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3511176109313965
36932 2023-02-16,23:36:48.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3631880283355713
38487 2023-02-16,23:36:48.167 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4,5', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37268 2023-02-16,23:36:48.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37393 2023-02-16,23:36:48.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37035 2023-02-16,23:36:48.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37152 2023-02-16,23:36:48.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
36814 2023-02-16,23:36:48.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4109584093093872
36932 2023-02-16,23:36:48.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3464683294296265
37035 2023-02-16,23:36:48.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
37393 2023-02-16,23:36:48.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
37152 2023-02-16,23:36:48.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
37268 2023-02-16,23:36:48.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36932 2023-02-16,23:36:48.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3503196239471436
36814 2023-02-16,23:36:48.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4382686614990234
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37393 2023-02-16,23:36:48.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37035 2023-02-16,23:36:48.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37152 2023-02-16,23:36:48.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
37268 2023-02-16,23:36:48.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
36932 2023-02-16,23:36:48.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3892518281936646
36814 2023-02-16,23:36:48.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3879222869873047
37035 2023-02-16,23:36:48.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37152 2023-02-16,23:36:48.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37393 2023-02-16,23:36:48.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36814 2023-02-16,23:36:48.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3512755632400513
36932 2023-02-16,23:36:48.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3447816371917725
37268 2023-02-16,23:36:48.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37035 2023-02-16,23:36:48.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37152 2023-02-16,23:36:48.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37268 2023-02-16,23:36:48.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37393 2023-02-16,23:36:48.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36814 2023-02-16,23:36:48.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4120330810546875
36932 2023-02-16,23:36:48.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3682026863098145
37035 2023-02-16,23:36:48.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
36814 2023-02-16,23:36:48.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.399109959602356
37152 2023-02-16,23:36:48.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37268 2023-02-16,23:36:48.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37393 2023-02-16,23:36:48.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36932 2023-02-16,23:36:48.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4131890535354614
37035 2023-02-16,23:36:48.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37268 2023-02-16,23:36:48.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
36814 2023-02-16,23:36:48.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4089319705963135
37152 2023-02-16,23:36:48.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37393 2023-02-16,23:36:48.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36932 2023-02-16,23:36:48.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3550167083740234
37035 2023-02-16,23:36:49.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
37152 2023-02-16,23:36:49.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
37268 2023-02-16,23:36:49.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37393 2023-02-16,23:36:49.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:36:49.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4103707075119019
36932 2023-02-16,23:36:49.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.359007716178894
37035 2023-02-16,23:36:49.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37152 2023-02-16,23:36:49.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37268 2023-02-16,23:36:49.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37393 2023-02-16,23:36:49.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:36:49.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3440511226654053
36932 2023-02-16,23:36:49.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.384819746017456
37035 2023-02-16,23:36:49.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
37152 2023-02-16,23:36:49.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37268 2023-02-16,23:36:49.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
36814 2023-02-16,23:36:49.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3778773546218872
36932 2023-02-16,23:36:49.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3570998907089233
37393 2023-02-16,23:36:49.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37035 2023-02-16,23:36:49.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37152 2023-02-16,23:36:49.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
36814 2023-02-16,23:36:49.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3727606534957886
37268 2023-02-16,23:36:49.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
37393 2023-02-16,23:36:49.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36932 2023-02-16,23:36:49.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3684139251708984
37757 2023-02-16,23:36:49.494 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37757 2023-02-16,23:36:49.496 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37757 2023-02-16,23:36:49.496 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e']
37757 2023-02-16,23:36:49.498 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 42530308}
37152 2023-02-16,23:36:49.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37035 2023-02-16,23:36:49.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37268 2023-02-16,23:36:49.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
37393 2023-02-16,23:36:49.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:36:49.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3463934659957886
36932 2023-02-16,23:36:49.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3602778911590576
37152 2023-02-16,23:36:49.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
36814 2023-02-16,23:36:49.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3987101316452026
37268 2023-02-16,23:36:49.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
37393 2023-02-16,23:36:49.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36932 2023-02-16,23:36:49.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.419083833694458
37035 2023-02-16,23:36:49.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37152 2023-02-16,23:36:49.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
36814 2023-02-16,23:36:49.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3875027894973755
36932 2023-02-16,23:36:49.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.431036353111267
37268 2023-02-16,23:36:49.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37393 2023-02-16,23:36:49.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
37035 2023-02-16,23:36:49.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.394166350364685
37152 2023-02-16,23:36:49.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37035 2023-02-16,23:36:49.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4110368490219116
37268 2023-02-16,23:36:49.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37393 2023-02-16,23:36:49.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36814 2023-02-16,23:36:49.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3498224020004272
36932 2023-02-16,23:36:49.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3963741064071655
37152 2023-02-16,23:36:50.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37035 2023-02-16,23:36:50.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.422668695449829
36814 2023-02-16,23:36:50.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3595073223114014
37268 2023-02-16,23:36:50.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37393 2023-02-16,23:36:50.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36932 2023-02-16,23:36:50.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3712139129638672
37152 2023-02-16,23:36:50.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37035 2023-02-16,23:36:50.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3734912872314453
37393 2023-02-16,23:36:50.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
36814 2023-02-16,23:36:50.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3676021099090576
37268 2023-02-16,23:36:50.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36932 2023-02-16,23:36:50.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4282734394073486
37152 2023-02-16,23:36:50.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37035 2023-02-16,23:36:50.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3700413703918457
36814 2023-02-16,23:36:50.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.3998812437057495
36932 2023-02-16,23:36:50.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4242315292358398
37268 2023-02-16,23:36:50.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37393 2023-02-16,23:36:50.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37152 2023-02-16,23:36:50.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37035 2023-02-16,23:36:50.380 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3988261222839355
36814 2023-02-16,23:36:50.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3869224786758423
36932 2023-02-16,23:36:50.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3953471183776855
37152 2023-02-16,23:36:50.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37268 2023-02-16,23:36:50.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37393 2023-02-16,23:36:50.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37035 2023-02-16,23:36:50.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.401347279548645
36814 2023-02-16,23:36:50.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4549890756607056
37152 2023-02-16,23:36:50.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37268 2023-02-16,23:36:50.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37393 2023-02-16,23:36:50.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36932 2023-02-16,23:36:50.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4126067161560059
37035 2023-02-16,23:36:50.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3843406438827515
36814 2023-02-16,23:36:50.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4054207801818848
37152 2023-02-16,23:36:50.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37268 2023-02-16,23:36:50.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
37393 2023-02-16,23:36:50.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36932 2023-02-16,23:36:50.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3715801239013672
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)38255 2023-02-16,23:36:50.701 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37035 2023-02-16,23:36:50.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.37798273563385
38255 2023-02-16,23:36:50.741 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:36:50.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849804401397705
37152 2023-02-16,23:36:50.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37393 2023-02-16,23:36:50.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36932 2023-02-16,23:36:50.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3738690614700317
37268 2023-02-16,23:36:50.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
37035 2023-02-16,23:36:50.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4411475658416748
36814 2023-02-16,23:36:50.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3916127681732178
37152 2023-02-16,23:36:50.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37393 2023-02-16,23:36:50.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37268 2023-02-16,23:36:50.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
36932 2023-02-16,23:36:50.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3907462358474731
37035 2023-02-16,23:36:50.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3565349578857422
36814 2023-02-16,23:36:50.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3383216857910156
37152 2023-02-16,23:36:50.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37393 2023-02-16,23:36:51.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37268 2023-02-16,23:36:51.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37035 2023-02-16,23:36:51.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4241633415222168
36932 2023-02-16,23:36:51.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3851269483566284
36814 2023-02-16,23:36:51.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3776403665542603
37152 2023-02-16,23:36:51.101 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37393 2023-02-16,23:36:51.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37268 2023-02-16,23:36:51.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
37035 2023-02-16,23:36:51.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3925492763519287
36932 2023-02-16,23:36:51.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4016106128692627
36814 2023-02-16,23:36:51.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4089670181274414
37152 2023-02-16,23:36:51.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37393 2023-02-16,23:36:51.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37268 2023-02-16,23:36:51.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
37035 2023-02-16,23:36:51.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4014188051223755
36814 2023-02-16,23:36:51.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3803749084472656
36932 2023-02-16,23:36:51.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4069958925247192
37152 2023-02-16,23:36:51.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37393 2023-02-16,23:36:51.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37268 2023-02-16,23:36:51.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
37035 2023-02-16,23:36:51.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4170869588851929
36814 2023-02-16,23:36:51.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.382609486579895
36932 2023-02-16,23:36:51.439 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.397334337234497
37152 2023-02-16,23:36:51.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37393 2023-02-16,23:36:51.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37268 2023-02-16,23:36:51.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
37035 2023-02-16,23:36:51.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3960249423980713
36814 2023-02-16,23:36:51.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.375091552734375
37152 2023-02-16,23:36:51.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37393 2023-02-16,23:36:51.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36932 2023-02-16,23:36:51.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4258108139038086
37268 2023-02-16,23:36:51.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37035 2023-02-16,23:36:51.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3369287252426147
37152 2023-02-16,23:36:51.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37393 2023-02-16,23:36:51.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
36814 2023-02-16,23:36:51.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.382200837135315
36932 2023-02-16,23:36:51.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.437598466873169
37268 2023-02-16,23:36:51.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37035 2023-02-16,23:36:51.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3634005784988403
37152 2023-02-16,23:36:51.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
36814 2023-02-16,23:36:51.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.428638219833374
36932 2023-02-16,23:36:51.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4172497987747192
37268 2023-02-16,23:36:51.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
37035 2023-02-16,23:36:51.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3466691970825195
37393 2023-02-16,23:36:51.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37152 2023-02-16,23:36:51.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
36814 2023-02-16,23:36:51.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3791435956954956
36932 2023-02-16,23:36:51.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4252655506134033
37035 2023-02-16,23:36:51.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3505923748016357
37268 2023-02-16,23:36:51.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
37393 2023-02-16,23:36:51.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37152 2023-02-16,23:36:51.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
36814 2023-02-16,23:36:52.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.336585283279419
36932 2023-02-16,23:36:52.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3933964967727661
37393 2023-02-16,23:36:52.067 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37035 2023-02-16,23:36:52.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3882148265838623
37268 2023-02-16,23:36:52.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
37152 2023-02-16,23:36:52.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
36814 2023-02-16,23:36:52.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3816419839859009
36932 2023-02-16,23:36:52.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4020123481750488
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37393 2023-02-16,23:36:52.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
37035 2023-02-16,23:36:52.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3438923358917236
37268 2023-02-16,23:36:52.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37152 2023-02-16,23:36:52.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
36814 2023-02-16,23:36:52.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.417863130569458
36932 2023-02-16,23:36:52.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4422743320465088
37393 2023-02-16,23:36:52.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37152 2023-02-16,23:36:52.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37268 2023-02-16,23:36:52.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
37035 2023-02-16,23:36:52.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.367254376411438
36814 2023-02-16,23:36:52.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.397599697113037
36932 2023-02-16,23:36:52.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3761382102966309
37393 2023-02-16,23:36:52.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37152 2023-02-16,23:36:52.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37035 2023-02-16,23:36:52.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4134232997894287
37268 2023-02-16,23:36:52.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
36814 2023-02-16,23:36:52.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3641157150268555
36932 2023-02-16,23:36:52.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4063301086425781
37393 2023-02-16,23:36:52.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
37152 2023-02-16,23:36:52.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37035 2023-02-16,23:36:52.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3533016443252563
37268 2023-02-16,23:36:52.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36814 2023-02-16,23:36:52.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3732504844665527
36932 2023-02-16,23:36:52.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.366058588027954
37152 2023-02-16,23:36:52.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37393 2023-02-16,23:36:52.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
37268 2023-02-16,23:36:52.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
37035 2023-02-16,23:36:52.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3588567972183228
36814 2023-02-16,23:36:52.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3979381322860718
36932 2023-02-16,23:36:52.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.370147705078125
37152 2023-02-16,23:36:52.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37393 2023-02-16,23:36:52.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
37268 2023-02-16,23:36:52.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
37035 2023-02-16,23:36:52.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3841581344604492
36814 2023-02-16,23:36:52.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4244991540908813
36932 2023-02-16,23:36:52.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.397002100944519
37393 2023-02-16,23:36:52.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
37152 2023-02-16,23:36:52.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37268 2023-02-16,23:36:52.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37035 2023-02-16,23:36:52.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3563084602355957
36814 2023-02-16,23:36:52.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4109559059143066
36932 2023-02-16,23:36:52.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3798118829727173
37393 2023-02-16,23:36:52.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37152 2023-02-16,23:36:53.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37035 2023-02-16,23:36:53.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3676636219024658
37268 2023-02-16,23:36:53.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
36814 2023-02-16,23:36:53.040 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4324678182601929
37862 2023-02-16,23:36:53.071 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37862 2023-02-16,23:36:53.073 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37862 2023-02-16,23:36:53.073 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0']
37862 2023-02-16,23:36:53.075 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 35442436}
36932 2023-02-16,23:36:53.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.36545991897583
37393 2023-02-16,23:36:53.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37152 2023-02-16,23:36:53.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37268 2023-02-16,23:36:53.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37035 2023-02-16,23:36:53.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3608744144439697
36814 2023-02-16,23:36:53.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3588788509368896
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)38362 2023-02-16,23:36:53.159 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36932 2023-02-16,23:36:53.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3584359884262085
37393 2023-02-16,23:36:53.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37152 2023-02-16,23:36:53.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
38362 2023-02-16,23:36:53.235 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37268 2023-02-16,23:36:53.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
37035 2023-02-16,23:36:53.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4207088947296143
36814 2023-02-16,23:36:53.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3234732151031494
37393 2023-02-16,23:36:53.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36932 2023-02-16,23:36:53.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3542176485061646
37152 2023-02-16,23:36:53.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37268 2023-02-16,23:36:53.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
36814 2023-02-16,23:36:53.382 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3789477348327637
37035 2023-02-16,23:36:53.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4325180053710938
37393 2023-02-16,23:36:53.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37152 2023-02-16,23:36:53.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
36932 2023-02-16,23:36:53.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4283952713012695
37268 2023-02-16,23:36:53.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
36814 2023-02-16,23:36:53.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3314764499664307
37035 2023-02-16,23:36:53.500 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.397099256515503
37393 2023-02-16,23:36:53.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37152 2023-02-16,23:36:53.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
36932 2023-02-16,23:36:53.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.431449055671692
37268 2023-02-16,23:36:53.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
36814 2023-02-16,23:36:53.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3857005834579468
37035 2023-02-16,23:36:53.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3702080249786377
37393 2023-02-16,23:36:53.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37152 2023-02-16,23:36:53.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37268 2023-02-16,23:36:53.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
36932 2023-02-16,23:36:53.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3832428455352783
36814 2023-02-16,23:36:53.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4049181938171387
37035 2023-02-16,23:36:53.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4288166761398315
37393 2023-02-16,23:36:53.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
37152 2023-02-16,23:36:53.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37268 2023-02-16,23:36:53.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
36932 2023-02-16,23:36:53.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.373924970626831
36814 2023-02-16,23:36:53.862 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3893150091171265
37035 2023-02-16,23:36:53.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4247705936431885
37393 2023-02-16,23:36:53.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
37152 2023-02-16,23:36:53.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
36932 2023-02-16,23:36:53.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.3994954824447632
37268 2023-02-16,23:36:53.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37035 2023-02-16,23:36:53.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3964160680770874
36814 2023-02-16,23:36:53.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3841919898986816
37393 2023-02-16,23:36:54.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
37152 2023-02-16,23:36:54.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37268 2023-02-16,23:36:54.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
36932 2023-02-16,23:36:54.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4021821022033691
37035 2023-02-16,23:36:54.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4121570587158203
36814 2023-02-16,23:36:54.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3632800579071045
37393 2023-02-16,23:36:54.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37152 2023-02-16,23:36:54.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37268 2023-02-16,23:36:54.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
36932 2023-02-16,23:36:54.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3813786506652832
37393 2023-02-16,23:36:54.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
36814 2023-02-16,23:36:54.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3563777208328247
37035 2023-02-16,23:36:54.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3711450099945068
37152 2023-02-16,23:36:54.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37268 2023-02-16,23:36:54.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
36932 2023-02-16,23:36:54.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.391746163368225
37393 2023-02-16,23:36:54.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
37152 2023-02-16,23:36:54.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
36814 2023-02-16,23:36:54.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.363357424736023
37035 2023-02-16,23:36:54.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3745208978652954
37268 2023-02-16,23:36:54.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
36932 2023-02-16,23:36:54.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3978066444396973
37152 2023-02-16,23:36:54.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37393 2023-02-16,23:36:54.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
37035 2023-02-16,23:36:54.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3908747434616089
36814 2023-02-16,23:36:54.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3599843978881836
37268 2023-02-16,23:36:54.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
36932 2023-02-16,23:36:54.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4326459169387817
37152 2023-02-16,23:36:54.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37393 2023-02-16,23:36:54.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
37035 2023-02-16,23:36:54.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3856109380722046
37268 2023-02-16,23:36:54.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
36814 2023-02-16,23:36:54.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3692384958267212
36932 2023-02-16,23:36:54.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4010274410247803
37152 2023-02-16,23:36:54.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37393 2023-02-16,23:36:54.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37268 2023-02-16,23:36:54.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
36814 2023-02-16,23:36:54.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3857587575912476
37035 2023-02-16,23:36:54.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4027951955795288
36932 2023-02-16,23:36:54.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.384976863861084
37152 2023-02-16,23:36:54.799 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37393 2023-02-16,23:36:54.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36814 2023-02-16,23:36:54.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4575164318084717
37035 2023-02-16,23:36:54.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4076013565063477
37268 2023-02-16,23:36:54.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
36932 2023-02-16,23:36:54.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4438930749893188
37152 2023-02-16,23:36:54.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37393 2023-02-16,23:36:54.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
36814 2023-02-16,23:36:54.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3967911005020142
37035 2023-02-16,23:36:54.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3972538709640503
37268 2023-02-16,23:36:54.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
36932 2023-02-16,23:36:54.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4106388092041016
37152 2023-02-16,23:36:55.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37393 2023-02-16,23:36:55.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
36814 2023-02-16,23:36:55.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3973958492279053
37268 2023-02-16,23:36:55.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37035 2023-02-16,23:36:55.101 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4270573854446411
36932 2023-02-16,23:36:55.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4295060634613037
37393 2023-02-16,23:36:55.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
37152 2023-02-16,23:36:55.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37268 2023-02-16,23:36:55.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
36814 2023-02-16,23:36:55.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3413835763931274
37035 2023-02-16,23:36:55.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4396941661834717
36932 2023-02-16,23:36:55.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.397299885749817
37393 2023-02-16,23:36:55.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37152 2023-02-16,23:36:55.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37268 2023-02-16,23:36:55.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37035 2023-02-16,23:36:55.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4194245338439941
36814 2023-02-16,23:36:55.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4149296283721924
37393 2023-02-16,23:36:55.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36932 2023-02-16,23:36:55.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4501370191574097
37152 2023-02-16,23:36:55.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37035 2023-02-16,23:36:55.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4267299175262451
37268 2023-02-16,23:36:55.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
36814 2023-02-16,23:36:55.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3821238279342651
37393 2023-02-16,23:36:55.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
37152 2023-02-16,23:36:55.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
36932 2023-02-16,23:36:55.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4374644756317139
37268 2023-02-16,23:36:55.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
36814 2023-02-16,23:36:55.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4115444421768188
37035 2023-02-16,23:36:55.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3941004276275635
37393 2023-02-16,23:36:55.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
37152 2023-02-16,23:36:55.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
36932 2023-02-16,23:36:55.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4008259773254395
37268 2023-02-16,23:36:55.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
36814 2023-02-16,23:36:55.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3540418148040771
37035 2023-02-16,23:36:55.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.403410792350769
37393 2023-02-16,23:36:55.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
37152 2023-02-16,23:36:55.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
36932 2023-02-16,23:36:55.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3918273448944092
37268 2023-02-16,23:36:55.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37035 2023-02-16,23:36:55.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4437915086746216
36814 2023-02-16,23:36:55.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3788801431655884
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37533 2023-02-16,23:36:55.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37393 2023-02-16,23:36:55.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
37152 2023-02-16,23:36:55.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
36932 2023-02-16,23:36:55.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4070322513580322
37268 2023-02-16,23:36:55.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37035 2023-02-16,23:36:55.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3767131567001343
37533 2023-02-16,23:36:55.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36814 2023-02-16,23:36:55.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3489189147949219
37393 2023-02-16,23:36:55.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37152 2023-02-16,23:36:55.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
36932 2023-02-16,23:36:56.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4119879007339478
37268 2023-02-16,23:36:56.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
37035 2023-02-16,23:36:56.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4070398807525635
37533 2023-02-16,23:36:56.103 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37393 2023-02-16,23:36:56.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
36814 2023-02-16,23:36:56.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4006401300430298
37152 2023-02-16,23:36:56.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
36932 2023-02-16,23:36:56.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3751742839813232
37268 2023-02-16,23:36:56.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
37035 2023-02-16,23:36:56.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.365751028060913
37393 2023-02-16,23:36:56.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37533 2023-02-16,23:36:56.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:36:56.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3739368915557861
37152 2023-02-16,23:36:56.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
36932 2023-02-16,23:36:56.281 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3888111114501953
37268 2023-02-16,23:36:56.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37393 2023-02-16,23:36:56.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
37035 2023-02-16,23:36:56.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3708349466323853
37533 2023-02-16,23:36:56.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:36:56.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4598972797393799
37152 2023-02-16,23:36:56.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
36932 2023-02-16,23:36:56.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.373390793800354
37268 2023-02-16,23:36:56.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37393 2023-02-16,23:36:56.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
37035 2023-02-16,23:36:56.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3979082107543945
37533 2023-02-16,23:36:56.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36814 2023-02-16,23:36:56.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.347257137298584
37152 2023-02-16,23:36:56.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
36932 2023-02-16,23:36:56.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3940931558609009
37983 2023-02-16,23:36:56.578 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37983 2023-02-16,23:36:56.580 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
37983 2023-02-16,23:36:56.580 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1']
37983 2023-02-16,23:36:56.582 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 28354564}
37268 2023-02-16,23:36:56.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37393 2023-02-16,23:36:56.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
37035 2023-02-16,23:36:56.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.379178524017334
37533 2023-02-16,23:36:56.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36814 2023-02-16,23:36:56.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4058061838150024
36932 2023-02-16,23:36:56.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4030332565307617
37152 2023-02-16,23:36:56.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37268 2023-02-16,23:36:56.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37393 2023-02-16,23:36:56.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37035 2023-02-16,23:36:56.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3648383617401123
37533 2023-02-16,23:36:56.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
37152 2023-02-16,23:36:56.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
36814 2023-02-16,23:36:56.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4007693529129028
36932 2023-02-16,23:36:56.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3719415664672852
37268 2023-02-16,23:36:56.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37393 2023-02-16,23:36:56.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37035 2023-02-16,23:36:56.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3578498363494873
37533 2023-02-16,23:36:56.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37152 2023-02-16,23:36:56.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)38487 2023-02-16,23:36:56.965 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:36:56.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3908659219741821
36932 2023-02-16,23:36:56.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3947949409484863
37268 2023-02-16,23:36:56.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37393 2023-02-16,23:36:57.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38487 2023-02-16,23:36:57.020 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37533 2023-02-16,23:36:57.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
37035 2023-02-16,23:36:57.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3529702425003052
37152 2023-02-16,23:36:57.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
36814 2023-02-16,23:36:57.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4019535779953003
37268 2023-02-16,23:36:57.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
36932 2023-02-16,23:36:57.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.360951542854309
37393 2023-02-16,23:36:57.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37533 2023-02-16,23:36:57.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
37035 2023-02-16,23:36:57.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4301222562789917
37152 2023-02-16,23:36:57.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37268 2023-02-16,23:36:57.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
36814 2023-02-16,23:36:57.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3378944396972656
36932 2023-02-16,23:36:57.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3876945972442627
37393 2023-02-16,23:36:57.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
37533 2023-02-16,23:36:57.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37035 2023-02-16,23:36:57.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4324371814727783
37152 2023-02-16,23:36:57.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37268 2023-02-16,23:36:57.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37393 2023-02-16,23:36:57.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
36814 2023-02-16,23:36:57.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.374245524406433
36932 2023-02-16,23:36:57.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4169247150421143
37533 2023-02-16,23:36:57.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37035 2023-02-16,23:36:57.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3828551769256592
37152 2023-02-16,23:36:57.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
36932 2023-02-16,23:36:57.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4190104007720947
37393 2023-02-16,23:36:57.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
37533 2023-02-16,23:36:57.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37268 2023-02-16,23:36:57.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
36814 2023-02-16,23:36:57.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3832398653030396
37035 2023-02-16,23:36:57.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3741408586502075
37152 2023-02-16,23:36:57.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37393 2023-02-16,23:36:57.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
37533 2023-02-16,23:36:57.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36932 2023-02-16,23:36:57.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3537659645080566
37268 2023-02-16,23:36:57.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
36814 2023-02-16,23:36:57.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.372274398803711
37035 2023-02-16,23:36:57.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.3995559215545654
37152 2023-02-16,23:36:57.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37393 2023-02-16,23:36:57.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
37268 2023-02-16,23:36:57.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37533 2023-02-16,23:36:57.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36814 2023-02-16,23:36:57.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3823319673538208
37035 2023-02-16,23:36:57.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.402777075767517
37152 2023-02-16,23:36:57.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
36932 2023-02-16,23:36:57.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4237791299819946
37393 2023-02-16,23:36:57.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
37152 2023-02-16,23:36:57.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37268 2023-02-16,23:36:57.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37533 2023-02-16,23:36:57.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:36:57.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4120532274246216
36932 2023-02-16,23:36:57.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3884726762771606
37035 2023-02-16,23:36:57.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3815646171569824
37393 2023-02-16,23:36:58.101 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
37152 2023-02-16,23:36:58.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
36932 2023-02-16,23:36:58.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.3996405601501465
37268 2023-02-16,23:36:58.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37533 2023-02-16,23:36:58.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
37035 2023-02-16,23:36:58.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3925583362579346
36814 2023-02-16,23:36:58.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3561949729919434
37393 2023-02-16,23:36:58.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
36932 2023-02-16,23:36:58.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.406418800354004
37152 2023-02-16,23:36:58.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
37035 2023-02-16,23:36:58.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3974820375442505
37268 2023-02-16,23:36:58.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37533 2023-02-16,23:36:58.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:36:58.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4533536434173584
37393 2023-02-16,23:36:58.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37152 2023-02-16,23:36:58.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
36932 2023-02-16,23:36:58.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4316082000732422
37035 2023-02-16,23:36:58.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4343703985214233
37268 2023-02-16,23:36:58.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37533 2023-02-16,23:36:58.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:36:58.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.383190393447876
37393 2023-02-16,23:36:58.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37152 2023-02-16,23:36:58.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37268 2023-02-16,23:36:58.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37533 2023-02-16,23:36:58.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36932 2023-02-16,23:36:58.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4227133989334106
37035 2023-02-16,23:36:58.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4014651775360107
36814 2023-02-16,23:36:58.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4050239324569702
37393 2023-02-16,23:36:58.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37152 2023-02-16,23:36:58.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37533 2023-02-16,23:36:58.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37268 2023-02-16,23:36:58.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
36814 2023-02-16,23:36:58.691 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3940284252166748
36932 2023-02-16,23:36:58.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3705179691314697
37035 2023-02-16,23:36:58.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3857609033584595
37393 2023-02-16,23:36:58.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37152 2023-02-16,23:36:58.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
37533 2023-02-16,23:36:58.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37035 2023-02-16,23:36:58.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.445773720741272
37268 2023-02-16,23:36:58.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
36814 2023-02-16,23:36:58.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.424075961112976
37393 2023-02-16,23:36:58.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
36932 2023-02-16,23:36:58.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3915005922317505
37152 2023-02-16,23:36:58.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37035 2023-02-16,23:36:58.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4119739532470703
37268 2023-02-16,23:36:58.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
37393 2023-02-16,23:36:58.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37533 2023-02-16,23:36:58.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37757 2023-02-16,23:36:58.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36932 2023-02-16,23:36:58.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3867708444595337
36814 2023-02-16,23:36:59.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.4007956981658936
37152 2023-02-16,23:36:59.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
36932 2023-02-16,23:36:59.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3503029346466064
37268 2023-02-16,23:36:59.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37393 2023-02-16,23:36:59.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37533 2023-02-16,23:36:59.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37757 2023-02-16,23:36:59.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36814 2023-02-16,23:36:59.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3969305753707886
37035 2023-02-16,23:36:59.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4304978847503662
37152 2023-02-16,23:36:59.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
36932 2023-02-16,23:36:59.280 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4108073711395264
37757 2023-02-16,23:36:59.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36814 2023-02-16,23:36:59.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4235270023345947
37152 2023-02-16,23:36:59.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
37268 2023-02-16,23:36:59.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37393 2023-02-16,23:36:59.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37533 2023-02-16,23:36:59.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
37035 2023-02-16,23:36:59.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3978132009506226
36932 2023-02-16,23:36:59.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4374080896377563
37757 2023-02-16,23:36:59.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37152 2023-02-16,23:36:59.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37268 2023-02-16,23:36:59.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37393 2023-02-16,23:36:59.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37533 2023-02-16,23:36:59.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36814 2023-02-16,23:36:59.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.386501431465149
37035 2023-02-16,23:36:59.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4525094032287598
36932 2023-02-16,23:36:59.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3874186277389526
37757 2023-02-16,23:36:59.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
37152 2023-02-16,23:36:59.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37268 2023-02-16,23:36:59.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37393 2023-02-16,23:36:59.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
37533 2023-02-16,23:36:59.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:36:59.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3835734128952026
37035 2023-02-16,23:36:59.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.438712477684021
36932 2023-02-16,23:36:59.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3518983125686646
37268 2023-02-16,23:36:59.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37393 2023-02-16,23:36:59.783 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
37757 2023-02-16,23:36:59.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37533 2023-02-16,23:36:59.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:36:59.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4235975742340088
37035 2023-02-16,23:36:59.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.401340365409851
37152 2023-02-16,23:36:59.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
36932 2023-02-16,23:36:59.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4115065336227417
37757 2023-02-16,23:36:59.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36814 2023-02-16,23:36:59.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4090118408203125
37152 2023-02-16,23:36:59.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3937647342681885
37268 2023-02-16,23:36:59.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37393 2023-02-16,23:36:59.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37533 2023-02-16,23:36:59.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
37035 2023-02-16,23:36:59.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3923676013946533
36932 2023-02-16,23:36:59.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.39905846118927
37757 2023-02-16,23:37:00.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:37:00.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.408419132232666
37152 2023-02-16,23:37:00.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4106745719909668
37393 2023-02-16,23:37:00.103 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37533 2023-02-16,23:37:00.105 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
37268 2023-02-16,23:37:00.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37035 2023-02-16,23:37:00.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4074100255966187
36932 2023-02-16,23:37:00.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4102509021759033
37757 2023-02-16,23:37:00.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37152 2023-02-16,23:37:00.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4224494695663452
37393 2023-02-16,23:37:00.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37533 2023-02-16,23:37:00.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37035 2023-02-16,23:37:00.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.412702202796936
37268 2023-02-16,23:37:00.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
36814 2023-02-16,23:37:00.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4014084339141846
36932 2023-02-16,23:37:00.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4098330736160278
37757 2023-02-16,23:37:00.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
37152 2023-02-16,23:37:00.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.373290777206421
37393 2023-02-16,23:37:00.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37268 2023-02-16,23:37:00.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37533 2023-02-16,23:37:00.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36814 2023-02-16,23:37:00.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.414417028427124
36932 2023-02-16,23:37:00.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3438721895217896
37035 2023-02-16,23:37:00.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.375392198562622
37757 2023-02-16,23:37:00.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
37152 2023-02-16,23:37:00.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3700356483459473
37268 2023-02-16,23:37:00.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37393 2023-02-16,23:37:00.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37533 2023-02-16,23:37:00.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36932 2023-02-16,23:37:00.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3786489963531494
37035 2023-02-16,23:37:00.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3884377479553223
36814 2023-02-16,23:37:00.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3658568859100342
37757 2023-02-16,23:37:00.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37152 2023-02-16,23:37:00.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3986005783081055
37035 2023-02-16,23:37:00.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3732109069824219
37268 2023-02-16,23:37:00.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37393 2023-02-16,23:37:00.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37533 2023-02-16,23:37:00.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36932 2023-02-16,23:37:00.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3740215301513672
36814 2023-02-16,23:37:00.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4072866439819336
38108 2023-02-16,23:37:00.835 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
38108 2023-02-16,23:37:00.836 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
38108 2023-02-16,23:37:00.836 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2']
38108 2023-02-16,23:37:00.838 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 21266692}
37757 2023-02-16,23:37:00.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37152 2023-02-16,23:37:00.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4015276432037354
36932 2023-02-16,23:37:00.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3462458848953247
37035 2023-02-16,23:37:00.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3938381671905518
37268 2023-02-16,23:37:00.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37393 2023-02-16,23:37:00.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37533 2023-02-16,23:37:00.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:37:00.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.408043622970581
37757 2023-02-16,23:37:01.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
36932 2023-02-16,23:37:01.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3983654975891113
37035 2023-02-16,23:37:01.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.403451919555664
37152 2023-02-16,23:37:01.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.384803056716919
37268 2023-02-16,23:37:01.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37393 2023-02-16,23:37:01.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37533 2023-02-16,23:37:01.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36814 2023-02-16,23:37:01.059 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4159785509109497
37268 2023-02-16,23:37:01.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37757 2023-02-16,23:37:01.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36932 2023-02-16,23:37:01.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3888089656829834
37035 2023-02-16,23:37:01.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3713401556015015
37152 2023-02-16,23:37:01.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3778215646743774
37393 2023-02-16,23:37:01.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37533 2023-02-16,23:37:01.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:37:01.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3915282487869263
37757 2023-02-16,23:37:01.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36932 2023-02-16,23:37:01.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3498793840408325
37035 2023-02-16,23:37:01.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3954354524612427
37268 2023-02-16,23:37:01.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37393 2023-02-16,23:37:01.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37533 2023-02-16,23:37:01.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37152 2023-02-16,23:37:01.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4407743215560913
36814 2023-02-16,23:37:01.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3323099613189697
37268 2023-02-16,23:37:01.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37757 2023-02-16,23:37:01.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36932 2023-02-16,23:37:01.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3587852716445923
37152 2023-02-16,23:37:01.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3575180768966675
37393 2023-02-16,23:37:01.529 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37533 2023-02-16,23:37:01.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37035 2023-02-16,23:37:01.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3599048852920532
36814 2023-02-16,23:37:01.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3890024423599243
37757 2023-02-16,23:37:01.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36932 2023-02-16,23:37:01.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.367974042892456
37035 2023-02-16,23:37:01.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3868262767791748
37268 2023-02-16,23:37:01.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37393 2023-02-16,23:37:01.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37533 2023-02-16,23:37:01.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
36814 2023-02-16,23:37:01.691 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3891795873641968
37152 2023-02-16,23:37:01.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.423225998878479
36932 2023-02-16,23:37:01.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.3999429941177368
36814 2023-02-16,23:37:01.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.422569751739502
37035 2023-02-16,23:37:01.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.418027639389038
37268 2023-02-16,23:37:01.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37393 2023-02-16,23:37:01.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37533 2023-02-16,23:37:01.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
37152 2023-02-16,23:37:01.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3922343254089355
37757 2023-02-16,23:37:01.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36932 2023-02-16,23:37:01.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3864022493362427
37268 2023-02-16,23:37:01.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37533 2023-02-16,23:37:02.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36814 2023-02-16,23:37:02.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4018529653549194
37035 2023-02-16,23:37:02.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4197860956192017
37152 2023-02-16,23:37:02.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4005882740020752
37393 2023-02-16,23:37:02.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37757 2023-02-16,23:37:02.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:37:02.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4087002277374268
37533 2023-02-16,23:37:02.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36932 2023-02-16,23:37:02.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4538414478302002
37035 2023-02-16,23:37:02.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3532524108886719
37152 2023-02-16,23:37:02.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4177603721618652
37268 2023-02-16,23:37:02.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37393 2023-02-16,23:37:02.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37757 2023-02-16,23:37:02.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36814 2023-02-16,23:37:02.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3876806497573853
36932 2023-02-16,23:37:02.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4045182466506958
37533 2023-02-16,23:37:02.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37152 2023-02-16,23:37:02.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3959225416183472
37268 2023-02-16,23:37:02.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37393 2023-02-16,23:37:02.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37757 2023-02-16,23:37:02.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37035 2023-02-16,23:37:02.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4243812561035156
36814 2023-02-16,23:37:02.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3804649114608765
36932 2023-02-16,23:37:02.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3851689100265503
37268 2023-02-16,23:37:02.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37393 2023-02-16,23:37:02.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37533 2023-02-16,23:37:02.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37035 2023-02-16,23:37:02.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3893877267837524
37152 2023-02-16,23:37:02.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372994661331177
37757 2023-02-16,23:37:02.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
36814 2023-02-16,23:37:02.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3487186431884766
36932 2023-02-16,23:37:02.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3909560441970825
37268 2023-02-16,23:37:02.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37393 2023-02-16,23:37:02.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
37533 2023-02-16,23:37:02.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37035 2023-02-16,23:37:02.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4007726907730103
37152 2023-02-16,23:37:02.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3629781007766724
37757 2023-02-16,23:37:02.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36814 2023-02-16,23:37:02.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3817787170410156
36932 2023-02-16,23:37:02.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3395135402679443
37268 2023-02-16,23:37:02.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37152 2023-02-16,23:37:02.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3474425077438354
37393 2023-02-16,23:37:02.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
37533 2023-02-16,23:37:02.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37757 2023-02-16,23:37:02.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37035 2023-02-16,23:37:02.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4058399200439453
36814 2023-02-16,23:37:02.828 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4010770320892334
36932 2023-02-16,23:37:02.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3779102563858032
37268 2023-02-16,23:37:02.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37533 2023-02-16,23:37:02.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37035 2023-02-16,23:37:02.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.432006597518921
37152 2023-02-16,23:37:02.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3508836030960083
37393 2023-02-16,23:37:02.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
37757 2023-02-16,23:37:02.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37862 2023-02-16,23:37:02.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36814 2023-02-16,23:37:02.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621459007263184
37035 2023-02-16,23:37:03.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.424481987953186
37152 2023-02-16,23:37:03.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3880313634872437
37268 2023-02-16,23:37:03.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37393 2023-02-16,23:37:03.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37533 2023-02-16,23:37:03.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37757 2023-02-16,23:37:03.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37862 2023-02-16,23:37:03.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
36814 2023-02-16,23:37:03.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.3957953453063965
36932 2023-02-16,23:37:03.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4089428186416626
36814 2023-02-16,23:37:03.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3777451515197754
37035 2023-02-16,23:37:03.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.370121717453003
37862 2023-02-16,23:37:03.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37152 2023-02-16,23:37:03.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441026210784912
37268 2023-02-16,23:37:03.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37393 2023-02-16,23:37:03.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37757 2023-02-16,23:37:03.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
37533 2023-02-16,23:37:03.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36932 2023-02-16,23:37:03.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3805327415466309
37862 2023-02-16,23:37:03.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36932 2023-02-16,23:37:03.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3824141025543213
37035 2023-02-16,23:37:03.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3918343782424927
37268 2023-02-16,23:37:03.515 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37393 2023-02-16,23:37:03.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37533 2023-02-16,23:37:03.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37757 2023-02-16,23:37:03.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
37152 2023-02-16,23:37:03.524 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3685020208358765
36814 2023-02-16,23:37:03.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3915424346923828
37862 2023-02-16,23:37:03.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36932 2023-02-16,23:37:03.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3761719465255737
37035 2023-02-16,23:37:03.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.387103796005249
37393 2023-02-16,23:37:03.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37533 2023-02-16,23:37:03.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
36814 2023-02-16,23:37:03.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.417910099029541
37152 2023-02-16,23:37:03.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4132640361785889
37268 2023-02-16,23:37:03.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37757 2023-02-16,23:37:03.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
37035 2023-02-16,23:37:03.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.349919080734253
37152 2023-02-16,23:37:03.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.353307843208313
37268 2023-02-16,23:37:03.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37393 2023-02-16,23:37:03.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37533 2023-02-16,23:37:03.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37862 2023-02-16,23:37:03.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37757 2023-02-16,23:37:03.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:37:03.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4015403985977173
36932 2023-02-16,23:37:03.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.382375717163086
38255 2023-02-16,23:37:03.973 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
38255 2023-02-16,23:37:03.976 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
38255 2023-02-16,23:37:03.976 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3']
38255 2023-02-16,23:37:03.978 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 14178820}
37035 2023-02-16,23:37:04.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4121125936508179
37268 2023-02-16,23:37:04.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37393 2023-02-16,23:37:04.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37533 2023-02-16,23:37:04.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37757 2023-02-16,23:37:04.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37862 2023-02-16,23:37:04.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37152 2023-02-16,23:37:04.059 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.359222173690796
36814 2023-02-16,23:37:04.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3906491994857788
36932 2023-02-16,23:37:04.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4277561902999878
37268 2023-02-16,23:37:04.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37393 2023-02-16,23:37:04.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37533 2023-02-16,23:37:04.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
37757 2023-02-16,23:37:04.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37862 2023-02-16,23:37:04.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
37152 2023-02-16,23:37:04.237 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848305940628052
36814 2023-02-16,23:37:04.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3969988822937012
36932 2023-02-16,23:37:04.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3791394233703613
37035 2023-02-16,23:37:04.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4391241073608398
37268 2023-02-16,23:37:04.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37862 2023-02-16,23:37:04.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37393 2023-02-16,23:37:04.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37533 2023-02-16,23:37:04.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37757 2023-02-16,23:37:04.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
37152 2023-02-16,23:37:04.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3571702241897583
36814 2023-02-16,23:37:04.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3856068849563599
36932 2023-02-16,23:37:04.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.337052822113037
37035 2023-02-16,23:37:04.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3879246711730957
37268 2023-02-16,23:37:04.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
37035 2023-02-16,23:37:04.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3517407178878784
37393 2023-02-16,23:37:04.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37533 2023-02-16,23:37:04.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37757 2023-02-16,23:37:04.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
37862 2023-02-16,23:37:04.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:37:04.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3700406551361084
36932 2023-02-16,23:37:04.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3806878328323364
37152 2023-02-16,23:37:04.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.367594599723816
37035 2023-02-16,23:37:04.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4121880531311035
37862 2023-02-16,23:37:04.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:37:04.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4043843746185303
37152 2023-02-16,23:37:04.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3610289096832275
37268 2023-02-16,23:37:04.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
37393 2023-02-16,23:37:04.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37533 2023-02-16,23:37:04.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
37757 2023-02-16,23:37:04.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36932 2023-02-16,23:37:04.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4185149669647217
37035 2023-02-16,23:37:04.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3986856937408447
37862 2023-02-16,23:37:04.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
36932 2023-02-16,23:37:04.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.396945595741272
37152 2023-02-16,23:37:04.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4203171730041504
37268 2023-02-16,23:37:04.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
37393 2023-02-16,23:37:04.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37533 2023-02-16,23:37:04.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36814 2023-02-16,23:37:04.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3919920921325684
37757 2023-02-16,23:37:04.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36932 2023-02-16,23:37:05.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3641103506088257
37035 2023-02-16,23:37:05.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4110393524169922
37533 2023-02-16,23:37:05.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
37862 2023-02-16,23:37:05.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37152 2023-02-16,23:37:05.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.431778073310852
37268 2023-02-16,23:37:05.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37393 2023-02-16,23:37:05.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37757 2023-02-16,23:37:05.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:37:05.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3734816312789917
36932 2023-02-16,23:37:05.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.373967170715332
37035 2023-02-16,23:37:05.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4109798669815063
37757 2023-02-16,23:37:05.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37862 2023-02-16,23:37:05.290 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37268 2023-02-16,23:37:05.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
37393 2023-02-16,23:37:05.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37533 2023-02-16,23:37:05.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36814 2023-02-16,23:37:05.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3526606559753418
37152 2023-02-16,23:37:05.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3967286348342896
37035 2023-02-16,23:37:05.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3436963558197021
37757 2023-02-16,23:37:05.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36932 2023-02-16,23:37:05.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3966305255889893
37268 2023-02-16,23:37:05.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
37393 2023-02-16,23:37:05.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37533 2023-02-16,23:37:05.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37862 2023-02-16,23:37:05.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
37152 2023-02-16,23:37:05.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3701725006103516
36814 2023-02-16,23:37:05.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3822119235992432
37035 2023-02-16,23:37:05.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3785978555679321
36932 2023-02-16,23:37:05.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4237631559371948
37393 2023-02-16,23:37:05.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37533 2023-02-16,23:37:05.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37757 2023-02-16,23:37:05.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37862 2023-02-16,23:37:05.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
37152 2023-02-16,23:37:05.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4277487993240356
37268 2023-02-16,23:37:05.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
36814 2023-02-16,23:37:05.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4376394748687744
37035 2023-02-16,23:37:05.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.373327612876892
36932 2023-02-16,23:37:05.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4105513095855713
37268 2023-02-16,23:37:05.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37393 2023-02-16,23:37:05.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37862 2023-02-16,23:37:05.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37152 2023-02-16,23:37:05.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4247900247573853
37533 2023-02-16,23:37:05.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37757 2023-02-16,23:37:05.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36814 2023-02-16,23:37:05.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3707953691482544
37035 2023-02-16,23:37:05.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3452907800674438
37268 2023-02-16,23:37:05.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
36932 2023-02-16,23:37:05.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.431746482849121
37152 2023-02-16,23:37:05.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.396834135055542
37393 2023-02-16,23:37:05.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37533 2023-02-16,23:37:05.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
37862 2023-02-16,23:37:05.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
37757 2023-02-16,23:37:05.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36814 2023-02-16,23:37:06.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.391819715499878
37035 2023-02-16,23:37:06.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3996872901916504
37152 2023-02-16,23:37:06.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.412245512008667
37268 2023-02-16,23:37:06.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
36932 2023-02-16,23:37:06.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3587491512298584
37393 2023-02-16,23:37:06.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37533 2023-02-16,23:37:06.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37757 2023-02-16,23:37:06.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37862 2023-02-16,23:37:06.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:37:06.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3230180740356445
37152 2023-02-16,23:37:06.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3714227676391602
37268 2023-02-16,23:37:06.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
36932 2023-02-16,23:37:06.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3229091167449951
37035 2023-02-16,23:37:06.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3883399963378906
37393 2023-02-16,23:37:06.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37533 2023-02-16,23:37:06.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37757 2023-02-16,23:37:06.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37862 2023-02-16,23:37:06.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:37:06.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3720979690551758
37268 2023-02-16,23:37:06.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37035 2023-02-16,23:37:06.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3486497402191162
37152 2023-02-16,23:37:06.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3744136095046997
37533 2023-02-16,23:37:06.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
36932 2023-02-16,23:37:06.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3794310092926025
37393 2023-02-16,23:37:06.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37757 2023-02-16,23:37:06.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37862 2023-02-16,23:37:06.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36814 2023-02-16,23:37:06.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.415411114692688
38362 2023-02-16,23:37:06.568 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
38362 2023-02-16,23:37:06.571 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
38362 2023-02-16,23:37:06.571 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3', '4']
38362 2023-02-16,23:37:06.573 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 7090948}
37268 2023-02-16,23:37:06.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
37035 2023-02-16,23:37:06.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3591622114181519
37862 2023-02-16,23:37:06.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
36932 2023-02-16,23:37:06.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.331274151802063
37152 2023-02-16,23:37:06.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3914833068847656
37393 2023-02-16,23:37:06.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37533 2023-02-16,23:37:06.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
37757 2023-02-16,23:37:06.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
36814 2023-02-16,23:37:06.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3430030345916748
37268 2023-02-16,23:37:06.831 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37862 2023-02-16,23:37:06.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37393 2023-02-16,23:37:06.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37757 2023-02-16,23:37:06.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37533 2023-02-16,23:37:06.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
36814 2023-02-16,23:37:06.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.376558542251587
37152 2023-02-16,23:37:06.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3851981163024902
36932 2023-02-16,23:37:06.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3860421180725098
37035 2023-02-16,23:37:06.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3669131994247437
37268 2023-02-16,23:37:07.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37862 2023-02-16,23:37:07.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
37393 2023-02-16,23:37:07.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37533 2023-02-16,23:37:07.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
37757 2023-02-16,23:37:07.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36814 2023-02-16,23:37:07.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3787487745285034
36932 2023-02-16,23:37:07.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.40572190284729
37035 2023-02-16,23:37:07.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4004555940628052
37152 2023-02-16,23:37:07.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4029746055603027
37533 2023-02-16,23:37:07.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37757 2023-02-16,23:37:07.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37862 2023-02-16,23:37:07.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36932 2023-02-16,23:37:07.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3898240327835083
37152 2023-02-16,23:37:07.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4074902534484863
37268 2023-02-16,23:37:07.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
37393 2023-02-16,23:37:07.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
37983 2023-02-16,23:37:07.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
36814 2023-02-16,23:37:07.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.37516188621521
37035 2023-02-16,23:37:07.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3863917589187622
37533 2023-02-16,23:37:07.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
37757 2023-02-16,23:37:07.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36932 2023-02-16,23:37:07.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3838852643966675
37862 2023-02-16,23:37:07.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
37152 2023-02-16,23:37:07.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3968505859375
37268 2023-02-16,23:37:07.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37393 2023-02-16,23:37:07.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
36814 2023-02-16,23:37:07.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4115058183670044
37035 2023-02-16,23:37:07.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.455637812614441
37983 2023-02-16,23:37:07.439 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37533 2023-02-16,23:37:07.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
37757 2023-02-16,23:37:07.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
36932 2023-02-16,23:37:07.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3633021116256714
37152 2023-02-16,23:37:07.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4269795417785645
37268 2023-02-16,23:37:07.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37393 2023-02-16,23:37:07.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37862 2023-02-16,23:37:07.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37983 2023-02-16,23:37:07.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37035 2023-02-16,23:37:07.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4057791233062744
36814 2023-02-16,23:37:07.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3795123100280762
37757 2023-02-16,23:37:07.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
36932 2023-02-16,23:37:07.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3554881811141968
37393 2023-02-16,23:37:07.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37533 2023-02-16,23:37:07.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
37983 2023-02-16,23:37:07.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37152 2023-02-16,23:37:07.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4392966032028198
37268 2023-02-16,23:37:07.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
37862 2023-02-16,23:37:07.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:37:07.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4459824562072754
37035 2023-02-16,23:37:07.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3846657276153564
37757 2023-02-16,23:37:07.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
36932 2023-02-16,23:37:07.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3648678064346313
37268 2023-02-16,23:37:08.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37393 2023-02-16,23:37:08.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37533 2023-02-16,23:37:08.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
37862 2023-02-16,23:37:08.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
37983 2023-02-16,23:37:08.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:37:08.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4426164627075195
37035 2023-02-16,23:37:08.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3919285535812378
37152 2023-02-16,23:37:08.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4184843301773071
37757 2023-02-16,23:37:08.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37862 2023-02-16,23:37:08.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
37983 2023-02-16,23:37:08.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37152 2023-02-16,23:37:08.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4253387451171875
37268 2023-02-16,23:37:08.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37393 2023-02-16,23:37:08.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
36814 2023-02-16,23:37:08.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.34261953830719
36932 2023-02-16,23:37:08.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3593302965164185
37035 2023-02-16,23:37:08.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3377013206481934
37533 2023-02-16,23:37:08.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37757 2023-02-16,23:37:08.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
37393 2023-02-16,23:37:08.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37862 2023-02-16,23:37:08.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
37983 2023-02-16,23:37:08.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37152 2023-02-16,23:37:08.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3940925598144531
37268 2023-02-16,23:37:08.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37533 2023-02-16,23:37:08.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36814 2023-02-16,23:37:08.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4045608043670654
36932 2023-02-16,23:37:08.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3705300092697144
37035 2023-02-16,23:37:08.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3765376806259155
37757 2023-02-16,23:37:08.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37152 2023-02-16,23:37:08.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4033148288726807
37268 2023-02-16,23:37:08.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
37393 2023-02-16,23:37:08.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37533 2023-02-16,23:37:08.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
37862 2023-02-16,23:37:08.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37983 2023-02-16,23:37:08.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:37:08.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.468400239944458
36932 2023-02-16,23:37:08.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.385640025138855
37035 2023-02-16,23:37:08.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.409542202949524
37268 2023-02-16,23:37:08.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37393 2023-02-16,23:37:08.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37533 2023-02-16,23:37:08.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
37757 2023-02-16,23:37:08.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37862 2023-02-16,23:37:08.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37983 2023-02-16,23:37:08.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:37:08.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3870770931243896
36932 2023-02-16,23:37:08.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4572300910949707
37035 2023-02-16,23:37:08.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3807175159454346
37152 2023-02-16,23:37:08.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4434000253677368
37983 2023-02-16,23:37:08.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36932 2023-02-16,23:37:08.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3978395462036133
37152 2023-02-16,23:37:08.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3765647411346436
37268 2023-02-16,23:37:08.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
37393 2023-02-16,23:37:08.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37533 2023-02-16,23:37:08.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
37862 2023-02-16,23:37:08.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:37:08.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3796855211257935
37035 2023-02-16,23:37:08.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3829865455627441
37757 2023-02-16,23:37:08.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
36932 2023-02-16,23:37:09.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3963000774383545
37393 2023-02-16,23:37:09.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
37533 2023-02-16,23:37:09.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37862 2023-02-16,23:37:09.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
37983 2023-02-16,23:37:09.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
37035 2023-02-16,23:37:09.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3757613897323608
37152 2023-02-16,23:37:09.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4068124294281006
37268 2023-02-16,23:37:09.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
36814 2023-02-16,23:37:09.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.3990554809570312
37757 2023-02-16,23:37:09.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36932 2023-02-16,23:37:09.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3409864902496338
37983 2023-02-16,23:37:09.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37393 2023-02-16,23:37:09.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
37533 2023-02-16,23:37:09.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
37035 2023-02-16,23:37:09.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3823052644729614
37152 2023-02-16,23:37:09.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.366303563117981
37268 2023-02-16,23:37:09.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
37862 2023-02-16,23:37:09.380 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:37:09.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.374527931213379
37757 2023-02-16,23:37:09.382 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36932 2023-02-16,23:37:09.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.415415644645691
37983 2023-02-16,23:37:09.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37268 2023-02-16,23:37:09.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37393 2023-02-16,23:37:09.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
37533 2023-02-16,23:37:09.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
36814 2023-02-16,23:37:09.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3920241594314575
37035 2023-02-16,23:37:09.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.429706335067749
37152 2023-02-16,23:37:09.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.371002435684204
37862 2023-02-16,23:37:09.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
37757 2023-02-16,23:37:09.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
37983 2023-02-16,23:37:09.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37393 2023-02-16,23:37:09.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37533 2023-02-16,23:37:09.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36932 2023-02-16,23:37:09.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3836029767990112
37035 2023-02-16,23:37:09.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3793158531188965
37152 2023-02-16,23:37:09.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3985557556152344
37268 2023-02-16,23:37:09.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37757 2023-02-16,23:37:09.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37862 2023-02-16,23:37:09.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:37:09.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.416387915611267
37983 2023-02-16,23:37:09.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
37035 2023-02-16,23:37:09.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.336371898651123
37393 2023-02-16,23:37:09.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
37533 2023-02-16,23:37:09.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
36814 2023-02-16,23:37:09.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.413743019104004
37152 2023-02-16,23:37:09.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3793326616287231
37757 2023-02-16,23:37:09.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37862 2023-02-16,23:37:09.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
36932 2023-02-16,23:37:09.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4107590913772583
37268 2023-02-16,23:37:09.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37983 2023-02-16,23:37:10.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
37035 2023-02-16,23:37:10.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3810728788375854
37393 2023-02-16,23:37:10.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
37533 2023-02-16,23:37:10.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
36814 2023-02-16,23:37:10.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3797866106033325
37152 2023-02-16,23:37:10.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3649524450302124
37757 2023-02-16,23:37:10.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37862 2023-02-16,23:37:10.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36932 2023-02-16,23:37:10.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.354783296585083
37268 2023-02-16,23:37:10.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3954029083251953
37983 2023-02-16,23:37:10.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37035 2023-02-16,23:37:10.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4197828769683838
37393 2023-02-16,23:37:10.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37533 2023-02-16,23:37:10.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
36814 2023-02-16,23:37:10.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4064738750457764
37152 2023-02-16,23:37:10.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.357981562614441
37757 2023-02-16,23:37:10.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
37268 2023-02-16,23:37:10.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.411370038986206
37862 2023-02-16,23:37:10.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
36932 2023-02-16,23:37:10.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3791441917419434
37983 2023-02-16,23:37:10.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:37:10.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.461685299873352
37035 2023-02-16,23:37:10.524 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3971033096313477
37393 2023-02-16,23:37:10.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37533 2023-02-16,23:37:10.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
37152 2023-02-16,23:37:10.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.353778600692749
37268 2023-02-16,23:37:10.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4225332736968994
37862 2023-02-16,23:37:10.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36932 2023-02-16,23:37:10.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3479145765304565
37757 2023-02-16,23:37:10.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37983 2023-02-16,23:37:10.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
37152 2023-02-16,23:37:10.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4297879934310913
37393 2023-02-16,23:37:10.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37533 2023-02-16,23:37:10.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
36814 2023-02-16,23:37:10.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.436003565788269
37035 2023-02-16,23:37:10.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3636040687561035
37268 2023-02-16,23:37:10.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3743622303009033
37757 2023-02-16,23:37:10.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37862 2023-02-16,23:37:10.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36932 2023-02-16,23:37:10.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4000669717788696
37983 2023-02-16,23:37:10.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
37152 2023-02-16,23:37:10.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4310017824172974
37268 2023-02-16,23:37:10.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.370473027229309
37393 2023-02-16,23:37:10.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
37533 2023-02-16,23:37:10.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36814 2023-02-16,23:37:10.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3947317600250244
36932 2023-02-16,23:37:10.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3737103939056396
37035 2023-02-16,23:37:10.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.373133897781372
37757 2023-02-16,23:37:10.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37862 2023-02-16,23:37:10.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37983 2023-02-16,23:37:11.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
37393 2023-02-16,23:37:11.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37533 2023-02-16,23:37:11.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
36814 2023-02-16,23:37:11.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3666362762451172
37035 2023-02-16,23:37:11.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3972012996673584
37268 2023-02-16,23:37:11.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.4000006914138794
37757 2023-02-16,23:37:11.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
37862 2023-02-16,23:37:11.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
36932 2023-02-16,23:37:11.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4599568843841553
37152 2023-02-16,23:37:11.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.382754921913147
37983 2023-02-16,23:37:11.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37393 2023-02-16,23:37:11.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37533 2023-02-16,23:37:11.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
36814 2023-02-16,23:37:11.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3544623851776123
37035 2023-02-16,23:37:11.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4250158071517944
37152 2023-02-16,23:37:11.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.374294400215149
37268 2023-02-16,23:37:11.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4009895324707031
37757 2023-02-16,23:37:11.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
37862 2023-02-16,23:37:11.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
36932 2023-02-16,23:37:11.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3482362031936646
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
38108 2023-02-16,23:37:11.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37983 2023-02-16,23:37:11.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
38487 2023-02-16,23:37:11.471 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
38487 2023-02-16,23:37:11.474 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
38487 2023-02-16,23:37:11.474 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3', '4', '5']
38487 2023-02-16,23:37:11.476 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 3076}
37393 2023-02-16,23:37:11.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
37533 2023-02-16,23:37:11.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37035 2023-02-16,23:37:11.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4119272232055664
37268 2023-02-16,23:37:11.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.382910132408142
37862 2023-02-16,23:37:11.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
36814 2023-02-16,23:37:11.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3874616622924805
37757 2023-02-16,23:37:11.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
37152 2023-02-16,23:37:11.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.400102972984314
36932 2023-02-16,23:37:11.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4078829288482666
38108 2023-02-16,23:37:11.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37983 2023-02-16,23:37:11.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
37393 2023-02-16,23:37:11.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37533 2023-02-16,23:37:11.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37035 2023-02-16,23:37:11.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.432395339012146
37152 2023-02-16,23:37:11.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.401663899421692
37268 2023-02-16,23:37:11.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.378426432609558
37757 2023-02-16,23:37:11.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37862 2023-02-16,23:37:11.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
36814 2023-02-16,23:37:11.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3309670686721802
36932 2023-02-16,23:37:11.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4011907577514648
38108 2023-02-16,23:37:11.783 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37983 2023-02-16,23:37:11.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37152 2023-02-16,23:37:11.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3813040256500244
37393 2023-02-16,23:37:11.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37533 2023-02-16,23:37:11.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
37268 2023-02-16,23:37:11.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.441474437713623
37757 2023-02-16,23:37:11.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
37862 2023-02-16,23:37:11.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36814 2023-02-16,23:37:11.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3715215921401978
36932 2023-02-16,23:37:11.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3903886079788208
37035 2023-02-16,23:37:11.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3579521179199219
38108 2023-02-16,23:37:11.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37983 2023-02-16,23:37:12.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
37152 2023-02-16,23:37:12.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3920845985412598
37393 2023-02-16,23:37:12.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
37533 2023-02-16,23:37:12.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37268 2023-02-16,23:37:12.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3546916246414185
37757 2023-02-16,23:37:12.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
37862 2023-02-16,23:37:12.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
38108 2023-02-16,23:37:12.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:37:12.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.362827181816101
36932 2023-02-16,23:37:12.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.402148962020874
37035 2023-02-16,23:37:12.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3205252885818481
37983 2023-02-16,23:37:12.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37152 2023-02-16,23:37:12.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3972291946411133
37393 2023-02-16,23:37:12.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37533 2023-02-16,23:37:12.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
37757 2023-02-16,23:37:12.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36814 2023-02-16,23:37:12.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3709863424301147
37268 2023-02-16,23:37:12.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4235751628875732
37862 2023-02-16,23:37:12.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
38108 2023-02-16,23:37:12.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36932 2023-02-16,23:37:12.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3363858461380005
37035 2023-02-16,23:37:12.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3787245750427246
37983 2023-02-16,23:37:12.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
37152 2023-02-16,23:37:12.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4330108165740967
36814 2023-02-16,23:37:12.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4441813230514526
37393 2023-02-16,23:37:12.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37533 2023-02-16,23:37:12.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
37757 2023-02-16,23:37:12.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38108 2023-02-16,23:37:12.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36932 2023-02-16,23:37:12.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3728762865066528
37035 2023-02-16,23:37:12.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3302215337753296
37268 2023-02-16,23:37:12.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3924031257629395
37862 2023-02-16,23:37:12.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37983 2023-02-16,23:37:12.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
37152 2023-02-16,23:37:12.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.401538372039795
37393 2023-02-16,23:37:12.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
37533 2023-02-16,23:37:12.817 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
38108 2023-02-16,23:37:12.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:37:12.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3966904878616333
37035 2023-02-16,23:37:12.831 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3857332468032837
37268 2023-02-16,23:37:12.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4011328220367432
37757 2023-02-16,23:37:12.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37862 2023-02-16,23:37:12.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37983 2023-02-16,23:37:12.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36932 2023-02-16,23:37:12.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3832027912139893
37152 2023-02-16,23:37:12.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3850847482681274
37393 2023-02-16,23:37:13.026 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37533 2023-02-16,23:37:13.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
38108 2023-02-16,23:37:13.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:37:13.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4169517755508423
37035 2023-02-16,23:37:13.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4063763618469238
37268 2023-02-16,23:37:13.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4173606634140015
37862 2023-02-16,23:37:13.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37983 2023-02-16,23:37:13.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36932 2023-02-16,23:37:13.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3724653720855713
37757 2023-02-16,23:37:13.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37152 2023-02-16,23:37:13.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4448286294937134
37393 2023-02-16,23:37:13.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37533 2023-02-16,23:37:13.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
38108 2023-02-16,23:37:13.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:37:13.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3761160373687744
37862 2023-02-16,23:37:13.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37983 2023-02-16,23:37:13.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
36932 2023-02-16,23:37:13.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3819873332977295
37035 2023-02-16,23:37:13.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3903828859329224
37268 2023-02-16,23:37:13.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3957951068878174
37757 2023-02-16,23:37:13.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
37152 2023-02-16,23:37:13.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.412183403968811
37393 2023-02-16,23:37:13.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37533 2023-02-16,23:37:13.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
38108 2023-02-16,23:37:13.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:37:13.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3648180961608887
36932 2023-02-16,23:37:13.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4116578102111816
37035 2023-02-16,23:37:13.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3846136331558228
37268 2023-02-16,23:37:13.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3366098403930664
37757 2023-02-16,23:37:13.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
37862 2023-02-16,23:37:13.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
37983 2023-02-16,23:37:13.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37152 2023-02-16,23:37:13.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4299564361572266
37393 2023-02-16,23:37:13.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
37533 2023-02-16,23:37:13.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
36814 2023-02-16,23:37:13.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.388103723526001
36932 2023-02-16,23:37:13.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3559679985046387
37035 2023-02-16,23:37:13.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3629179000854492
37268 2023-02-16,23:37:13.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3640706539154053
37757 2023-02-16,23:37:13.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
37862 2023-02-16,23:37:13.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37983 2023-02-16,23:37:13.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
38108 2023-02-16,23:37:13.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37152 2023-02-16,23:37:13.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.398348093032837
36814 2023-02-16,23:37:13.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3636901378631592
36932 2023-02-16,23:37:13.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4521620273590088
37152 2023-02-16,23:37:13.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4520474672317505
37393 2023-02-16,23:37:13.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37533 2023-02-16,23:37:13.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
37757 2023-02-16,23:37:13.902 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37983 2023-02-16,23:37:13.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
38108 2023-02-16,23:37:13.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37035 2023-02-16,23:37:13.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3561910390853882
37268 2023-02-16,23:37:13.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.345934271812439
37862 2023-02-16,23:37:13.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37152 2023-02-16,23:37:14.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.438592791557312
37393 2023-02-16,23:37:14.105 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
37533 2023-02-16,23:37:14.106 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37757 2023-02-16,23:37:14.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
37983 2023-02-16,23:37:14.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36814 2023-02-16,23:37:14.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3723117113113403
36932 2023-02-16,23:37:14.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3820877075195312
37268 2023-02-16,23:37:14.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3499507904052734
37862 2023-02-16,23:37:14.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
38108 2023-02-16,23:37:14.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37035 2023-02-16,23:37:14.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3650553226470947
37393 2023-02-16,23:37:14.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37533 2023-02-16,23:37:14.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37757 2023-02-16,23:37:14.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
36814 2023-02-16,23:37:14.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3548953533172607
36932 2023-02-16,23:37:14.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4065219163894653
37035 2023-02-16,23:37:14.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3593381643295288
37152 2023-02-16,23:37:14.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4011502265930176
37268 2023-02-16,23:37:14.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.388310432434082
37862 2023-02-16,23:37:14.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
37983 2023-02-16,23:37:14.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
38108 2023-02-16,23:37:14.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
37035 2023-02-16,23:37:14.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3698002099990845
37393 2023-02-16,23:37:14.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
37533 2023-02-16,23:37:14.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37757 2023-02-16,23:37:14.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
37983 2023-02-16,23:37:14.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
38108 2023-02-16,23:37:14.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36814 2023-02-16,23:37:14.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372108817100525
36932 2023-02-16,23:37:14.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3939694166183472
37152 2023-02-16,23:37:14.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.392416000366211
37268 2023-02-16,23:37:14.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3439335823059082
37862 2023-02-16,23:37:14.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
37152 2023-02-16,23:37:14.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4069489240646362
37533 2023-02-16,23:37:14.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37983 2023-02-16,23:37:14.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37268 2023-02-16,23:37:14.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3668867349624634
37393 2023-02-16,23:37:14.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37757 2023-02-16,23:37:14.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
38108 2023-02-16,23:37:14.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:37:14.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3597110509872437
36932 2023-02-16,23:37:14.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.424127221107483
37035 2023-02-16,23:37:14.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3863039016723633
37862 2023-02-16,23:37:14.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
37152 2023-02-16,23:37:14.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4124270677566528
37533 2023-02-16,23:37:14.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
37983 2023-02-16,23:37:14.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37268 2023-02-16,23:37:14.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4133158922195435
37393 2023-02-16,23:37:14.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37757 2023-02-16,23:37:14.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38108 2023-02-16,23:37:14.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:37:14.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4098927974700928
36932 2023-02-16,23:37:14.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.3992403745651245
37862 2023-02-16,23:37:14.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
37035 2023-02-16,23:37:14.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4585988521575928
37152 2023-02-16,23:37:15.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.374894618988037
37533 2023-02-16,23:37:15.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37983 2023-02-16,23:37:15.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37757 2023-02-16,23:37:15.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
38108 2023-02-16,23:37:15.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:37:15.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4205663204193115
36932 2023-02-16,23:37:15.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3966400623321533
37035 2023-02-16,23:37:15.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3985965251922607
37268 2023-02-16,23:37:15.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3543622493743896
37862 2023-02-16,23:37:15.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37393 2023-02-16,23:37:15.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37533 2023-02-16,23:37:15.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37152 2023-02-16,23:37:15.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.388512372970581
37983 2023-02-16,23:37:15.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
37268 2023-02-16,23:37:15.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.358686923980713
37757 2023-02-16,23:37:15.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
37862 2023-02-16,23:37:15.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
38108 2023-02-16,23:37:15.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:37:15.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4408448934555054
36932 2023-02-16,23:37:15.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.422194480895996
37035 2023-02-16,23:37:15.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3966902494430542
37393 2023-02-16,23:37:15.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3942652940750122
37533 2023-02-16,23:37:15.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37152 2023-02-16,23:37:15.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3733159303665161
37393 2023-02-16,23:37:15.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.411234974861145
37983 2023-02-16,23:37:15.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
38108 2023-02-16,23:37:15.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
37035 2023-02-16,23:37:15.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3404040336608887
37268 2023-02-16,23:37:15.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.384205937385559
37757 2023-02-16,23:37:15.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37862 2023-02-16,23:37:15.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36814 2023-02-16,23:37:15.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3694924116134644
36932 2023-02-16,23:37:15.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3874526023864746
37152 2023-02-16,23:37:15.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3934483528137207
37983 2023-02-16,23:37:15.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37393 2023-02-16,23:37:15.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4228297472000122
37533 2023-02-16,23:37:15.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
38108 2023-02-16,23:37:15.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37035 2023-02-16,23:37:15.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4157228469848633
37268 2023-02-16,23:37:15.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3563973903656006
37862 2023-02-16,23:37:15.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37757 2023-02-16,23:37:15.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36814 2023-02-16,23:37:15.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3975130319595337
36932 2023-02-16,23:37:15.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3825217485427856
37983 2023-02-16,23:37:15.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37533 2023-02-16,23:37:16.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
36814 2023-02-16,23:37:16.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4129308462142944
37268 2023-02-16,23:37:16.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3673549890518188
37393 2023-02-16,23:37:16.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3741276264190674
37757 2023-02-16,23:37:16.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
37862 2023-02-16,23:37:16.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
38108 2023-02-16,23:37:16.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37035 2023-02-16,23:37:16.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3835543394088745
37152 2023-02-16,23:37:16.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4037597179412842
36932 2023-02-16,23:37:16.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4222440719604492
37983 2023-02-16,23:37:16.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37533 2023-02-16,23:37:16.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
36814 2023-02-16,23:37:16.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4040447473526
37268 2023-02-16,23:37:16.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3604543209075928
37393 2023-02-16,23:37:16.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.370268702507019
37757 2023-02-16,23:37:16.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
37862 2023-02-16,23:37:16.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
38108 2023-02-16,23:37:16.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36932 2023-02-16,23:37:16.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4090744256973267
37035 2023-02-16,23:37:16.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4122538566589355
37152 2023-02-16,23:37:16.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3706687688827515
37533 2023-02-16,23:37:16.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37983 2023-02-16,23:37:16.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
38108 2023-02-16,23:37:16.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36814 2023-02-16,23:37:16.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4007737636566162
37268 2023-02-16,23:37:16.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4199975728988647
37393 2023-02-16,23:37:16.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.399355173110962
37757 2023-02-16,23:37:16.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37862 2023-02-16,23:37:16.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
36932 2023-02-16,23:37:16.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4063867330551147
37035 2023-02-16,23:37:16.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3536944389343262
37152 2023-02-16,23:37:16.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3948357105255127
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
38255 2023-02-16,23:37:16.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37533 2023-02-16,23:37:16.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37393 2023-02-16,23:37:16.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4015185832977295
37983 2023-02-16,23:37:16.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
36814 2023-02-16,23:37:16.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3962767124176025
37152 2023-02-16,23:37:16.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.360054612159729
37268 2023-02-16,23:37:16.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4319576025009155
37757 2023-02-16,23:37:16.691 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37862 2023-02-16,23:37:16.691 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
38108 2023-02-16,23:37:16.691 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
37035 2023-02-16,23:37:16.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3790279626846313
36932 2023-02-16,23:37:16.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.401718020439148
38255 2023-02-16,23:37:16.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37533 2023-02-16,23:37:16.883 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37983 2023-02-16,23:37:16.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37393 2023-02-16,23:37:16.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3840508460998535
36814 2023-02-16,23:37:16.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.440758466720581
37152 2023-02-16,23:37:16.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3862152099609375
37268 2023-02-16,23:37:16.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3957570791244507
37757 2023-02-16,23:37:16.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
37862 2023-02-16,23:37:16.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
38108 2023-02-16,23:37:16.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37035 2023-02-16,23:37:16.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3475006818771362
36932 2023-02-16,23:37:16.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4171233177185059
38255 2023-02-16,23:37:16.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37533 2023-02-16,23:37:17.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37983 2023-02-16,23:37:17.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
36814 2023-02-16,23:37:17.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3969672918319702
37268 2023-02-16,23:37:17.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3701987266540527
37393 2023-02-16,23:37:17.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3782298564910889
37152 2023-02-16,23:37:17.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4179383516311646
37757 2023-02-16,23:37:17.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37862 2023-02-16,23:37:17.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
38108 2023-02-16,23:37:17.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36932 2023-02-16,23:37:17.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3657197952270508
37035 2023-02-16,23:37:17.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4006396532058716
38255 2023-02-16,23:37:17.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37533 2023-02-16,23:37:17.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37393 2023-02-16,23:37:17.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4413366317749023
37983 2023-02-16,23:37:17.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36814 2023-02-16,23:37:17.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3491429090499878
37268 2023-02-16,23:37:17.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4279327392578125
37152 2023-02-16,23:37:17.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4199329614639282
37757 2023-02-16,23:37:17.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
37862 2023-02-16,23:37:17.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
38108 2023-02-16,23:37:17.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36932 2023-02-16,23:37:17.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.408352255821228
37035 2023-02-16,23:37:17.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3736612796783447
38255 2023-02-16,23:37:17.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
37533 2023-02-16,23:37:17.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37983 2023-02-16,23:37:17.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37268 2023-02-16,23:37:17.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4241095781326294
37393 2023-02-16,23:37:17.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3564406633377075
38108 2023-02-16,23:37:17.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:37:17.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.34749436378479
37152 2023-02-16,23:37:17.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3540247678756714
37757 2023-02-16,23:37:17.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
37862 2023-02-16,23:37:17.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
38255 2023-02-16,23:37:17.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
36932 2023-02-16,23:37:17.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4081376791000366
37035 2023-02-16,23:37:17.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4613583087921143
37533 2023-02-16,23:37:17.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37983 2023-02-16,23:37:17.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37393 2023-02-16,23:37:17.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4243110418319702
38108 2023-02-16,23:37:17.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
38255 2023-02-16,23:37:17.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
36814 2023-02-16,23:37:17.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4189738035202026
37152 2023-02-16,23:37:17.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.424211025238037
37268 2023-02-16,23:37:17.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3958640098571777
37757 2023-02-16,23:37:17.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
37862 2023-02-16,23:37:17.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36932 2023-02-16,23:37:17.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4158101081848145
37035 2023-02-16,23:37:17.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3463908433914185
37533 2023-02-16,23:37:17.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37268 2023-02-16,23:37:18.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4123485088348389
37393 2023-02-16,23:37:18.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.392768144607544
37983 2023-02-16,23:37:18.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
38255 2023-02-16,23:37:18.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:37:18.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4302901029586792
37152 2023-02-16,23:37:18.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3895206451416016
37757 2023-02-16,23:37:18.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
37862 2023-02-16,23:37:18.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38108 2023-02-16,23:37:18.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
36932 2023-02-16,23:37:18.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.392155408859253
37035 2023-02-16,23:37:18.089 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.407604694366455
37533 2023-02-16,23:37:18.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37268 2023-02-16,23:37:18.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3715450763702393
37393 2023-02-16,23:37:18.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.401072382926941
37983 2023-02-16,23:37:18.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
38255 2023-02-16,23:37:18.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:37:18.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3867888450622559
37152 2023-02-16,23:37:18.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4001399278640747
37757 2023-02-16,23:37:18.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
37862 2023-02-16,23:37:18.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
38108 2023-02-16,23:37:18.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36932 2023-02-16,23:37:18.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3318289518356323
37035 2023-02-16,23:37:18.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.401895523071289
37533 2023-02-16,23:37:18.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37268 2023-02-16,23:37:18.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3748451471328735
37393 2023-02-16,23:37:18.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4178392887115479
37983 2023-02-16,23:37:18.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
38255 2023-02-16,23:37:18.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:37:18.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4302923679351807
37152 2023-02-16,23:37:18.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4058645963668823
37757 2023-02-16,23:37:18.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
37862 2023-02-16,23:37:18.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
38108 2023-02-16,23:37:18.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36932 2023-02-16,23:37:18.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.388385534286499
37035 2023-02-16,23:37:18.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3904615640640259
37533 2023-02-16,23:37:18.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37393 2023-02-16,23:37:18.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3961918354034424
37983 2023-02-16,23:37:18.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
38255 2023-02-16,23:37:18.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:37:18.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3573061227798462
37152 2023-02-16,23:37:18.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4314773082733154
37268 2023-02-16,23:37:18.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.390134572982788
37533 2023-02-16,23:37:18.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37757 2023-02-16,23:37:18.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
37862 2023-02-16,23:37:18.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
38108 2023-02-16,23:37:18.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36932 2023-02-16,23:37:18.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.390507459640503
37035 2023-02-16,23:37:18.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4030165672302246
37533 2023-02-16,23:37:18.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37983 2023-02-16,23:37:18.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
36814 2023-02-16,23:37:18.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3843843936920166
37035 2023-02-16,23:37:18.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3360342979431152
37152 2023-02-16,23:37:18.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4237163066864014
37393 2023-02-16,23:37:19.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3373526334762573
37862 2023-02-16,23:37:19.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
38108 2023-02-16,23:37:19.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
38255 2023-02-16,23:37:19.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
36932 2023-02-16,23:37:19.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.424504041671753
37268 2023-02-16,23:37:19.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3854519128799438
37757 2023-02-16,23:37:19.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
37533 2023-02-16,23:37:19.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37035 2023-02-16,23:37:19.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.373378038406372
37983 2023-02-16,23:37:19.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
38255 2023-02-16,23:37:19.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
36814 2023-02-16,23:37:19.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3769043684005737
37152 2023-02-16,23:37:19.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.370192527770996
37268 2023-02-16,23:37:19.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.401865005493164
37393 2023-02-16,23:37:19.237 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.363745093345642
37757 2023-02-16,23:37:19.237 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37862 2023-02-16,23:37:19.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
38108 2023-02-16,23:37:19.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36932 2023-02-16,23:37:19.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4028635025024414
37035 2023-02-16,23:37:19.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.383273959159851
37533 2023-02-16,23:37:19.445 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37757 2023-02-16,23:37:19.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37983 2023-02-16,23:37:19.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
38255 2023-02-16,23:37:19.454 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37393 2023-02-16,23:37:19.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3465685844421387
37862 2023-02-16,23:37:19.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
38108 2023-02-16,23:37:19.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:37:19.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.377610683441162
36932 2023-02-16,23:37:19.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4083669185638428
37152 2023-02-16,23:37:19.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3921704292297363
37268 2023-02-16,23:37:19.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4068946838378906
37035 2023-02-16,23:37:19.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3720835447311401
37393 2023-02-16,23:37:19.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3514994382858276
37533 2023-02-16,23:37:19.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37983 2023-02-16,23:37:19.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
38108 2023-02-16,23:37:19.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
38255 2023-02-16,23:37:19.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36814 2023-02-16,23:37:19.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3908355236053467
37152 2023-02-16,23:37:19.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3868454694747925
37268 2023-02-16,23:37:19.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3974629640579224
37757 2023-02-16,23:37:19.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37862 2023-02-16,23:37:19.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36932 2023-02-16,23:37:19.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3887099027633667
38255 2023-02-16,23:37:19.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
37035 2023-02-16,23:37:19.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3823747634887695
37152 2023-02-16,23:37:19.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3497159481048584
37268 2023-02-16,23:37:19.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4271578788757324
37393 2023-02-16,23:37:19.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3882933855056763
37757 2023-02-16,23:37:19.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37862 2023-02-16,23:37:19.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
37983 2023-02-16,23:37:19.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
38108 2023-02-16,23:37:19.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37533 2023-02-16,23:37:19.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
36814 2023-02-16,23:37:19.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3951936960220337
36932 2023-02-16,23:37:19.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3790408372879028
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
38362 2023-02-16,23:37:19.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
38255 2023-02-16,23:37:20.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37035 2023-02-16,23:37:20.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.412003755569458
37152 2023-02-16,23:37:20.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4113863706588745
37268 2023-02-16,23:37:20.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4384193420410156
37393 2023-02-16,23:37:20.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3442822694778442
37533 2023-02-16,23:37:20.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
37757 2023-02-16,23:37:20.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
37862 2023-02-16,23:37:20.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
37983 2023-02-16,23:37:20.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
38108 2023-02-16,23:37:20.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
36814 2023-02-16,23:37:20.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.365347981452942
36932 2023-02-16,23:37:20.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3493245840072632
38362 2023-02-16,23:37:20.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37393 2023-02-16,23:37:20.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3681234121322632
38255 2023-02-16,23:37:20.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
37035 2023-02-16,23:37:20.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.35613214969635
37152 2023-02-16,23:37:20.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4391225576400757
37268 2023-02-16,23:37:20.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4183332920074463
37533 2023-02-16,23:37:20.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
37757 2023-02-16,23:37:20.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37862 2023-02-16,23:37:20.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
37983 2023-02-16,23:37:20.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
38108 2023-02-16,23:37:20.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
38362 2023-02-16,23:37:20.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
36814 2023-02-16,23:37:20.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4070651531219482
36932 2023-02-16,23:37:20.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.381090760231018
37393 2023-02-16,23:37:20.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4132792949676514
38255 2023-02-16,23:37:20.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
37035 2023-02-16,23:37:20.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4542388916015625
37533 2023-02-16,23:37:20.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
38362 2023-02-16,23:37:20.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
36814 2023-02-16,23:37:20.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3833504915237427
36932 2023-02-16,23:37:20.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.40202796459198
37268 2023-02-16,23:37:20.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4265037775039673
37152 2023-02-16,23:37:20.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3868749141693115
37757 2023-02-16,23:37:20.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37862 2023-02-16,23:37:20.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
37983 2023-02-16,23:37:20.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
38108 2023-02-16,23:37:20.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
37393 2023-02-16,23:37:20.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3536914587020874
38255 2023-02-16,23:37:20.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
37035 2023-02-16,23:37:20.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3821877241134644
37533 2023-02-16,23:37:20.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
38362 2023-02-16,23:37:20.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
36814 2023-02-16,23:37:20.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3933149576187134
36932 2023-02-16,23:37:20.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3617463111877441
37268 2023-02-16,23:37:20.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3943644762039185
37152 2023-02-16,23:37:20.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3514586687088013
37862 2023-02-16,23:37:20.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37757 2023-02-16,23:37:20.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37983 2023-02-16,23:37:20.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
38108 2023-02-16,23:37:20.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
38255 2023-02-16,23:37:21.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
37393 2023-02-16,23:37:21.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3592076301574707
38108 2023-02-16,23:37:21.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
38362 2023-02-16,23:37:21.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37035 2023-02-16,23:37:21.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.407301902770996
37533 2023-02-16,23:37:21.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37757 2023-02-16,23:37:21.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37862 2023-02-16,23:37:21.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
37983 2023-02-16,23:37:21.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
36814 2023-02-16,23:37:21.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4010555744171143
36932 2023-02-16,23:37:21.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.3956035375595093
37152 2023-02-16,23:37:21.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4117765426635742
37268 2023-02-16,23:37:21.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4030598402023315
37393 2023-02-16,23:37:21.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.384445071220398
38255 2023-02-16,23:37:21.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37035 2023-02-16,23:37:21.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3945626020431519
38108 2023-02-16,23:37:21.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
38362 2023-02-16,23:37:21.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37152 2023-02-16,23:37:21.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3987561464309692
37268 2023-02-16,23:37:21.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4422566890716553
37533 2023-02-16,23:37:21.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37757 2023-02-16,23:37:21.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
37862 2023-02-16,23:37:21.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37983 2023-02-16,23:37:21.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
36814 2023-02-16,23:37:21.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.42906653881073
36932 2023-02-16,23:37:21.425 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3784844875335693
37393 2023-02-16,23:37:21.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3570525646209717
38255 2023-02-16,23:37:21.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37035 2023-02-16,23:37:21.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4250080585479736
37533 2023-02-16,23:37:21.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37757 2023-02-16,23:37:21.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38108 2023-02-16,23:37:21.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
38362 2023-02-16,23:37:21.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:37:21.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3800437450408936
36932 2023-02-16,23:37:21.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.391854166984558
37152 2023-02-16,23:37:21.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.412093997001648
37268 2023-02-16,23:37:21.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3769434690475464
37862 2023-02-16,23:37:21.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
37983 2023-02-16,23:37:21.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37393 2023-02-16,23:37:21.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3676369190216064
37533 2023-02-16,23:37:21.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
38255 2023-02-16,23:37:21.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
38362 2023-02-16,23:37:21.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37035 2023-02-16,23:37:21.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.3994063138961792
37152 2023-02-16,23:37:21.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4106512069702148
37268 2023-02-16,23:37:21.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.407543659210205
37757 2023-02-16,23:37:21.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37862 2023-02-16,23:37:21.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
37983 2023-02-16,23:37:21.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
38108 2023-02-16,23:37:21.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
36814 2023-02-16,23:37:21.919 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4198570251464844
36932 2023-02-16,23:37:21.919 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4169909954071045
38255 2023-02-16,23:37:22.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37035 2023-02-16,23:37:22.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.396036148071289
37152 2023-02-16,23:37:22.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.344677448272705
37393 2023-02-16,23:37:22.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.361231803894043
37533 2023-02-16,23:37:22.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
38362 2023-02-16,23:37:22.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
37268 2023-02-16,23:37:22.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3659420013427734
37757 2023-02-16,23:37:22.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37862 2023-02-16,23:37:22.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
37983 2023-02-16,23:37:22.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
38108 2023-02-16,23:37:22.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36814 2023-02-16,23:37:22.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3778574466705322
36932 2023-02-16,23:37:22.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4011831283569336
38255 2023-02-16,23:37:22.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
37035 2023-02-16,23:37:22.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.423384428024292
37533 2023-02-16,23:37:22.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
38362 2023-02-16,23:37:22.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:37:22.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.39694082736969
36932 2023-02-16,23:37:22.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3918426036834717
37268 2023-02-16,23:37:22.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3702794313430786
37393 2023-02-16,23:37:22.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4208598136901855
37757 2023-02-16,23:37:22.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37862 2023-02-16,23:37:22.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37983 2023-02-16,23:37:22.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
38108 2023-02-16,23:37:22.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37152 2023-02-16,23:37:22.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.37870454788208
38255 2023-02-16,23:37:22.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
38362 2023-02-16,23:37:22.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37035 2023-02-16,23:37:22.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3868119716644287
37152 2023-02-16,23:37:22.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3741437196731567
37268 2023-02-16,23:37:22.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3971742391586304
37393 2023-02-16,23:37:22.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4321519136428833
37533 2023-02-16,23:37:22.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37757 2023-02-16,23:37:22.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37862 2023-02-16,23:37:22.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37983 2023-02-16,23:37:22.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
38108 2023-02-16,23:37:22.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36814 2023-02-16,23:37:22.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.355365514755249
36932 2023-02-16,23:37:22.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3972190618515015
37152 2023-02-16,23:37:22.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.345383644104004
38255 2023-02-16,23:37:22.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
38362 2023-02-16,23:37:22.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37035 2023-02-16,23:37:22.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3836504220962524
37268 2023-02-16,23:37:22.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3798048496246338
37393 2023-02-16,23:37:22.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.396926760673523
37533 2023-02-16,23:37:22.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37757 2023-02-16,23:37:22.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37862 2023-02-16,23:37:22.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
37983 2023-02-16,23:37:22.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
36814 2023-02-16,23:37:22.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.422749638557434
36932 2023-02-16,23:37:22.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3856953382492065
38108 2023-02-16,23:37:22.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37152 2023-02-16,23:37:23.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.399224877357483
38255 2023-02-16,23:37:23.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
38362 2023-02-16,23:37:23.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37035 2023-02-16,23:37:23.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4236540794372559
37268 2023-02-16,23:37:23.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3647546768188477
37393 2023-02-16,23:37:23.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3702430725097656
37533 2023-02-16,23:37:23.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37757 2023-02-16,23:37:23.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37862 2023-02-16,23:37:23.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37983 2023-02-16,23:37:23.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
36814 2023-02-16,23:37:23.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3809452056884766
36932 2023-02-16,23:37:23.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3704092502593994
38108 2023-02-16,23:37:23.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
38255 2023-02-16,23:37:23.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
37035 2023-02-16,23:37:23.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4106411933898926
37152 2023-02-16,23:37:23.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3881158828735352
37393 2023-02-16,23:37:23.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.428318977355957
37533 2023-02-16,23:37:23.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37757 2023-02-16,23:37:23.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37983 2023-02-16,23:37:23.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
38362 2023-02-16,23:37:23.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36932 2023-02-16,23:37:23.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4046905040740967
37268 2023-02-16,23:37:23.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3577977418899536
37862 2023-02-16,23:37:23.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
38108 2023-02-16,23:37:23.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
36814 2023-02-16,23:37:23.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3953633308410645
38255 2023-02-16,23:37:23.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
37035 2023-02-16,23:37:23.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.407429575920105
37152 2023-02-16,23:37:23.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3494288921356201
37393 2023-02-16,23:37:23.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4249261617660522
37533 2023-02-16,23:37:23.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37757 2023-02-16,23:37:23.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37983 2023-02-16,23:37:23.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38362 2023-02-16,23:37:23.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
38108 2023-02-16,23:37:23.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
36932 2023-02-16,23:37:23.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3925012350082397
37268 2023-02-16,23:37:23.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3529736995697021
37862 2023-02-16,23:37:23.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
36814 2023-02-16,23:37:23.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3944751024246216
37533 2023-02-16,23:37:23.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
38255 2023-02-16,23:37:23.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37152 2023-02-16,23:37:23.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.359646201133728
37268 2023-02-16,23:37:23.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.430199384689331
37393 2023-02-16,23:37:23.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3968522548675537
37757 2023-02-16,23:37:23.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37862 2023-02-16,23:37:23.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
37983 2023-02-16,23:37:23.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
38108 2023-02-16,23:37:23.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
38362 2023-02-16,23:37:23.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36814 2023-02-16,23:37:23.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3849616050720215
36932 2023-02-16,23:37:23.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3727195262908936
37035 2023-02-16,23:37:23.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4020717144012451
38255 2023-02-16,23:37:24.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37152 2023-02-16,23:37:24.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3676401376724243
37393 2023-02-16,23:37:24.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4125525951385498
37533 2023-02-16,23:37:24.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37757 2023-02-16,23:37:24.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
38108 2023-02-16,23:37:24.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
38362 2023-02-16,23:37:24.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:37:24.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4053723812103271
37268 2023-02-16,23:37:24.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4319504499435425
37862 2023-02-16,23:37:24.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
37983 2023-02-16,23:37:24.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36932 2023-02-16,23:37:24.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3542308807373047
37035 2023-02-16,23:37:24.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4178217649459839
38255 2023-02-16,23:37:24.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
37152 2023-02-16,23:37:24.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4000892639160156
37393 2023-02-16,23:37:24.382 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.371520757675171
37757 2023-02-16,23:37:24.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
38108 2023-02-16,23:37:24.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
38362 2023-02-16,23:37:24.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36814 2023-02-16,23:37:24.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.403733253479004
36932 2023-02-16,23:37:24.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.382411003112793
37035 2023-02-16,23:37:24.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3652433156967163
37268 2023-02-16,23:37:24.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3832253217697144
37533 2023-02-16,23:37:24.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37862 2023-02-16,23:37:24.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
37983 2023-02-16,23:37:24.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
38255 2023-02-16,23:37:24.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
37152 2023-02-16,23:37:24.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3860528469085693
37393 2023-02-16,23:37:24.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3750195503234863
37533 2023-02-16,23:37:24.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37757 2023-02-16,23:37:24.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
38108 2023-02-16,23:37:24.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
38362 2023-02-16,23:37:24.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:37:24.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4139199256896973
36932 2023-02-16,23:37:24.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4388353824615479
37035 2023-02-16,23:37:24.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4089077711105347
37268 2023-02-16,23:37:24.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3724595308303833
37862 2023-02-16,23:37:24.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
37983 2023-02-16,23:37:24.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
36814 2023-02-16,23:37:24.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4105677604675293
37035 2023-02-16,23:37:24.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.407515525817871
37152 2023-02-16,23:37:24.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4545269012451172
37393 2023-02-16,23:37:24.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3914152383804321
37757 2023-02-16,23:37:24.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
38255 2023-02-16,23:37:24.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36932 2023-02-16,23:37:24.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3710269927978516
37268 2023-02-16,23:37:24.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.398899793624878
37533 2023-02-16,23:37:24.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37862 2023-02-16,23:37:24.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
37983 2023-02-16,23:37:24.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
38108 2023-02-16,23:37:24.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
38362 2023-02-16,23:37:24.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36814 2023-02-16,23:37:25.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.422394037246704
37035 2023-02-16,23:37:25.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4147753715515137
37152 2023-02-16,23:37:25.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4058223962783813
37393 2023-02-16,23:37:25.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3860836029052734
37533 2023-02-16,23:37:25.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
38362 2023-02-16,23:37:25.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37268 2023-02-16,23:37:25.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4031699895858765
37757 2023-02-16,23:37:25.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37862 2023-02-16,23:37:25.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
37983 2023-02-16,23:37:25.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
38108 2023-02-16,23:37:25.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
38255 2023-02-16,23:37:25.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36932 2023-02-16,23:37:25.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.391568660736084
36814 2023-02-16,23:37:25.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3800954818725586
37035 2023-02-16,23:37:25.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.390385627746582
37152 2023-02-16,23:37:25.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3846399784088135
37393 2023-02-16,23:37:25.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4028888940811157
38255 2023-02-16,23:37:25.379 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
38362 2023-02-16,23:37:25.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37268 2023-02-16,23:37:25.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3812565803527832
37533 2023-02-16,23:37:25.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37757 2023-02-16,23:37:25.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37862 2023-02-16,23:37:25.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37983 2023-02-16,23:37:25.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
38108 2023-02-16,23:37:25.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36932 2023-02-16,23:37:25.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3242518901824951
36814 2023-02-16,23:37:25.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4072355031967163
37393 2023-02-16,23:37:25.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4080901145935059
37533 2023-02-16,23:37:25.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37757 2023-02-16,23:37:25.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
38362 2023-02-16,23:37:25.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36932 2023-02-16,23:37:25.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.372238278388977
37035 2023-02-16,23:37:25.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3313747644424438
37268 2023-02-16,23:37:25.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3925784826278687
37862 2023-02-16,23:37:25.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37983 2023-02-16,23:37:25.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
38108 2023-02-16,23:37:25.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
38255 2023-02-16,23:37:25.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37152 2023-02-16,23:37:25.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3907848596572876
36814 2023-02-16,23:37:25.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3778363466262817
38362 2023-02-16,23:37:25.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36932 2023-02-16,23:37:25.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.415927529335022
37035 2023-02-16,23:37:25.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3893301486968994
37268 2023-02-16,23:37:25.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.397385597229004
37393 2023-02-16,23:37:25.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3974660634994507
37533 2023-02-16,23:37:25.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37757 2023-02-16,23:37:25.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37862 2023-02-16,23:37:25.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37983 2023-02-16,23:37:25.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
38108 2023-02-16,23:37:25.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
37152 2023-02-16,23:37:25.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3389214277267456
38255 2023-02-16,23:37:25.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36814 2023-02-16,23:37:26.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3763155937194824
37268 2023-02-16,23:37:26.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4337763786315918
37393 2023-02-16,23:37:26.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.427809238433838
37533 2023-02-16,23:37:26.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37757 2023-02-16,23:37:26.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
37862 2023-02-16,23:37:26.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
38362 2023-02-16,23:37:26.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36932 2023-02-16,23:37:26.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3432300090789795
37035 2023-02-16,23:37:26.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3902335166931152
37152 2023-02-16,23:37:26.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3762589693069458
37983 2023-02-16,23:37:26.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
38255 2023-02-16,23:37:26.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
38108 2023-02-16,23:37:26.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
36814 2023-02-16,23:37:26.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3817973136901855
37393 2023-02-16,23:37:26.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4401838779449463
37533 2023-02-16,23:37:26.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37757 2023-02-16,23:37:26.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
38362 2023-02-16,23:37:26.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37035 2023-02-16,23:37:26.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.425192952156067
37152 2023-02-16,23:37:26.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4089657068252563
37268 2023-02-16,23:37:26.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4003405570983887
37862 2023-02-16,23:37:26.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
37983 2023-02-16,23:37:26.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38255 2023-02-16,23:37:26.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
38108 2023-02-16,23:37:26.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36932 2023-02-16,23:37:26.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3763772249221802
36814 2023-02-16,23:37:26.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3279880285263062
37393 2023-02-16,23:37:26.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4191644191741943
37533 2023-02-16,23:37:26.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
38362 2023-02-16,23:37:26.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
37035 2023-02-16,23:37:26.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.402441382408142
37268 2023-02-16,23:37:26.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.385449767112732
37862 2023-02-16,23:37:26.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37983 2023-02-16,23:37:26.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
38255 2023-02-16,23:37:26.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36932 2023-02-16,23:37:26.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3786919116973877
37152 2023-02-16,23:37:26.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3801085948944092
37757 2023-02-16,23:37:26.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
38108 2023-02-16,23:37:26.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37393 2023-02-16,23:37:26.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4261481761932373
38362 2023-02-16,23:37:26.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
37268 2023-02-16,23:37:26.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4452283382415771
37757 2023-02-16,23:37:26.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37862 2023-02-16,23:37:26.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37983 2023-02-16,23:37:26.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
38255 2023-02-16,23:37:26.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36814 2023-02-16,23:37:26.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.376837134361267
36932 2023-02-16,23:37:26.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3760957717895508
37035 2023-02-16,23:37:26.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4087594747543335
37152 2023-02-16,23:37:26.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3828434944152832
37533 2023-02-16,23:37:26.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
38108 2023-02-16,23:37:26.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
38487 2023-02-16,23:37:27.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37393 2023-02-16,23:37:27.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.394104242324829
37533 2023-02-16,23:37:27.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37757 2023-02-16,23:37:27.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37862 2023-02-16,23:37:27.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37983 2023-02-16,23:37:27.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
38255 2023-02-16,23:37:27.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
38362 2023-02-16,23:37:27.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:37:27.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3479852676391602
36932 2023-02-16,23:37:27.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4116817712783813
37035 2023-02-16,23:37:27.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3876863718032837
37152 2023-02-16,23:37:27.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.376556158065796
37268 2023-02-16,23:37:27.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4099953174591064
38108 2023-02-16,23:37:27.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
38487 2023-02-16,23:37:27.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37533 2023-02-16,23:37:27.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37757 2023-02-16,23:37:27.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37862 2023-02-16,23:37:27.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37393 2023-02-16,23:37:27.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4035935401916504
37983 2023-02-16,23:37:27.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
38255 2023-02-16,23:37:27.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
38362 2023-02-16,23:37:27.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:37:27.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4001749753952026
36932 2023-02-16,23:37:27.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.380221962928772
37035 2023-02-16,23:37:27.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3812689781188965
37152 2023-02-16,23:37:27.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3819999694824219
37268 2023-02-16,23:37:27.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4315376281738281
38108 2023-02-16,23:37:27.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
38487 2023-02-16,23:37:27.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
38362 2023-02-16,23:37:27.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37035 2023-02-16,23:37:27.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3481485843658447
37393 2023-02-16,23:37:27.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4437463283538818
37533 2023-02-16,23:37:27.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37757 2023-02-16,23:37:27.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37862 2023-02-16,23:37:27.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
37983 2023-02-16,23:37:27.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
38108 2023-02-16,23:37:27.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
38255 2023-02-16,23:37:27.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
36814 2023-02-16,23:37:27.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4179316759109497
36932 2023-02-16,23:37:27.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4469468593597412
37152 2023-02-16,23:37:27.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4293190240859985
37268 2023-02-16,23:37:27.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3973751068115234
38487 2023-02-16,23:37:27.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37533 2023-02-16,23:37:27.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
36814 2023-02-16,23:37:27.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4105128049850464
37035 2023-02-16,23:37:27.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.380798101425171
37393 2023-02-16,23:37:27.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3766522407531738
37757 2023-02-16,23:37:27.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37862 2023-02-16,23:37:27.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
37983 2023-02-16,23:37:27.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
38108 2023-02-16,23:37:27.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
38255 2023-02-16,23:37:27.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
38362 2023-02-16,23:37:27.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
36932 2023-02-16,23:37:27.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4448022842407227
37152 2023-02-16,23:37:27.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3794727325439453
37268 2023-02-16,23:37:27.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4527714252471924
38487 2023-02-16,23:37:27.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
37533 2023-02-16,23:37:28.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38362 2023-02-16,23:37:28.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:37:28.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3947445154190063
37035 2023-02-16,23:37:28.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4015557765960693
37152 2023-02-16,23:37:28.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3368743658065796
37393 2023-02-16,23:37:28.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4070688486099243
37757 2023-02-16,23:37:28.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37862 2023-02-16,23:37:28.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
37983 2023-02-16,23:37:28.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
38108 2023-02-16,23:37:28.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
38255 2023-02-16,23:37:28.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36932 2023-02-16,23:37:28.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3429244756698608
37268 2023-02-16,23:37:28.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.438020944595337
38487 2023-02-16,23:37:28.219 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37393 2023-02-16,23:37:28.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3659919500350952
37035 2023-02-16,23:37:28.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3618396520614624
37533 2023-02-16,23:37:28.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37757 2023-02-16,23:37:28.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37862 2023-02-16,23:37:28.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
37983 2023-02-16,23:37:28.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
38108 2023-02-16,23:37:28.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
38255 2023-02-16,23:37:28.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
38362 2023-02-16,23:37:28.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36814 2023-02-16,23:37:28.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3935827016830444
36932 2023-02-16,23:37:28.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4052679538726807
37152 2023-02-16,23:37:28.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3809515237808228
37268 2023-02-16,23:37:28.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.400869369506836
38487 2023-02-16,23:37:28.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37035 2023-02-16,23:37:28.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.395074486732483
37757 2023-02-16,23:37:28.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
38108 2023-02-16,23:37:28.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
38487 2023-02-16,23:37:28.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
36814 2023-02-16,23:37:28.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3923165798187256
37152 2023-02-16,23:37:28.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4196735620498657
37268 2023-02-16,23:37:28.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3915256261825562
37393 2023-02-16,23:37:28.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3712444305419922
37533 2023-02-16,23:37:28.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37862 2023-02-16,23:37:28.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37983 2023-02-16,23:37:28.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38255 2023-02-16,23:37:28.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
38362 2023-02-16,23:37:28.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36932 2023-02-16,23:37:28.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4701358079910278
38487 2023-02-16,23:37:29.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
36814 2023-02-16,23:37:29.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4443376064300537
37035 2023-02-16,23:37:29.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3779910802841187
37152 2023-02-16,23:37:29.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.396849513053894
37393 2023-02-16,23:37:29.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3984698057174683
37533 2023-02-16,23:37:29.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37757 2023-02-16,23:37:29.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37862 2023-02-16,23:37:29.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
37983 2023-02-16,23:37:29.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
38108 2023-02-16,23:37:29.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
38255 2023-02-16,23:37:29.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
38362 2023-02-16,23:37:29.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36932 2023-02-16,23:37:29.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3870790004730225
37268 2023-02-16,23:37:29.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4074074029922485
37983 2023-02-16,23:37:29.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
38362 2023-02-16,23:37:29.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
38487 2023-02-16,23:37:29.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
36814 2023-02-16,23:37:29.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3752092123031616
37035 2023-02-16,23:37:29.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3934303522109985
37152 2023-02-16,23:37:29.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3639081716537476
37393 2023-02-16,23:37:29.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3796801567077637
37533 2023-02-16,23:37:29.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
37862 2023-02-16,23:37:29.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
38108 2023-02-16,23:37:29.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38255 2023-02-16,23:37:29.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
36932 2023-02-16,23:37:29.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3811204433441162
37268 2023-02-16,23:37:29.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4123525619506836
37757 2023-02-16,23:37:29.279 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37035 2023-02-16,23:37:29.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4180563688278198
37152 2023-02-16,23:37:29.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3731743097305298
37393 2023-02-16,23:37:29.524 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3650879859924316
37533 2023-02-16,23:37:29.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
37983 2023-02-16,23:37:29.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
38487 2023-02-16,23:37:29.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
36814 2023-02-16,23:37:29.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3516851663589478
36932 2023-02-16,23:37:29.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4003549814224243
37268 2023-02-16,23:37:29.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3757917881011963
37862 2023-02-16,23:37:29.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
38108 2023-02-16,23:37:29.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
38255 2023-02-16,23:37:29.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
38362 2023-02-16,23:37:29.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37757 2023-02-16,23:37:29.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
36814 2023-02-16,23:37:29.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.3994183540344238
37035 2023-02-16,23:37:29.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.401237964630127
37152 2023-02-16,23:37:29.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3964354991912842
37533 2023-02-16,23:37:29.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
37983 2023-02-16,23:37:29.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
38487 2023-02-16,23:37:29.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
36932 2023-02-16,23:37:29.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.373869776725769
37268 2023-02-16,23:37:29.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.388912558555603
37393 2023-02-16,23:37:29.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3577040433883667
37757 2023-02-16,23:37:29.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37862 2023-02-16,23:37:29.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
38108 2023-02-16,23:37:29.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
38255 2023-02-16,23:37:29.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
38362 2023-02-16,23:37:29.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37983 2023-02-16,23:37:30.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
36814 2023-02-16,23:37:30.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3589444160461426
37035 2023-02-16,23:37:30.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3906728029251099
37533 2023-02-16,23:37:30.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
38487 2023-02-16,23:37:30.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
36932 2023-02-16,23:37:30.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3925789594650269
37152 2023-02-16,23:37:30.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.425696849822998
37268 2023-02-16,23:37:30.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3733748197555542
37393 2023-02-16,23:37:30.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.353850245475769
37862 2023-02-16,23:37:30.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
38108 2023-02-16,23:37:30.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
38255 2023-02-16,23:37:30.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
38362 2023-02-16,23:37:30.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37757 2023-02-16,23:37:30.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37983 2023-02-16,23:37:30.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
38487 2023-02-16,23:37:30.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
36814 2023-02-16,23:37:30.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3790743350982666
37035 2023-02-16,23:37:30.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3972080945968628
37533 2023-02-16,23:37:30.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
38108 2023-02-16,23:37:30.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
38255 2023-02-16,23:37:30.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
38362 2023-02-16,23:37:30.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36932 2023-02-16,23:37:30.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4170804023742676
37268 2023-02-16,23:37:30.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3948214054107666
37152 2023-02-16,23:37:30.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4117519855499268
37393 2023-02-16,23:37:30.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4306126832962036
37757 2023-02-16,23:37:30.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37862 2023-02-16,23:37:30.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37983 2023-02-16,23:37:30.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
38487 2023-02-16,23:37:30.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
36814 2023-02-16,23:37:30.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4399871826171875
37035 2023-02-16,23:37:30.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3854079246520996
37268 2023-02-16,23:37:30.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4030756950378418
37533 2023-02-16,23:37:30.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
38108 2023-02-16,23:37:30.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
38255 2023-02-16,23:37:30.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
38362 2023-02-16,23:37:30.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36932 2023-02-16,23:37:30.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.414407730102539
37152 2023-02-16,23:37:30.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4319369792938232
37393 2023-02-16,23:37:30.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4321967363357544
37862 2023-02-16,23:37:30.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37757 2023-02-16,23:37:30.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37393 2023-02-16,23:37:30.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.383307695388794
37533 2023-02-16,23:37:30.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37983 2023-02-16,23:37:30.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
36814 2023-02-16,23:37:30.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.379361867904663
37035 2023-02-16,23:37:30.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.369936466217041
37268 2023-02-16,23:37:30.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3718832731246948
37862 2023-02-16,23:37:30.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
38108 2023-02-16,23:37:30.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
38255 2023-02-16,23:37:30.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
38362 2023-02-16,23:37:30.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
38487 2023-02-16,23:37:30.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
36932 2023-02-16,23:37:30.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3789300918579102
37152 2023-02-16,23:37:30.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.357750654220581
37757 2023-02-16,23:37:30.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37393 2023-02-16,23:37:31.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3733818531036377
37983 2023-02-16,23:37:31.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
36814 2023-02-16,23:37:31.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4109498262405396
37035 2023-02-16,23:37:31.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.405433177947998
37533 2023-02-16,23:37:31.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37862 2023-02-16,23:37:31.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
38108 2023-02-16,23:37:31.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
38255 2023-02-16,23:37:31.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
38362 2023-02-16,23:37:31.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
38487 2023-02-16,23:37:31.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
36932 2023-02-16,23:37:31.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4074105024337769
37152 2023-02-16,23:37:31.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3215439319610596
37268 2023-02-16,23:37:31.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3953137397766113
37757 2023-02-16,23:37:31.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37035 2023-02-16,23:37:31.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3923178911209106
37393 2023-02-16,23:37:31.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4003169536590576
37533 2023-02-16,23:37:31.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37983 2023-02-16,23:37:31.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
38487 2023-02-16,23:37:31.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
36814 2023-02-16,23:37:31.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3884011507034302
36932 2023-02-16,23:37:31.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4627244472503662
37268 2023-02-16,23:37:31.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.359818696975708
37757 2023-02-16,23:37:31.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37862 2023-02-16,23:37:31.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
38108 2023-02-16,23:37:31.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
38255 2023-02-16,23:37:31.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
38362 2023-02-16,23:37:31.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37152 2023-02-16,23:37:31.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3784632682800293
36814 2023-02-16,23:37:31.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.385868787765503
37035 2023-02-16,23:37:31.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3728574514389038
37533 2023-02-16,23:37:31.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
37983 2023-02-16,23:37:31.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
38487 2023-02-16,23:37:31.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
36932 2023-02-16,23:37:31.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4369959831237793
37152 2023-02-16,23:37:31.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3307216167449951
37268 2023-02-16,23:37:31.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.387253761291504
37393 2023-02-16,23:37:31.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4022796154022217
37757 2023-02-16,23:37:31.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37862 2023-02-16,23:37:31.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
38255 2023-02-16,23:37:31.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
38108 2023-02-16,23:37:31.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
38362 2023-02-16,23:37:31.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37393 2023-02-16,23:37:31.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.381592035293579
37533 2023-02-16,23:37:31.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
38108 2023-02-16,23:37:31.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
38487 2023-02-16,23:37:31.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
36814 2023-02-16,23:37:31.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.379570722579956
36932 2023-02-16,23:37:31.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3950016498565674
37035 2023-02-16,23:37:31.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.353085994720459
37152 2023-02-16,23:37:31.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3849048614501953
37757 2023-02-16,23:37:31.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37862 2023-02-16,23:37:31.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37983 2023-02-16,23:37:31.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
38255 2023-02-16,23:37:31.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
38362 2023-02-16,23:37:31.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37268 2023-02-16,23:37:31.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4173225164413452
38487 2023-02-16,23:37:32.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
36814 2023-02-16,23:37:32.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3847771883010864
37393 2023-02-16,23:37:32.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3929286003112793
37533 2023-02-16,23:37:32.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37757 2023-02-16,23:37:32.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37983 2023-02-16,23:37:32.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
38108 2023-02-16,23:37:32.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38255 2023-02-16,23:37:32.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
38362 2023-02-16,23:37:32.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36932 2023-02-16,23:37:32.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3661738634109497
37035 2023-02-16,23:37:32.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.383319616317749
37152 2023-02-16,23:37:32.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4063646793365479
37268 2023-02-16,23:37:32.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4187580347061157
37862 2023-02-16,23:37:32.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37393 2023-02-16,23:37:32.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3975398540496826
37533 2023-02-16,23:37:32.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
37757 2023-02-16,23:37:32.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
38255 2023-02-16,23:37:32.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
38362 2023-02-16,23:37:32.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
38487 2023-02-16,23:37:32.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
36814 2023-02-16,23:37:32.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4375392198562622
36932 2023-02-16,23:37:32.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3536165952682495
37035 2023-02-16,23:37:32.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4390935897827148
37152 2023-02-16,23:37:32.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3904657363891602
37268 2023-02-16,23:37:32.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.353273868560791
37862 2023-02-16,23:37:32.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37983 2023-02-16,23:37:32.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
38108 2023-02-16,23:37:32.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
38255 2023-02-16,23:37:32.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36814 2023-02-16,23:37:32.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3783539533615112
37035 2023-02-16,23:37:32.731 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.370892882347107
37268 2023-02-16,23:37:32.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.423898458480835
37393 2023-02-16,23:37:32.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4338269233703613
37757 2023-02-16,23:37:32.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37533 2023-02-16,23:37:32.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37862 2023-02-16,23:37:32.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
38108 2023-02-16,23:37:32.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
38362 2023-02-16,23:37:32.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
38487 2023-02-16,23:37:32.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
36932 2023-02-16,23:37:32.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3875834941864014
37152 2023-02-16,23:37:32.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3840010166168213
37983 2023-02-16,23:37:32.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
38487 2023-02-16,23:37:32.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
36814 2023-02-16,23:37:32.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3917165994644165
37035 2023-02-16,23:37:32.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.392385721206665
37268 2023-02-16,23:37:32.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3887860774993896
37393 2023-02-16,23:37:32.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4015185832977295
37533 2023-02-16,23:37:32.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37757 2023-02-16,23:37:33.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37862 2023-02-16,23:37:33.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
37983 2023-02-16,23:37:33.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
38108 2023-02-16,23:37:33.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
38255 2023-02-16,23:37:33.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
38362 2023-02-16,23:37:33.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
36932 2023-02-16,23:37:33.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3311569690704346
37152 2023-02-16,23:37:33.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3633174896240234
37393 2023-02-16,23:37:33.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3857873678207397
36814 2023-02-16,23:37:33.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3728550672531128
37035 2023-02-16,23:37:33.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3232274055480957
37268 2023-02-16,23:37:33.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4008208513259888
37533 2023-02-16,23:37:33.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
37757 2023-02-16,23:37:33.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37862 2023-02-16,23:37:33.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
37983 2023-02-16,23:37:33.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
38108 2023-02-16,23:37:33.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
38255 2023-02-16,23:37:33.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
38362 2023-02-16,23:37:33.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
38487 2023-02-16,23:37:33.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
36932 2023-02-16,23:37:33.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3714344501495361
37152 2023-02-16,23:37:33.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3564164638519287
37268 2023-02-16,23:37:33.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4060492515563965
36814 2023-02-16,23:37:33.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3737410306930542
37393 2023-02-16,23:37:33.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4456067085266113
37533 2023-02-16,23:37:33.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37757 2023-02-16,23:37:33.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37862 2023-02-16,23:37:33.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37983 2023-02-16,23:37:33.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
38108 2023-02-16,23:37:33.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
38255 2023-02-16,23:37:33.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
38362 2023-02-16,23:37:33.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
38487 2023-02-16,23:37:33.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
36932 2023-02-16,23:37:33.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3618351221084595
37035 2023-02-16,23:37:33.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3712044954299927
37152 2023-02-16,23:37:33.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3657548427581787
37268 2023-02-16,23:37:33.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4321314096450806
37393 2023-02-16,23:37:33.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.412393569946289
37533 2023-02-16,23:37:33.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37757 2023-02-16,23:37:33.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37983 2023-02-16,23:37:33.795 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
38255 2023-02-16,23:37:33.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
38487 2023-02-16,23:37:33.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
36814 2023-02-16,23:37:33.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3877590894699097
36932 2023-02-16,23:37:33.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3724298477172852
37035 2023-02-16,23:37:33.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.416264533996582
37152 2023-02-16,23:37:33.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3597370386123657
37862 2023-02-16,23:37:33.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
38108 2023-02-16,23:37:33.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
38362 2023-02-16,23:37:33.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37393 2023-02-16,23:37:34.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4306892156600952
37533 2023-02-16,23:37:34.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
38255 2023-02-16,23:37:34.059 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
38487 2023-02-16,23:37:34.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
36814 2023-02-16,23:37:34.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3897504806518555
36932 2023-02-16,23:37:34.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.445926308631897
37035 2023-02-16,23:37:34.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3431503772735596
37268 2023-02-16,23:37:34.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.424810528755188
37757 2023-02-16,23:37:34.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37862 2023-02-16,23:37:34.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37983 2023-02-16,23:37:34.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38362 2023-02-16,23:37:34.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
38108 2023-02-16,23:37:34.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
37152 2023-02-16,23:37:34.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3705296516418457
37393 2023-02-16,23:37:34.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3987174034118652
38255 2023-02-16,23:37:34.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
38487 2023-02-16,23:37:34.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
36814 2023-02-16,23:37:34.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3809267282485962
36932 2023-02-16,23:37:34.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3967323303222656
37533 2023-02-16,23:37:34.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37757 2023-02-16,23:37:34.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37983 2023-02-16,23:37:34.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38362 2023-02-16,23:37:34.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37035 2023-02-16,23:37:34.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3742499351501465
37152 2023-02-16,23:37:34.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.385998249053955
37268 2023-02-16,23:37:34.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3693724870681763
37862 2023-02-16,23:37:34.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
38108 2023-02-16,23:37:34.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37393 2023-02-16,23:37:34.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4533417224884033
37533 2023-02-16,23:37:34.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
38255 2023-02-16,23:37:34.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
38487 2023-02-16,23:37:34.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
36814 2023-02-16,23:37:34.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.363434910774231
36932 2023-02-16,23:37:34.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4185799360275269
37035 2023-02-16,23:37:34.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3785136938095093
37152 2023-02-16,23:37:34.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.458569884300232
37268 2023-02-16,23:37:34.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3910913467407227
37757 2023-02-16,23:37:34.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37862 2023-02-16,23:37:34.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37983 2023-02-16,23:37:34.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
38108 2023-02-16,23:37:34.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38362 2023-02-16,23:37:34.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37035 2023-02-16,23:37:34.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3762102127075195
37268 2023-02-16,23:37:34.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.38694167137146
37393 2023-02-16,23:37:34.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4392287731170654
37533 2023-02-16,23:37:34.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
38108 2023-02-16,23:37:34.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
38255 2023-02-16,23:37:34.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
38487 2023-02-16,23:37:34.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
36814 2023-02-16,23:37:34.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3675055503845215
36932 2023-02-16,23:37:34.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3772305250167847
37152 2023-02-16,23:37:34.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3976619243621826
37757 2023-02-16,23:37:34.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37862 2023-02-16,23:37:34.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37983 2023-02-16,23:37:34.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
38362 2023-02-16,23:37:34.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
37393 2023-02-16,23:37:35.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4012830257415771
38108 2023-02-16,23:37:35.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
36814 2023-02-16,23:37:35.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3495910167694092
36932 2023-02-16,23:37:35.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.365203619003296
37035 2023-02-16,23:37:35.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4122103452682495
37152 2023-02-16,23:37:35.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3960493803024292
37268 2023-02-16,23:37:35.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3505314588546753
37533 2023-02-16,23:37:35.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
37757 2023-02-16,23:37:35.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37862 2023-02-16,23:37:35.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37983 2023-02-16,23:37:35.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
38255 2023-02-16,23:37:35.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
38362 2023-02-16,23:37:35.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
38487 2023-02-16,23:37:35.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
38108 2023-02-16,23:37:35.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
38487 2023-02-16,23:37:35.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37035 2023-02-16,23:37:35.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3796075582504272
36932 2023-02-16,23:37:35.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3873317241668701
37152 2023-02-16,23:37:35.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3412268161773682
37268 2023-02-16,23:37:35.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4112558364868164
37393 2023-02-16,23:37:35.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3928831815719604
37533 2023-02-16,23:37:35.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37757 2023-02-16,23:37:35.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37862 2023-02-16,23:37:35.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37983 2023-02-16,23:37:35.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
38255 2023-02-16,23:37:35.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
38362 2023-02-16,23:37:35.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36814 2023-02-16,23:37:35.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4107484817504883
37533 2023-02-16,23:37:35.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
36932 2023-02-16,23:37:35.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3623757362365723
37035 2023-02-16,23:37:35.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4479049444198608
37268 2023-02-16,23:37:35.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4387030601501465
37393 2023-02-16,23:37:35.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4075809717178345
37757 2023-02-16,23:37:35.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
37862 2023-02-16,23:37:35.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37983 2023-02-16,23:37:35.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
38108 2023-02-16,23:37:35.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
38255 2023-02-16,23:37:35.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38362 2023-02-16,23:37:35.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
38487 2023-02-16,23:37:35.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
36814 2023-02-16,23:37:35.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3816584348678589
37152 2023-02-16,23:37:35.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4151787757873535
38487 2023-02-16,23:37:35.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
36932 2023-02-16,23:37:35.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3720979690551758
37035 2023-02-16,23:37:35.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.445572853088379
37268 2023-02-16,23:37:35.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3884555101394653
37393 2023-02-16,23:37:35.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.412523627281189
37533 2023-02-16,23:37:35.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37757 2023-02-16,23:37:35.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
37862 2023-02-16,23:37:35.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37983 2023-02-16,23:37:35.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
38108 2023-02-16,23:37:35.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
38255 2023-02-16,23:37:35.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
38362 2023-02-16,23:37:35.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
36814 2023-02-16,23:37:35.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4335803985595703
37152 2023-02-16,23:37:35.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3840327262878418
37533 2023-02-16,23:37:36.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
38362 2023-02-16,23:37:36.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
38487 2023-02-16,23:37:36.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
36932 2023-02-16,23:37:36.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3550645112991333
37035 2023-02-16,23:37:36.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3436185121536255
37268 2023-02-16,23:37:36.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3516849279403687
37393 2023-02-16,23:37:36.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3752564191818237
37757 2023-02-16,23:37:36.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
37862 2023-02-16,23:37:36.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37983 2023-02-16,23:37:36.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
38108 2023-02-16,23:37:36.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
38255 2023-02-16,23:37:36.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36814 2023-02-16,23:37:36.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4314038753509521
37152 2023-02-16,23:37:36.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4123876094818115
37533 2023-02-16,23:37:36.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
36932 2023-02-16,23:37:36.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372265338897705
37035 2023-02-16,23:37:36.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4052516222000122
37268 2023-02-16,23:37:36.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4116597175598145
37757 2023-02-16,23:37:36.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37862 2023-02-16,23:37:36.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37983 2023-02-16,23:37:36.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
38108 2023-02-16,23:37:36.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
38255 2023-02-16,23:37:36.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
38487 2023-02-16,23:37:36.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
38362 2023-02-16,23:37:36.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
36814 2023-02-16,23:37:36.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4276692867279053
37152 2023-02-16,23:37:36.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3540581464767456
37393 2023-02-16,23:37:36.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3885891437530518
38362 2023-02-16,23:37:36.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36932 2023-02-16,23:37:36.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3600633144378662
37035 2023-02-16,23:37:36.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4699105024337769
37268 2023-02-16,23:37:36.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.398473858833313
37533 2023-02-16,23:37:36.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37757 2023-02-16,23:37:36.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
37983 2023-02-16,23:37:36.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
38108 2023-02-16,23:37:36.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
38255 2023-02-16,23:37:36.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
38487 2023-02-16,23:37:36.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:37:36.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3668394088745117
37152 2023-02-16,23:37:36.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3783971071243286
37393 2023-02-16,23:37:36.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3738269805908203
37862 2023-02-16,23:37:36.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37035 2023-02-16,23:37:36.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3870233297348022
38108 2023-02-16,23:37:36.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
38362 2023-02-16,23:37:36.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
38487 2023-02-16,23:37:36.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37268 2023-02-16,23:37:37.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4092774391174316
37393 2023-02-16,23:37:37.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3942409753799438
37757 2023-02-16,23:37:37.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
37862 2023-02-16,23:37:37.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37983 2023-02-16,23:37:37.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
38255 2023-02-16,23:37:37.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
36814 2023-02-16,23:37:37.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.374495506286621
36932 2023-02-16,23:37:37.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4095062017440796
37152 2023-02-16,23:37:37.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.347790002822876
37533 2023-02-16,23:37:37.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37393 2023-02-16,23:37:37.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4041364192962646
38487 2023-02-16,23:37:37.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37035 2023-02-16,23:37:37.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3802257776260376
38108 2023-02-16,23:37:37.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
38362 2023-02-16,23:37:37.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
36814 2023-02-16,23:37:37.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4184175729751587
36932 2023-02-16,23:37:37.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4204654693603516
37152 2023-02-16,23:37:37.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4003397226333618
37268 2023-02-16,23:37:37.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4105329513549805
37533 2023-02-16,23:37:37.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3951804637908936
37757 2023-02-16,23:37:37.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37862 2023-02-16,23:37:37.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37983 2023-02-16,23:37:37.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
38255 2023-02-16,23:37:37.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37393 2023-02-16,23:37:37.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3717747926712036
38108 2023-02-16,23:37:37.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
38255 2023-02-16,23:37:37.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
38487 2023-02-16,23:37:37.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37035 2023-02-16,23:37:37.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.3996115922927856
37268 2023-02-16,23:37:37.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3443098068237305
37533 2023-02-16,23:37:37.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4110201597213745
37757 2023-02-16,23:37:37.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37862 2023-02-16,23:37:37.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37983 2023-02-16,23:37:37.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
38362 2023-02-16,23:37:37.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
36814 2023-02-16,23:37:37.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3487589359283447
36932 2023-02-16,23:37:37.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4413392543792725
37152 2023-02-16,23:37:37.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3738871812820435
37268 2023-02-16,23:37:37.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3777568340301514
37393 2023-02-16,23:37:37.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.395603060722351
37983 2023-02-16,23:37:37.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
38108 2023-02-16,23:37:37.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
38255 2023-02-16,23:37:37.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
38362 2023-02-16,23:37:37.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
38487 2023-02-16,23:37:37.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
36814 2023-02-16,23:37:37.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3323386907577515
37035 2023-02-16,23:37:37.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.372527837753296
37152 2023-02-16,23:37:37.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4598619937896729
37533 2023-02-16,23:37:37.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.422855019569397
37757 2023-02-16,23:37:37.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37862 2023-02-16,23:37:37.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
36932 2023-02-16,23:37:37.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3682700395584106
37393 2023-02-16,23:37:38.059 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3603602647781372
37533 2023-02-16,23:37:38.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.374497890472412
37983 2023-02-16,23:37:38.067 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
38108 2023-02-16,23:37:38.067 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
38255 2023-02-16,23:37:38.067 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
38362 2023-02-16,23:37:38.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
38487 2023-02-16,23:37:38.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
36814 2023-02-16,23:37:38.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4140489101409912
37035 2023-02-16,23:37:38.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3921030759811401
37152 2023-02-16,23:37:38.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3460773229599
37268 2023-02-16,23:37:38.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3730268478393555
37757 2023-02-16,23:37:38.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
37862 2023-02-16,23:37:38.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
36932 2023-02-16,23:37:38.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3970527648925781
37393 2023-02-16,23:37:38.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.386788010597229
37983 2023-02-16,23:37:38.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
38108 2023-02-16,23:37:38.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
38255 2023-02-16,23:37:38.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
38362 2023-02-16,23:37:38.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
38487 2023-02-16,23:37:38.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
36814 2023-02-16,23:37:38.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.400510311126709
36932 2023-02-16,23:37:38.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4130163192749023
37035 2023-02-16,23:37:38.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4173460006713867
37152 2023-02-16,23:37:38.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4077649116516113
37268 2023-02-16,23:37:38.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.346195101737976
37533 2023-02-16,23:37:38.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3705596923828125
37757 2023-02-16,23:37:38.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37862 2023-02-16,23:37:38.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37862 2023-02-16,23:37:38.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37983 2023-02-16,23:37:38.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
38108 2023-02-16,23:37:38.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
38255 2023-02-16,23:37:38.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38362 2023-02-16,23:37:38.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
38487 2023-02-16,23:37:38.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
36814 2023-02-16,23:37:38.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.37012779712677
36932 2023-02-16,23:37:38.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4049286842346191
37035 2023-02-16,23:37:38.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4147183895111084
37152 2023-02-16,23:37:38.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.402237057685852
37268 2023-02-16,23:37:38.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.399336338043213
37393 2023-02-16,23:37:38.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4184932708740234
37533 2023-02-16,23:37:38.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3995542526245117
37757 2023-02-16,23:37:38.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
38255 2023-02-16,23:37:38.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37268 2023-02-16,23:37:38.862 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.388084888458252
37393 2023-02-16,23:37:38.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4201678037643433
37533 2023-02-16,23:37:38.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4018793106079102
37862 2023-02-16,23:37:38.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37983 2023-02-16,23:37:38.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
38362 2023-02-16,23:37:38.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
38487 2023-02-16,23:37:38.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
36814 2023-02-16,23:37:38.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4149327278137207
36932 2023-02-16,23:37:38.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4013781547546387
37035 2023-02-16,23:37:38.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3799165487289429
37152 2023-02-16,23:37:38.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3901070356369019
37757 2023-02-16,23:37:38.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38108 2023-02-16,23:37:38.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37268 2023-02-16,23:37:39.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.348463773727417
37393 2023-02-16,23:37:39.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3539104461669922
37862 2023-02-16,23:37:39.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37983 2023-02-16,23:37:39.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
38255 2023-02-16,23:37:39.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
38362 2023-02-16,23:37:39.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
38487 2023-02-16,23:37:39.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
36814 2023-02-16,23:37:39.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.43235182762146
36932 2023-02-16,23:37:39.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3972139358520508
37035 2023-02-16,23:37:39.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4070262908935547
37152 2023-02-16,23:37:39.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4030791521072388
37533 2023-02-16,23:37:39.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3846968412399292
37757 2023-02-16,23:37:39.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
38108 2023-02-16,23:37:39.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37862 2023-02-16,23:37:39.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37983 2023-02-16,23:37:39.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
38487 2023-02-16,23:37:39.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37035 2023-02-16,23:37:39.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4646049737930298
37152 2023-02-16,23:37:39.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3363875150680542
37268 2023-02-16,23:37:39.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3588652610778809
37393 2023-02-16,23:37:39.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.424514889717102
36814 2023-02-16,23:37:39.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3806549310684204
37533 2023-02-16,23:37:39.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3783923387527466
37757 2023-02-16,23:37:39.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
38255 2023-02-16,23:37:39.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
38108 2023-02-16,23:37:39.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
38362 2023-02-16,23:37:39.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36932 2023-02-16,23:37:39.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4414052963256836
37393 2023-02-16,23:37:39.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3895729780197144
37862 2023-02-16,23:37:39.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37983 2023-02-16,23:37:39.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37035 2023-02-16,23:37:39.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4380077123641968
37268 2023-02-16,23:37:39.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.367438793182373
37533 2023-02-16,23:37:39.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4408342838287354
37757 2023-02-16,23:37:39.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
38108 2023-02-16,23:37:39.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
38255 2023-02-16,23:37:39.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
38362 2023-02-16,23:37:39.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38487 2023-02-16,23:37:39.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
36814 2023-02-16,23:37:39.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3596464395523071
36932 2023-02-16,23:37:39.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3960515260696411
37152 2023-02-16,23:37:39.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3736088275909424
37268 2023-02-16,23:37:39.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.3999888896942139
37393 2023-02-16,23:37:39.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.400611162185669
37533 2023-02-16,23:37:39.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3567745685577393
37983 2023-02-16,23:37:39.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
38108 2023-02-16,23:37:39.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38255 2023-02-16,23:37:39.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
38362 2023-02-16,23:37:39.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
38487 2023-02-16,23:37:39.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
36814 2023-02-16,23:37:39.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4235458374023438
36932 2023-02-16,23:37:39.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3504856824874878
37035 2023-02-16,23:37:39.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.394012212753296
37152 2023-02-16,23:37:39.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3830013275146484
37757 2023-02-16,23:37:39.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37862 2023-02-16,23:37:39.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37268 2023-02-16,23:37:40.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3868366479873657
37393 2023-02-16,23:37:40.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4062831401824951
37862 2023-02-16,23:37:40.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
38108 2023-02-16,23:37:40.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38255 2023-02-16,23:37:40.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
38487 2023-02-16,23:37:40.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
36814 2023-02-16,23:37:40.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.390479326248169
36932 2023-02-16,23:37:40.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3480379581451416
37035 2023-02-16,23:37:40.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.365180253982544
37152 2023-02-16,23:37:40.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.37259840965271
37533 2023-02-16,23:37:40.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4237967729568481
37757 2023-02-16,23:37:40.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37983 2023-02-16,23:37:40.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
38362 2023-02-16,23:37:40.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37268 2023-02-16,23:37:40.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4559886455535889
37393 2023-02-16,23:37:40.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.432205319404602
38108 2023-02-16,23:37:40.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
38487 2023-02-16,23:37:40.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37035 2023-02-16,23:37:40.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3520965576171875
37152 2023-02-16,23:37:40.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3818249702453613
37533 2023-02-16,23:37:40.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3927040100097656
37757 2023-02-16,23:37:40.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
37862 2023-02-16,23:37:40.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37983 2023-02-16,23:37:40.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
38255 2023-02-16,23:37:40.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
38362 2023-02-16,23:37:40.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
36814 2023-02-16,23:37:40.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3852177858352661
36932 2023-02-16,23:37:40.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.419737696647644
38108 2023-02-16,23:37:40.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37268 2023-02-16,23:37:40.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4053367376327515
37393 2023-02-16,23:37:40.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4250028133392334
37533 2023-02-16,23:37:40.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4015986919403076
37862 2023-02-16,23:37:40.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37983 2023-02-16,23:37:40.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
38362 2023-02-16,23:37:40.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
38487 2023-02-16,23:37:40.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
36814 2023-02-16,23:37:40.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3232767581939697
36932 2023-02-16,23:37:40.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4298534393310547
37035 2023-02-16,23:37:40.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3880239725112915
37152 2023-02-16,23:37:40.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4109141826629639
37757 2023-02-16,23:37:40.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
38255 2023-02-16,23:37:40.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
37268 2023-02-16,23:37:41.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3841201066970825
37393 2023-02-16,23:37:41.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.370457649230957
37862 2023-02-16,23:37:41.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37983 2023-02-16,23:37:41.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
38108 2023-02-16,23:37:41.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
38255 2023-02-16,23:37:41.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38362 2023-02-16,23:37:41.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
38487 2023-02-16,23:37:41.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
36814 2023-02-16,23:37:41.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4088189601898193
36932 2023-02-16,23:37:41.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3874123096466064
37035 2023-02-16,23:37:41.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3297182321548462
37152 2023-02-16,23:37:41.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3558555841445923
37533 2023-02-16,23:37:41.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4176576137542725
37757 2023-02-16,23:37:41.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37862 2023-02-16,23:37:41.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38108 2023-02-16,23:37:41.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
38255 2023-02-16,23:37:41.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37035 2023-02-16,23:37:41.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3706855773925781
37393 2023-02-16,23:37:41.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3922685384750366
37533 2023-02-16,23:37:41.283 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.396545171737671
37983 2023-02-16,23:37:41.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
38362 2023-02-16,23:37:41.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
38487 2023-02-16,23:37:41.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
36814 2023-02-16,23:37:41.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3758289813995361
36932 2023-02-16,23:37:41.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4311498403549194
37152 2023-02-16,23:37:41.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4533501863479614
37268 2023-02-16,23:37:41.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3922345638275146
37757 2023-02-16,23:37:41.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37393 2023-02-16,23:37:41.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.386675477027893
37862 2023-02-16,23:37:41.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
38108 2023-02-16,23:37:41.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
38255 2023-02-16,23:37:41.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
38362 2023-02-16,23:37:41.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
38487 2023-02-16,23:37:41.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
36814 2023-02-16,23:37:41.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.412182331085205
36932 2023-02-16,23:37:41.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.357830286026001
37035 2023-02-16,23:37:41.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3615307807922363
37152 2023-02-16,23:37:41.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.381777048110962
37533 2023-02-16,23:37:41.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372626304626465
37757 2023-02-16,23:37:41.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
37983 2023-02-16,23:37:41.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37268 2023-02-16,23:37:41.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3384342193603516
38108 2023-02-16,23:37:41.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
38255 2023-02-16,23:37:41.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
37035 2023-02-16,23:37:41.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.370309829711914
37393 2023-02-16,23:37:41.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3504023551940918
37533 2023-02-16,23:37:41.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.364020824432373
37757 2023-02-16,23:37:41.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37862 2023-02-16,23:37:41.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37983 2023-02-16,23:37:41.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
38362 2023-02-16,23:37:41.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
38487 2023-02-16,23:37:41.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
36814 2023-02-16,23:37:41.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3831381797790527
36932 2023-02-16,23:37:41.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.383757472038269
37152 2023-02-16,23:37:41.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.406883955001831
37268 2023-02-16,23:37:41.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.377382516860962
37035 2023-02-16,23:37:42.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.446067214012146
37393 2023-02-16,23:37:42.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4118045568466187
37862 2023-02-16,23:37:42.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
38255 2023-02-16,23:37:42.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
38362 2023-02-16,23:37:42.089 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
38487 2023-02-16,23:37:42.090 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
36814 2023-02-16,23:37:42.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3602346181869507
36932 2023-02-16,23:37:42.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.377625823020935
37983 2023-02-16,23:37:42.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
38108 2023-02-16,23:37:42.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37152 2023-02-16,23:37:42.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3950417041778564
37268 2023-02-16,23:37:42.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4091548919677734
37533 2023-02-16,23:37:42.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3468059301376343
37757 2023-02-16,23:37:42.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
37035 2023-02-16,23:37:42.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3984458446502686
37393 2023-02-16,23:37:42.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4394553899765015
38255 2023-02-16,23:37:42.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
37268 2023-02-16,23:37:42.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3809611797332764
37757 2023-02-16,23:37:42.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37862 2023-02-16,23:37:42.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
38108 2023-02-16,23:37:42.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
38362 2023-02-16,23:37:42.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
38487 2023-02-16,23:37:42.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
36814 2023-02-16,23:37:42.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4253090620040894
36932 2023-02-16,23:37:42.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.378379225730896
37152 2023-02-16,23:37:42.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4239602088928223
37533 2023-02-16,23:37:42.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3510043621063232
37983 2023-02-16,23:37:42.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37035 2023-02-16,23:37:42.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4185335636138916
37393 2023-02-16,23:37:42.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3879247903823853
37862 2023-02-16,23:37:42.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38255 2023-02-16,23:37:42.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
38108 2023-02-16,23:37:42.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
38362 2023-02-16,23:37:42.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38487 2023-02-16,23:37:42.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
36814 2023-02-16,23:37:42.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4218521118164062
36932 2023-02-16,23:37:42.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3914241790771484
37152 2023-02-16,23:37:42.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.3996388912200928
37268 2023-02-16,23:37:42.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3825931549072266
37533 2023-02-16,23:37:42.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3889336585998535
37757 2023-02-16,23:37:42.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
37983 2023-02-16,23:37:42.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37533 2023-02-16,23:37:42.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3443655967712402
37862 2023-02-16,23:37:42.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
38255 2023-02-16,23:37:42.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
38487 2023-02-16,23:37:42.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
36814 2023-02-16,23:37:42.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.410174012184143
36932 2023-02-16,23:37:42.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3943315744400024
37035 2023-02-16,23:37:42.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3775063753128052
37152 2023-02-16,23:37:42.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3964953422546387
37268 2023-02-16,23:37:42.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.375124454498291
37393 2023-02-16,23:37:42.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3513891696929932
37757 2023-02-16,23:37:42.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37983 2023-02-16,23:37:42.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
38108 2023-02-16,23:37:42.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
38362 2023-02-16,23:37:42.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37035 2023-02-16,23:37:43.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3650444746017456
37152 2023-02-16,23:37:43.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.422957420349121
37268 2023-02-16,23:37:43.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3818851709365845
37533 2023-02-16,23:37:43.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3682246208190918
37757 2023-02-16,23:37:43.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37862 2023-02-16,23:37:43.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37983 2023-02-16,23:37:43.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
38108 2023-02-16,23:37:43.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
38255 2023-02-16,23:37:43.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
38362 2023-02-16,23:37:43.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
38487 2023-02-16,23:37:43.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
36814 2023-02-16,23:37:43.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4498248100280762
36932 2023-02-16,23:37:43.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3655701875686646
37393 2023-02-16,23:37:43.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.412503719329834
37035 2023-02-16,23:37:43.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3871581554412842
37152 2023-02-16,23:37:43.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3869377374649048
37862 2023-02-16,23:37:43.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
38255 2023-02-16,23:37:43.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
38362 2023-02-16,23:37:43.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
38487 2023-02-16,23:37:43.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
36814 2023-02-16,23:37:43.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4025263786315918
36932 2023-02-16,23:37:43.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4083261489868164
37268 2023-02-16,23:37:43.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.428966999053955
37393 2023-02-16,23:37:43.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3988953828811646
37533 2023-02-16,23:37:43.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4135363101959229
37983 2023-02-16,23:37:43.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
38108 2023-02-16,23:37:43.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37757 2023-02-16,23:37:43.439 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37862 2023-02-16,23:37:43.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
38255 2023-02-16,23:37:43.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37035 2023-02-16,23:37:43.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3622913360595703
37152 2023-02-16,23:37:43.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3831441402435303
37268 2023-02-16,23:37:43.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.379841923713684
37393 2023-02-16,23:37:43.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4113335609436035
37533 2023-02-16,23:37:43.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.354655385017395
37757 2023-02-16,23:37:43.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3951733112335205
37983 2023-02-16,23:37:43.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
38108 2023-02-16,23:37:43.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
38362 2023-02-16,23:37:43.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
38487 2023-02-16,23:37:43.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
36814 2023-02-16,23:37:43.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3912893533706665
36932 2023-02-16,23:37:43.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3847761154174805
37035 2023-02-16,23:37:43.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.371790885925293
37152 2023-02-16,23:37:43.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4227052927017212
37393 2023-02-16,23:37:43.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4112770557403564
37533 2023-02-16,23:37:43.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3591387271881104
37757 2023-02-16,23:37:43.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4115536212921143
37862 2023-02-16,23:37:43.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
37983 2023-02-16,23:37:43.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
38108 2023-02-16,23:37:43.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
38255 2023-02-16,23:37:43.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
38362 2023-02-16,23:37:43.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
38487 2023-02-16,23:37:43.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
36814 2023-02-16,23:37:43.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4008140563964844
36932 2023-02-16,23:37:43.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3947017192840576
37268 2023-02-16,23:37:43.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3360788822174072
37533 2023-02-16,23:37:44.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848416805267334
37757 2023-02-16,23:37:44.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4231411218643188
37862 2023-02-16,23:37:44.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
38108 2023-02-16,23:37:44.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
38255 2023-02-16,23:37:44.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
38362 2023-02-16,23:37:44.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
38487 2023-02-16,23:37:44.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
36932 2023-02-16,23:37:44.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4021317958831787
37035 2023-02-16,23:37:44.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3538269996643066
37152 2023-02-16,23:37:44.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4106156826019287
37983 2023-02-16,23:37:44.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37393 2023-02-16,23:37:44.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3439619541168213
36814 2023-02-16,23:37:44.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4053537845611572
37268 2023-02-16,23:37:44.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.381349802017212
37862 2023-02-16,23:37:44.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
38255 2023-02-16,23:37:44.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
36932 2023-02-16,23:37:44.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4301327466964722
37152 2023-02-16,23:37:44.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.406989336013794
37393 2023-02-16,23:37:44.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.378387212753296
37533 2023-02-16,23:37:44.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3574072122573853
37757 2023-02-16,23:37:44.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3744126558303833
37983 2023-02-16,23:37:44.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
38108 2023-02-16,23:37:44.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
38362 2023-02-16,23:37:44.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
38487 2023-02-16,23:37:44.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
36814 2023-02-16,23:37:44.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4027073383331299
37035 2023-02-16,23:37:44.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3714606761932373
37268 2023-02-16,23:37:44.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4183850288391113
36932 2023-02-16,23:37:44.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3790113925933838
37393 2023-02-16,23:37:44.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3739043474197388
37533 2023-02-16,23:37:44.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3682035207748413
37757 2023-02-16,23:37:44.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3703778982162476
37862 2023-02-16,23:37:44.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
37983 2023-02-16,23:37:44.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
38108 2023-02-16,23:37:44.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
38255 2023-02-16,23:37:44.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
38362 2023-02-16,23:37:44.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
38487 2023-02-16,23:37:44.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
36814 2023-02-16,23:37:44.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.3992671966552734
37035 2023-02-16,23:37:44.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.360802173614502
37268 2023-02-16,23:37:44.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3978180885314941
37152 2023-02-16,23:37:44.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.402687668800354
36932 2023-02-16,23:37:45.033 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.422278642654419
37757 2023-02-16,23:37:45.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3999966382980347
37862 2023-02-16,23:37:45.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37983 2023-02-16,23:37:45.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
38108 2023-02-16,23:37:45.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
38255 2023-02-16,23:37:45.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
38362 2023-02-16,23:37:45.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38487 2023-02-16,23:37:45.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
36814 2023-02-16,23:37:45.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3972814083099365
37035 2023-02-16,23:37:45.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.411255121231079
37152 2023-02-16,23:37:45.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4170383214950562
37268 2023-02-16,23:37:45.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3637887239456177
37393 2023-02-16,23:37:45.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3451861143112183
37533 2023-02-16,23:37:45.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3613303899765015
37533 2023-02-16,23:37:45.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4203002452850342
37757 2023-02-16,23:37:45.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4017772674560547
37862 2023-02-16,23:37:45.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37983 2023-02-16,23:37:45.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
38108 2023-02-16,23:37:45.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
38255 2023-02-16,23:37:45.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
38362 2023-02-16,23:37:45.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
38487 2023-02-16,23:37:45.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
36814 2023-02-16,23:37:45.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4316836595535278
36932 2023-02-16,23:37:45.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3771607875823975
37035 2023-02-16,23:37:45.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4215272665023804
37152 2023-02-16,23:37:45.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.364949107170105
37268 2023-02-16,23:37:45.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3724706172943115
37393 2023-02-16,23:37:45.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3999123573303223
37152 2023-02-16,23:37:45.567 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4084265232086182
37757 2023-02-16,23:37:45.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3844361305236816
37862 2023-02-16,23:37:45.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38255 2023-02-16,23:37:45.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
38362 2023-02-16,23:37:45.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
38487 2023-02-16,23:37:45.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
36932 2023-02-16,23:37:45.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3970216512680054
37035 2023-02-16,23:37:45.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4419059753417969
37268 2023-02-16,23:37:45.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3972464799880981
37393 2023-02-16,23:37:45.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3883768320083618
37533 2023-02-16,23:37:45.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4319050312042236
37983 2023-02-16,23:37:45.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
38108 2023-02-16,23:37:45.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
36814 2023-02-16,23:37:45.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4390666484832764
37862 2023-02-16,23:37:45.828 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
37035 2023-02-16,23:37:45.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.368981957435608
37268 2023-02-16,23:37:45.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.424929141998291
37533 2023-02-16,23:37:45.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3962899446487427
37757 2023-02-16,23:37:45.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3785078525543213
37983 2023-02-16,23:37:45.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
38255 2023-02-16,23:37:45.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
38362 2023-02-16,23:37:45.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
38487 2023-02-16,23:37:45.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
36814 2023-02-16,23:37:45.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4072129726409912
36932 2023-02-16,23:37:45.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.355787992477417
37152 2023-02-16,23:37:45.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.407375693321228
37393 2023-02-16,23:37:45.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3493757247924805
38108 2023-02-16,23:37:45.862 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37035 2023-02-16,23:37:46.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3965072631835938
37268 2023-02-16,23:37:46.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.412340760231018
37533 2023-02-16,23:37:46.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3706778287887573
37862 2023-02-16,23:37:46.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
37983 2023-02-16,23:37:46.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
38255 2023-02-16,23:37:46.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
38487 2023-02-16,23:37:46.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
38362 2023-02-16,23:37:46.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
36814 2023-02-16,23:37:46.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4327982664108276
36932 2023-02-16,23:37:46.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4235432147979736
37152 2023-02-16,23:37:46.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4149117469787598
37393 2023-02-16,23:37:46.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.359749674797058
37757 2023-02-16,23:37:46.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.441704273223877
38108 2023-02-16,23:37:46.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37035 2023-02-16,23:37:46.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.412277102470398
37533 2023-02-16,23:37:46.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4281337261199951
37862 2023-02-16,23:37:46.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
37152 2023-02-16,23:37:46.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.39173424243927
37268 2023-02-16,23:37:46.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4321520328521729
37757 2023-02-16,23:37:46.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3563814163208008
37983 2023-02-16,23:37:46.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
38108 2023-02-16,23:37:46.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
38255 2023-02-16,23:37:46.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38362 2023-02-16,23:37:46.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
38487 2023-02-16,23:37:46.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
36814 2023-02-16,23:37:46.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4380347728729248
36932 2023-02-16,23:37:46.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3830009698867798
37393 2023-02-16,23:37:46.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3677467107772827
37533 2023-02-16,23:37:46.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.424612283706665
37862 2023-02-16,23:37:46.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
37035 2023-02-16,23:37:46.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4049601554870605
37152 2023-02-16,23:37:46.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3321938514709473
37268 2023-02-16,23:37:46.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3582416772842407
37393 2023-02-16,23:37:46.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4010236263275146
37757 2023-02-16,23:37:46.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4245340824127197
37983 2023-02-16,23:37:46.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
38108 2023-02-16,23:37:46.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
38255 2023-02-16,23:37:46.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38362 2023-02-16,23:37:46.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
38487 2023-02-16,23:37:46.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
36814 2023-02-16,23:37:46.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3731763362884521
36932 2023-02-16,23:37:46.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3969160318374634
37035 2023-02-16,23:37:46.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4025062322616577
37268 2023-02-16,23:37:46.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3213762044906616
37533 2023-02-16,23:37:46.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3963202238082886
37757 2023-02-16,23:37:46.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3931273221969604
37862 2023-02-16,23:37:46.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
37983 2023-02-16,23:37:46.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
38108 2023-02-16,23:37:46.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
38255 2023-02-16,23:37:46.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
38362 2023-02-16,23:37:46.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
38487 2023-02-16,23:37:46.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
36814 2023-02-16,23:37:46.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.403322696685791
36932 2023-02-16,23:37:46.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3948789834976196
37152 2023-02-16,23:37:46.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3884769678115845
37393 2023-02-16,23:37:46.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3861992359161377
37035 2023-02-16,23:37:47.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3960760831832886
37268 2023-02-16,23:37:47.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.378774642944336
37862 2023-02-16,23:37:47.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
38108 2023-02-16,23:37:47.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
38255 2023-02-16,23:37:47.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
38362 2023-02-16,23:37:47.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
38487 2023-02-16,23:37:47.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
36814 2023-02-16,23:37:47.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.419925332069397
36932 2023-02-16,23:37:47.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.386473298072815
37152 2023-02-16,23:37:47.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.389127254486084
37533 2023-02-16,23:37:47.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4126453399658203
37757 2023-02-16,23:37:47.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4014761447906494
37983 2023-02-16,23:37:47.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37393 2023-02-16,23:37:47.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4560245275497437
37862 2023-02-16,23:37:47.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
36814 2023-02-16,23:37:47.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4163386821746826
37035 2023-02-16,23:37:47.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4425549507141113
37268 2023-02-16,23:37:47.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.330734133720398
37533 2023-02-16,23:37:47.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3717856407165527
37757 2023-02-16,23:37:47.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4182064533233643
37983 2023-02-16,23:37:47.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38108 2023-02-16,23:37:47.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
38255 2023-02-16,23:37:47.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
38362 2023-02-16,23:37:47.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
38487 2023-02-16,23:37:47.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
36932 2023-02-16,23:37:47.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4067838191986084
37152 2023-02-16,23:37:47.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4244110584259033
37393 2023-02-16,23:37:47.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4058083295822144
37035 2023-02-16,23:37:47.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.397043228149414
37268 2023-02-16,23:37:47.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.386290431022644
37862 2023-02-16,23:37:47.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37983 2023-02-16,23:37:47.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
38108 2023-02-16,23:37:47.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
38255 2023-02-16,23:37:47.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
38362 2023-02-16,23:37:47.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
38487 2023-02-16,23:37:47.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
36814 2023-02-16,23:37:47.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4337329864501953
36932 2023-02-16,23:37:47.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4031591415405273
37152 2023-02-16,23:37:47.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4013829231262207
37393 2023-02-16,23:37:47.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.38481605052948
37533 2023-02-16,23:37:47.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.375058889389038
37757 2023-02-16,23:37:47.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3967396020889282
37862 2023-02-16,23:37:47.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
36814 2023-02-16,23:37:48.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.3997762203216553
37268 2023-02-16,23:37:48.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4052448272705078
37983 2023-02-16,23:37:48.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
38108 2023-02-16,23:37:48.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
38255 2023-02-16,23:37:48.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
38362 2023-02-16,23:37:48.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
38487 2023-02-16,23:37:48.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
36932 2023-02-16,23:37:48.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4138238430023193
37035 2023-02-16,23:37:48.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.349552869796753
37152 2023-02-16,23:37:48.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4074461460113525
37393 2023-02-16,23:37:48.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3917524814605713
37533 2023-02-16,23:37:48.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3913233280181885
37757 2023-02-16,23:37:48.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3373470306396484
37862 2023-02-16,23:37:48.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
36814 2023-02-16,23:37:48.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4186519384384155
37268 2023-02-16,23:37:48.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.388988971710205
37983 2023-02-16,23:37:48.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
38255 2023-02-16,23:37:48.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
38362 2023-02-16,23:37:48.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
38487 2023-02-16,23:37:48.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
36932 2023-02-16,23:37:48.290 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4123878479003906
37035 2023-02-16,23:37:48.290 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3483127355575562
37152 2023-02-16,23:37:48.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.389288306236267
37393 2023-02-16,23:37:48.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.338568925857544
37533 2023-02-16,23:37:48.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3856987953186035
37757 2023-02-16,23:37:48.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3641064167022705
38108 2023-02-16,23:37:48.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37862 2023-02-16,23:37:48.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
36814 2023-02-16,23:37:48.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.393747091293335
37268 2023-02-16,23:37:48.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.384400486946106
37393 2023-02-16,23:37:48.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3765202760696411
37533 2023-02-16,23:37:48.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4024930000305176
37983 2023-02-16,23:37:48.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
38255 2023-02-16,23:37:48.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
38362 2023-02-16,23:37:48.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
38487 2023-02-16,23:37:48.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
36932 2023-02-16,23:37:48.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4241338968276978
37035 2023-02-16,23:37:48.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4209002256393433
37152 2023-02-16,23:37:48.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3795912265777588
37757 2023-02-16,23:37:48.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3465557098388672
38108 2023-02-16,23:37:48.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37152 2023-02-16,23:37:48.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3508524894714355
37393 2023-02-16,23:37:48.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.409721851348877
36814 2023-02-16,23:37:48.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4546300172805786
37268 2023-02-16,23:37:48.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3628345727920532
37533 2023-02-16,23:37:48.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.407083511352539
37757 2023-02-16,23:37:48.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3511523008346558
37862 2023-02-16,23:37:48.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
37983 2023-02-16,23:37:48.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38108 2023-02-16,23:37:48.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
38255 2023-02-16,23:37:48.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
38362 2023-02-16,23:37:48.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
38487 2023-02-16,23:37:48.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
36932 2023-02-16,23:37:48.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.379174828529358
37035 2023-02-16,23:37:48.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4310953617095947
37152 2023-02-16,23:37:49.067 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3806662559509277
38108 2023-02-16,23:37:49.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37393 2023-02-16,23:37:49.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3807203769683838
37533 2023-02-16,23:37:49.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.397746205329895
37862 2023-02-16,23:37:49.106 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37983 2023-02-16,23:37:49.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
38362 2023-02-16,23:37:49.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
38487 2023-02-16,23:37:49.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
36814 2023-02-16,23:37:49.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3520171642303467
36932 2023-02-16,23:37:49.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4074325561523438
37035 2023-02-16,23:37:49.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3872970342636108
37268 2023-02-16,23:37:49.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3557919263839722
37757 2023-02-16,23:37:49.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3885964155197144
38255 2023-02-16,23:37:49.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37152 2023-02-16,23:37:49.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4002211093902588
38108 2023-02-16,23:37:49.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
38255 2023-02-16,23:37:49.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
38362 2023-02-16,23:37:49.379 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
38487 2023-02-16,23:37:49.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
36814 2023-02-16,23:37:49.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.416731595993042
36932 2023-02-16,23:37:49.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3775622844696045
37035 2023-02-16,23:37:49.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4314448833465576
37268 2023-02-16,23:37:49.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3636722564697266
37393 2023-02-16,23:37:49.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.383054494857788
37757 2023-02-16,23:37:49.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.344197154045105
37862 2023-02-16,23:37:49.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
37983 2023-02-16,23:37:49.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37533 2023-02-16,23:37:49.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.427146553993225
38108 2023-02-16,23:37:49.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
38255 2023-02-16,23:37:49.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37152 2023-02-16,23:37:49.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3618264198303223
37393 2023-02-16,23:37:49.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3755590915679932
37268 2023-02-16,23:37:49.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3595513105392456
37862 2023-02-16,23:37:49.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
37983 2023-02-16,23:37:49.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
38362 2023-02-16,23:37:49.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
38487 2023-02-16,23:37:49.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
36814 2023-02-16,23:37:49.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.378026008605957
36932 2023-02-16,23:37:49.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.376006841659546
37035 2023-02-16,23:37:49.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3562308549880981
37533 2023-02-16,23:37:49.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4387844800949097
37757 2023-02-16,23:37:49.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3682498931884766
38108 2023-02-16,23:37:49.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37862 2023-02-16,23:37:49.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
37983 2023-02-16,23:37:49.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
38255 2023-02-16,23:37:49.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
38362 2023-02-16,23:37:49.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
38487 2023-02-16,23:37:49.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
36814 2023-02-16,23:37:49.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3171792030334473
36932 2023-02-16,23:37:49.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3804532289505005
37035 2023-02-16,23:37:49.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3831920623779297
37152 2023-02-16,23:37:49.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.3944861888885498
37268 2023-02-16,23:37:49.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.369566559791565
37393 2023-02-16,23:37:49.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3823308944702148
37533 2023-02-16,23:37:49.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4185535907745361
37757 2023-02-16,23:37:49.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4136464595794678
38108 2023-02-16,23:37:50.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
38362 2023-02-16,23:37:50.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
38487 2023-02-16,23:37:50.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
36814 2023-02-16,23:37:50.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.363124132156372
36932 2023-02-16,23:37:50.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3263992071151733
37152 2023-02-16,23:37:50.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.378619909286499
37533 2023-02-16,23:37:50.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.42649245262146
37757 2023-02-16,23:37:50.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3541839122772217
37983 2023-02-16,23:37:50.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
38255 2023-02-16,23:37:50.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37035 2023-02-16,23:37:50.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3774420022964478
37268 2023-02-16,23:37:50.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3861101865768433
37393 2023-02-16,23:37:50.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4294288158416748
37862 2023-02-16,23:37:50.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
38362 2023-02-16,23:37:50.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38487 2023-02-16,23:37:50.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
36814 2023-02-16,23:37:50.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4367036819458008
37533 2023-02-16,23:37:50.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3941738605499268
37983 2023-02-16,23:37:50.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
38108 2023-02-16,23:37:50.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
38255 2023-02-16,23:37:50.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
36932 2023-02-16,23:37:50.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3785549402236938
37035 2023-02-16,23:37:50.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3767021894454956
37152 2023-02-16,23:37:50.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3939577341079712
37268 2023-02-16,23:37:50.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4572043418884277
37393 2023-02-16,23:37:50.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.379683017730713
37757 2023-02-16,23:37:50.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.359212875366211
37862 2023-02-16,23:37:50.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.395155906677246
38362 2023-02-16,23:37:50.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38487 2023-02-16,23:37:50.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
36814 2023-02-16,23:37:50.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.394286870956421
37035 2023-02-16,23:37:50.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3914153575897217
37983 2023-02-16,23:37:50.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
38108 2023-02-16,23:37:50.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
38255 2023-02-16,23:37:50.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
36932 2023-02-16,23:37:50.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3474068641662598
37152 2023-02-16,23:37:50.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4169211387634277
37268 2023-02-16,23:37:50.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3975818157196045
37393 2023-02-16,23:37:50.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3368580341339111
37533 2023-02-16,23:37:50.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4029922485351562
37757 2023-02-16,23:37:50.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3847991228103638
37862 2023-02-16,23:37:50.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4115142822265625
38108 2023-02-16,23:37:51.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
38362 2023-02-16,23:37:51.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
38487 2023-02-16,23:37:51.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
36814 2023-02-16,23:37:51.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3411844968795776
36932 2023-02-16,23:37:51.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4013530015945435
37035 2023-02-16,23:37:51.033 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.393304467201233
37152 2023-02-16,23:37:51.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.3995896577835083
37268 2023-02-16,23:37:51.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3976852893829346
37393 2023-02-16,23:37:51.035 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3815521001815796
37533 2023-02-16,23:37:51.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4430021047592163
37757 2023-02-16,23:37:51.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3571122884750366
37862 2023-02-16,23:37:51.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4231401681900024
37983 2023-02-16,23:37:51.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
38255 2023-02-16,23:37:51.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
38108 2023-02-16,23:37:51.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
38362 2023-02-16,23:37:51.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
38487 2023-02-16,23:37:51.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
36814 2023-02-16,23:37:51.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.391595482826233
36932 2023-02-16,23:37:51.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4193428754806519
37035 2023-02-16,23:37:51.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3657783269882202
37152 2023-02-16,23:37:51.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.391650915145874
37268 2023-02-16,23:37:51.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410296440124512
37393 2023-02-16,23:37:51.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4193131923675537
37533 2023-02-16,23:37:51.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3769022226333618
37757 2023-02-16,23:37:51.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3678629398345947
37862 2023-02-16,23:37:51.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3743586540222168
37983 2023-02-16,23:37:51.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
38255 2023-02-16,23:37:51.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
38487 2023-02-16,23:37:51.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
38108 2023-02-16,23:37:51.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
38362 2023-02-16,23:37:51.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
36932 2023-02-16,23:37:51.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4113267660140991
37035 2023-02-16,23:37:51.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4094343185424805
37152 2023-02-16,23:37:51.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3971340656280518
37268 2023-02-16,23:37:51.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4152040481567383
37393 2023-02-16,23:37:51.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3977735042572021
37533 2023-02-16,23:37:51.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.407391905784607
37757 2023-02-16,23:37:51.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.361407995223999
37862 2023-02-16,23:37:51.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3703480958938599
37983 2023-02-16,23:37:51.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
38255 2023-02-16,23:37:51.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
36814 2023-02-16,23:37:51.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3879241943359375
38108 2023-02-16,23:37:51.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
38362 2023-02-16,23:37:51.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
38487 2023-02-16,23:37:51.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
36932 2023-02-16,23:37:51.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.394294261932373
37035 2023-02-16,23:37:51.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3842593431472778
37152 2023-02-16,23:37:51.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.386256456375122
37268 2023-02-16,23:37:51.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3823500871658325
37393 2023-02-16,23:37:51.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3641806840896606
37533 2023-02-16,23:37:51.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.366267204284668
37757 2023-02-16,23:37:51.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4207611083984375
37862 2023-02-16,23:37:51.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3999621868133545
37983 2023-02-16,23:37:51.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38255 2023-02-16,23:37:51.862 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
36814 2023-02-16,23:37:51.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3828319311141968
38108 2023-02-16,23:37:52.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
38255 2023-02-16,23:37:52.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
38362 2023-02-16,23:37:52.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
36932 2023-02-16,23:37:52.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3941255807876587
37035 2023-02-16,23:37:52.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.394524335861206
37152 2023-02-16,23:37:52.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.369992971420288
37268 2023-02-16,23:37:52.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.411899447441101
37393 2023-02-16,23:37:52.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3732750415802002
37533 2023-02-16,23:37:52.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3707412481307983
37757 2023-02-16,23:37:52.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4325580596923828
37862 2023-02-16,23:37:52.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4019347429275513
37983 2023-02-16,23:37:52.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
38487 2023-02-16,23:37:52.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
36814 2023-02-16,23:37:52.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4133858680725098
38108 2023-02-16,23:37:52.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
38255 2023-02-16,23:37:52.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
38362 2023-02-16,23:37:52.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
36932 2023-02-16,23:37:52.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3922840356826782
37035 2023-02-16,23:37:52.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4002799987792969
37152 2023-02-16,23:37:52.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4052870273590088
37268 2023-02-16,23:37:52.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3534873723983765
37533 2023-02-16,23:37:52.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3980028629302979
37393 2023-02-16,23:37:52.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.397195816040039
37757 2023-02-16,23:37:52.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3970050811767578
37862 2023-02-16,23:37:52.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.384591817855835
37983 2023-02-16,23:37:52.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
38487 2023-02-16,23:37:52.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
36814 2023-02-16,23:37:52.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.41017746925354
36932 2023-02-16,23:37:52.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4443373680114746
37035 2023-02-16,23:37:52.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4292998313903809
37152 2023-02-16,23:37:52.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3933663368225098
37268 2023-02-16,23:37:52.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.379069447517395
37393 2023-02-16,23:37:52.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4260740280151367
37533 2023-02-16,23:37:52.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.380196213722229
37757 2023-02-16,23:37:52.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.370586633682251
37862 2023-02-16,23:37:52.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3785134553909302
37983 2023-02-16,23:37:52.691 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
38108 2023-02-16,23:37:52.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
38255 2023-02-16,23:37:52.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
38362 2023-02-16,23:37:52.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
38487 2023-02-16,23:37:52.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
36814 2023-02-16,23:37:52.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4097870588302612
36932 2023-02-16,23:37:52.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3761820793151855
37035 2023-02-16,23:37:52.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3788965940475464
37152 2023-02-16,23:37:52.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.372281789779663
37268 2023-02-16,23:37:52.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3478279113769531
37533 2023-02-16,23:37:52.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.365285873413086
37757 2023-02-16,23:37:52.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4286038875579834
37393 2023-02-16,23:37:52.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4124140739440918
37862 2023-02-16,23:37:52.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4417126178741455
37983 2023-02-16,23:37:52.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
38108 2023-02-16,23:37:52.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
38255 2023-02-16,23:37:52.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
38362 2023-02-16,23:37:52.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
38487 2023-02-16,23:37:52.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
36814 2023-02-16,23:37:52.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4189056158065796
37035 2023-02-16,23:37:53.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4235635995864868
36932 2023-02-16,23:37:53.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3518643379211426
37268 2023-02-16,23:37:53.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4002636671066284
37862 2023-02-16,23:37:53.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3567512035369873
37983 2023-02-16,23:37:53.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
38108 2023-02-16,23:37:53.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
38255 2023-02-16,23:37:53.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
38362 2023-02-16,23:37:53.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
38487 2023-02-16,23:37:53.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
36814 2023-02-16,23:37:53.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4197280406951904
37152 2023-02-16,23:37:53.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.354418396949768
37393 2023-02-16,23:37:53.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4326889514923096
37533 2023-02-16,23:37:53.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.358467698097229
37757 2023-02-16,23:37:53.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4252269268035889
37035 2023-02-16,23:37:53.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3766748905181885
37533 2023-02-16,23:37:53.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.354203224182129
36932 2023-02-16,23:37:53.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4008558988571167
37152 2023-02-16,23:37:53.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3843965530395508
37268 2023-02-16,23:37:53.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3734475374221802
37393 2023-02-16,23:37:53.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3578462600708008
37862 2023-02-16,23:37:53.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4244186878204346
37983 2023-02-16,23:37:53.524 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
38108 2023-02-16,23:37:53.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38255 2023-02-16,23:37:53.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
38362 2023-02-16,23:37:53.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
38487 2023-02-16,23:37:53.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
36814 2023-02-16,23:37:53.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.3996233940124512
37757 2023-02-16,23:37:53.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3969017267227173
37035 2023-02-16,23:37:53.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.397623062133789
37533 2023-02-16,23:37:53.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.429702877998352
36932 2023-02-16,23:37:53.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3596875667572021
37152 2023-02-16,23:37:53.799 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4380871057510376
37268 2023-02-16,23:37:53.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4614611864089966
37757 2023-02-16,23:37:53.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4127919673919678
37393 2023-02-16,23:37:53.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3213680982589722
37862 2023-02-16,23:37:53.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3930782079696655
37983 2023-02-16,23:37:53.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
38108 2023-02-16,23:37:53.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
38255 2023-02-16,23:37:53.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
38362 2023-02-16,23:37:53.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
38487 2023-02-16,23:37:53.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
36814 2023-02-16,23:37:53.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4036623239517212
36932 2023-02-16,23:37:54.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3784220218658447
37035 2023-02-16,23:37:54.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3544409275054932
37268 2023-02-16,23:37:54.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3475595712661743
37393 2023-02-16,23:37:54.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3789799213409424
37533 2023-02-16,23:37:54.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4320118427276611
37757 2023-02-16,23:37:54.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3716790676116943
37862 2023-02-16,23:37:54.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4015629291534424
37983 2023-02-16,23:37:54.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
38108 2023-02-16,23:37:54.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
38255 2023-02-16,23:37:54.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
38362 2023-02-16,23:37:54.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
38487 2023-02-16,23:37:54.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
36814 2023-02-16,23:37:54.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3673275709152222
37152 2023-02-16,23:37:54.089 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.372125506401062
37533 2023-02-16,23:37:54.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3834034204483032
38108 2023-02-16,23:37:54.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
38255 2023-02-16,23:37:54.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
38362 2023-02-16,23:37:54.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
38487 2023-02-16,23:37:54.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
36814 2023-02-16,23:37:54.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4164977073669434
36932 2023-02-16,23:37:54.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4408972263336182
37035 2023-02-16,23:37:54.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4247854948043823
37152 2023-02-16,23:37:54.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.393388271331787
37268 2023-02-16,23:37:54.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4065452814102173
37393 2023-02-16,23:37:54.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3309109210968018
37862 2023-02-16,23:37:54.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4181574583053589
37757 2023-02-16,23:37:54.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.375171184539795
37983 2023-02-16,23:37:54.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
37533 2023-02-16,23:37:54.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3741803169250488
38362 2023-02-16,23:37:54.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
36932 2023-02-16,23:37:54.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3791239261627197
37035 2023-02-16,23:37:54.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.382643461227417
37152 2023-02-16,23:37:54.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3242679834365845
37393 2023-02-16,23:37:54.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.385472297668457
37862 2023-02-16,23:37:54.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.396691083908081
37983 2023-02-16,23:37:54.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
38108 2023-02-16,23:37:54.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
38255 2023-02-16,23:37:54.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
38487 2023-02-16,23:37:54.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
36814 2023-02-16,23:37:54.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4345741271972656
37268 2023-02-16,23:37:54.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4005515575408936
37757 2023-02-16,23:37:54.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3915222883224487
36932 2023-02-16,23:37:54.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4116719961166382
37035 2023-02-16,23:37:54.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3958061933517456
37393 2023-02-16,23:37:54.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4063794612884521
37533 2023-02-16,23:37:54.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4001314640045166
37862 2023-02-16,23:37:54.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372548818588257
37983 2023-02-16,23:37:54.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
38108 2023-02-16,23:37:54.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38255 2023-02-16,23:37:54.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
38362 2023-02-16,23:37:54.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
38487 2023-02-16,23:37:54.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
36814 2023-02-16,23:37:54.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4179246425628662
37268 2023-02-16,23:37:54.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.390444040298462
37757 2023-02-16,23:37:54.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3861420154571533
37152 2023-02-16,23:37:54.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3732037544250488
37035 2023-02-16,23:37:55.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3945660591125488
37393 2023-02-16,23:37:55.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3899974822998047
37533 2023-02-16,23:37:55.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4029535055160522
37862 2023-02-16,23:37:55.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.364069938659668
37983 2023-02-16,23:37:55.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
38108 2023-02-16,23:37:55.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
38255 2023-02-16,23:37:55.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
38362 2023-02-16,23:37:55.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
38487 2023-02-16,23:37:55.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
36814 2023-02-16,23:37:55.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4097328186035156
36932 2023-02-16,23:37:55.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3880164623260498
37152 2023-02-16,23:37:55.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4153635501861572
37268 2023-02-16,23:37:55.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4022860527038574
37757 2023-02-16,23:37:55.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4028148651123047
37393 2023-02-16,23:37:55.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3847472667694092
37533 2023-02-16,23:37:55.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3815242052078247
37862 2023-02-16,23:37:55.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.346639633178711
38108 2023-02-16,23:37:55.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
38255 2023-02-16,23:37:55.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
38362 2023-02-16,23:37:55.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
36814 2023-02-16,23:37:55.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3782553672790527
38487 2023-02-16,23:37:55.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
36932 2023-02-16,23:37:55.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3860960006713867
37035 2023-02-16,23:37:55.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3859760761260986
37152 2023-02-16,23:37:55.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3451735973358154
37268 2023-02-16,23:37:55.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3367935419082642
37757 2023-02-16,23:37:55.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.407848834991455
37983 2023-02-16,23:37:55.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
37533 2023-02-16,23:37:55.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3924992084503174
37862 2023-02-16,23:37:55.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.351152777671814
38255 2023-02-16,23:37:55.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
38362 2023-02-16,23:37:55.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
38487 2023-02-16,23:37:55.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
36932 2023-02-16,23:37:55.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3803631067276
37035 2023-02-16,23:37:55.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.406104326248169
37152 2023-02-16,23:37:55.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3753552436828613
37268 2023-02-16,23:37:55.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3735406398773193
37393 2023-02-16,23:37:55.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.363397479057312
37757 2023-02-16,23:37:55.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3978571891784668
37983 2023-02-16,23:37:55.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
38108 2023-02-16,23:37:55.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
36814 2023-02-16,23:37:55.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.388359785079956
38362 2023-02-16,23:37:56.035 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
36932 2023-02-16,23:37:56.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3852237462997437
37035 2023-02-16,23:37:56.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4040136337280273
37152 2023-02-16,23:37:56.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3779888153076172
37268 2023-02-16,23:37:56.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3829845190048218
37393 2023-02-16,23:37:56.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3565146923065186
37533 2023-02-16,23:37:56.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3982046842575073
37757 2023-02-16,23:37:56.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4279705286026
37862 2023-02-16,23:37:56.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3887264728546143
37983 2023-02-16,23:37:56.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
38108 2023-02-16,23:37:56.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
38255 2023-02-16,23:37:56.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
38487 2023-02-16,23:37:56.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
36814 2023-02-16,23:37:56.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3797569274902344
37035 2023-02-16,23:37:56.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4139608144760132
37152 2023-02-16,23:37:56.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3759467601776123
37393 2023-02-16,23:37:56.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3649353981018066
37757 2023-02-16,23:37:56.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4400378465652466
37862 2023-02-16,23:37:56.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441855907440186
37983 2023-02-16,23:37:56.328 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
38108 2023-02-16,23:37:56.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
38255 2023-02-16,23:37:56.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
38362 2023-02-16,23:37:56.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
38487 2023-02-16,23:37:56.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
36932 2023-02-16,23:37:56.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4389272928237915
37268 2023-02-16,23:37:56.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3723622560501099
37533 2023-02-16,23:37:56.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4335793256759644
36814 2023-02-16,23:37:56.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.370602011680603
37393 2023-02-16,23:37:56.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3599474430084229
37862 2023-02-16,23:37:56.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.368231177330017
38108 2023-02-16,23:37:56.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
38255 2023-02-16,23:37:56.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
38362 2023-02-16,23:37:56.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
38487 2023-02-16,23:37:56.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
36932 2023-02-16,23:37:56.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.379244327545166
37035 2023-02-16,23:37:56.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4125640392303467
37152 2023-02-16,23:37:56.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4124737977981567
37268 2023-02-16,23:37:56.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3822706937789917
37533 2023-02-16,23:37:56.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4012713432312012
37757 2023-02-16,23:37:56.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4191220998764038
36814 2023-02-16,23:37:56.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.420889973640442
37983 2023-02-16,23:37:56.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37152 2023-02-16,23:37:56.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.381927251815796
37393 2023-02-16,23:37:56.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3698971271514893
37533 2023-02-16,23:37:56.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.385624647140503
37757 2023-02-16,23:37:56.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4270707368850708
37862 2023-02-16,23:37:56.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.413750410079956
38108 2023-02-16,23:37:56.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
38255 2023-02-16,23:37:56.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
38362 2023-02-16,23:37:56.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
38487 2023-02-16,23:37:56.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
36814 2023-02-16,23:37:56.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.395334005355835
36932 2023-02-16,23:37:56.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3941649198532104
37035 2023-02-16,23:37:56.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4241799116134644
37268 2023-02-16,23:37:56.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4127488136291504
37983 2023-02-16,23:37:56.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3951470851898193
37152 2023-02-16,23:37:57.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4480020999908447
37393 2023-02-16,23:37:57.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3862577676773071
37533 2023-02-16,23:37:57.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4448442459106445
37757 2023-02-16,23:37:57.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3946223258972168
37862 2023-02-16,23:37:57.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3542490005493164
38255 2023-02-16,23:37:57.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
38362 2023-02-16,23:37:57.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
38487 2023-02-16,23:37:57.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
36814 2023-02-16,23:37:57.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.401688575744629
36932 2023-02-16,23:37:57.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.372875452041626
37035 2023-02-16,23:37:57.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3782907724380493
37268 2023-02-16,23:37:57.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3557453155517578
38108 2023-02-16,23:37:57.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
37983 2023-02-16,23:37:57.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4115842580795288
37533 2023-02-16,23:37:57.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.411427617073059
37862 2023-02-16,23:37:57.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3591903448104858
38255 2023-02-16,23:37:57.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
38362 2023-02-16,23:37:57.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
38487 2023-02-16,23:37:57.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
36814 2023-02-16,23:37:57.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3663750886917114
36932 2023-02-16,23:37:57.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3745392560958862
37035 2023-02-16,23:37:57.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4069428443908691
37152 2023-02-16,23:37:57.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4438267946243286
37268 2023-02-16,23:37:57.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4534831047058105
37393 2023-02-16,23:37:57.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.45831298828125
37757 2023-02-16,23:37:57.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.403804063796997
37983 2023-02-16,23:37:57.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4231858253479004
38108 2023-02-16,23:37:57.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
37862 2023-02-16,23:37:57.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848730325698853
38362 2023-02-16,23:37:57.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
36814 2023-02-16,23:37:57.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3704404830932617
36932 2023-02-16,23:37:57.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3871688842773438
37152 2023-02-16,23:37:57.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3449848890304565
37268 2023-02-16,23:37:57.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.382018804550171
37393 2023-02-16,23:37:57.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3979763984680176
37533 2023-02-16,23:37:57.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4310139417648315
37757 2023-02-16,23:37:57.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.443543553352356
37983 2023-02-16,23:37:57.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3742746114730835
38108 2023-02-16,23:37:57.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
38255 2023-02-16,23:37:57.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
38487 2023-02-16,23:37:57.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37035 2023-02-16,23:37:57.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3771352767944336
36932 2023-02-16,23:37:58.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3898662328720093
37152 2023-02-16,23:37:58.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4051450490951538
37393 2023-02-16,23:37:58.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.397695779800415
37533 2023-02-16,23:37:58.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3979250192642212
37757 2023-02-16,23:37:58.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.377148151397705
37862 2023-02-16,23:37:58.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3571302890777588
37983 2023-02-16,23:37:58.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3702974319458008
38108 2023-02-16,23:37:58.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38255 2023-02-16,23:37:58.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
38362 2023-02-16,23:37:58.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
38487 2023-02-16,23:37:58.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
36814 2023-02-16,23:37:58.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.379662275314331
37268 2023-02-16,23:37:58.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.406538486480713
37035 2023-02-16,23:37:58.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3737050294876099
37152 2023-02-16,23:37:58.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4688599109649658
37393 2023-02-16,23:37:58.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410896062850952
37533 2023-02-16,23:37:58.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4519556760787964
38108 2023-02-16,23:37:58.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
38255 2023-02-16,23:37:58.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
38362 2023-02-16,23:37:58.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
38487 2023-02-16,23:37:58.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
36814 2023-02-16,23:37:58.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3974995613098145
36932 2023-02-16,23:37:58.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3823646306991577
37268 2023-02-16,23:37:58.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3940540552139282
37757 2023-02-16,23:37:58.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4075937271118164
37862 2023-02-16,23:37:58.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.367995023727417
37983 2023-02-16,23:37:58.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3999793529510498
37035 2023-02-16,23:37:58.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3806437253952026
36814 2023-02-16,23:37:58.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.45657217502594
36932 2023-02-16,23:37:58.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3620076179504395
37152 2023-02-16,23:37:58.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3866475820541382
37393 2023-02-16,23:37:58.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4155718088150024
37533 2023-02-16,23:37:58.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4384081363677979
37757 2023-02-16,23:37:58.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3665121793746948
37862 2023-02-16,23:37:58.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3613202571868896
37983 2023-02-16,23:37:58.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4019192457199097
38108 2023-02-16,23:37:58.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
38362 2023-02-16,23:37:58.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
38487 2023-02-16,23:37:58.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38255 2023-02-16,23:37:58.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37035 2023-02-16,23:37:58.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3248792886734009
37268 2023-02-16,23:37:58.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.425074577331543
37533 2023-02-16,23:37:58.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4017009735107422
37862 2023-02-16,23:37:58.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4207258224487305
38362 2023-02-16,23:37:58.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
38487 2023-02-16,23:37:58.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
36814 2023-02-16,23:37:58.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4066638946533203
36932 2023-02-16,23:37:58.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677101135253906
37035 2023-02-16,23:37:58.902 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3780293464660645
37152 2023-02-16,23:37:58.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3800982236862183
37268 2023-02-16,23:37:58.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.399694800376892
37393 2023-02-16,23:37:58.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3835053443908691
37757 2023-02-16,23:37:58.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3709079027175903
37983 2023-02-16,23:37:58.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3844958543777466
38108 2023-02-16,23:37:58.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
38255 2023-02-16,23:37:58.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
36814 2023-02-16,23:37:59.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.407085657119751
36932 2023-02-16,23:37:59.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3491488695144653
37152 2023-02-16,23:37:59.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.40005362033844
37268 2023-02-16,23:37:59.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3962740898132324
37393 2023-02-16,23:37:59.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4126527309417725
37533 2023-02-16,23:37:59.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3925131559371948
37757 2023-02-16,23:37:59.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3987334966659546
37862 2023-02-16,23:37:59.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4325342178344727
37983 2023-02-16,23:37:59.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3784877061843872
38108 2023-02-16,23:37:59.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
38255 2023-02-16,23:37:59.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
38362 2023-02-16,23:37:59.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
38487 2023-02-16,23:37:59.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
37035 2023-02-16,23:37:59.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3459783792495728
37393 2023-02-16,23:37:59.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3537815809249878
37533 2023-02-16,23:37:59.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4074361324310303
37862 2023-02-16,23:37:59.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3971368074417114
38108 2023-02-16,23:37:59.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
38362 2023-02-16,23:37:59.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
38487 2023-02-16,23:37:59.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
36814 2023-02-16,23:37:59.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3729681968688965
36932 2023-02-16,23:37:59.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.411813497543335
37035 2023-02-16,23:37:59.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4010379314422607
37152 2023-02-16,23:37:59.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3734556436538696
37268 2023-02-16,23:37:59.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4233243465423584
37757 2023-02-16,23:37:59.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3799289464950562
37983 2023-02-16,23:37:59.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4417836666107178
38255 2023-02-16,23:37:59.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
36814 2023-02-16,23:37:59.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3719409704208374
36932 2023-02-16,23:37:59.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3806805610656738
37035 2023-02-16,23:37:59.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4191069602966309
37152 2023-02-16,23:37:59.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3921020030975342
37268 2023-02-16,23:37:59.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3874146938323975
37393 2023-02-16,23:37:59.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3789290189743042
37533 2023-02-16,23:37:59.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4126335382461548
37757 2023-02-16,23:37:59.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3652158975601196
37862 2023-02-16,23:37:59.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.370619773864746
37983 2023-02-16,23:37:59.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3566666841506958
38108 2023-02-16,23:37:59.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
38255 2023-02-16,23:37:59.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
38362 2023-02-16,23:37:59.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
38487 2023-02-16,23:37:59.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
36814 2023-02-16,23:38:00.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4060742855072021
37393 2023-02-16,23:38:00.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3479976654052734
37533 2023-02-16,23:38:00.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3760219812393188
37862 2023-02-16,23:38:00.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4286136627197266
37983 2023-02-16,23:38:00.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4245846271514893
38255 2023-02-16,23:38:00.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
38362 2023-02-16,23:38:00.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
38487 2023-02-16,23:38:00.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
36932 2023-02-16,23:38:00.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4355300664901733
37035 2023-02-16,23:38:00.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4107810258865356
37152 2023-02-16,23:38:00.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4167113304138184
37268 2023-02-16,23:38:00.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3838142156600952
37757 2023-02-16,23:38:00.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3582196235656738
38108 2023-02-16,23:38:00.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37533 2023-02-16,23:38:00.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.389320969581604
37862 2023-02-16,23:38:00.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.425236463546753
38255 2023-02-16,23:38:00.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38362 2023-02-16,23:38:00.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
38487 2023-02-16,23:38:00.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
36814 2023-02-16,23:38:00.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.396666407585144
36932 2023-02-16,23:38:00.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4326363801956177
37035 2023-02-16,23:38:00.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3944392204284668
37152 2023-02-16,23:38:00.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4137866497039795
37268 2023-02-16,23:38:00.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4235626459121704
37393 2023-02-16,23:38:00.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4003175497055054
37757 2023-02-16,23:38:00.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3541285991668701
37983 2023-02-16,23:38:00.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.393145203590393
38108 2023-02-16,23:38:00.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
37862 2023-02-16,23:38:00.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3969101905822754
36814 2023-02-16,23:38:00.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3801116943359375
36932 2023-02-16,23:38:00.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4289557933807373
37035 2023-02-16,23:38:00.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3928821086883545
37152 2023-02-16,23:38:00.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3795479536056519
37268 2023-02-16,23:38:00.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.40890371799469
37393 2023-02-16,23:38:00.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3739842176437378
37533 2023-02-16,23:38:00.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.373889684677124
37757 2023-02-16,23:38:00.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4308624267578125
37983 2023-02-16,23:38:00.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.401566505432129
38108 2023-02-16,23:38:00.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
38255 2023-02-16,23:38:00.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
38362 2023-02-16,23:38:00.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
38487 2023-02-16,23:38:00.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
36814 2023-02-16,23:38:00.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.428708791732788
37393 2023-02-16,23:38:00.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.460958480834961
37533 2023-02-16,23:38:00.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3943822383880615
37757 2023-02-16,23:38:00.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.432383418083191
37862 2023-02-16,23:38:00.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4128789901733398
38108 2023-02-16,23:38:00.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
38255 2023-02-16,23:38:00.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
38487 2023-02-16,23:38:00.902 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
36932 2023-02-16,23:38:00.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3671836853027344
37035 2023-02-16,23:38:00.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3922786712646484
37152 2023-02-16,23:38:00.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4068810939788818
37268 2023-02-16,23:38:00.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.40800940990448
37983 2023-02-16,23:38:00.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.418233036994934
38362 2023-02-16,23:38:00.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
36814 2023-02-16,23:38:01.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3896516561508179
36932 2023-02-16,23:38:01.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3757853507995605
37152 2023-02-16,23:38:01.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.461806058883667
37393 2023-02-16,23:38:01.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3464722633361816
37533 2023-02-16,23:38:01.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4035922288894653
37757 2023-02-16,23:38:01.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3834648132324219
37862 2023-02-16,23:38:01.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3717255592346191
37983 2023-02-16,23:38:01.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.396716833114624
38108 2023-02-16,23:38:01.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
38255 2023-02-16,23:38:01.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
38362 2023-02-16,23:38:01.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
38487 2023-02-16,23:38:01.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37035 2023-02-16,23:38:01.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4450284242630005
37268 2023-02-16,23:38:01.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.401456594467163
36814 2023-02-16,23:38:01.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4098857641220093
36932 2023-02-16,23:38:01.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.419104814529419
37393 2023-02-16,23:38:01.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4074965715408325
37533 2023-02-16,23:38:01.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3719842433929443
37757 2023-02-16,23:38:01.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3740864992141724
37862 2023-02-16,23:38:01.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3750131130218506
37983 2023-02-16,23:38:01.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.337222933769226
38108 2023-02-16,23:38:01.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
38255 2023-02-16,23:38:01.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
38362 2023-02-16,23:38:01.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
38487 2023-02-16,23:38:01.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37035 2023-02-16,23:38:01.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3747241497039795
37152 2023-02-16,23:38:01.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4365662336349487
37268 2023-02-16,23:38:01.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.417610764503479
37393 2023-02-16,23:38:01.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4021574258804321
37862 2023-02-16,23:38:01.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3915976285934448
37983 2023-02-16,23:38:01.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3640999794006348
38108 2023-02-16,23:38:01.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
38487 2023-02-16,23:38:01.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
36814 2023-02-16,23:38:01.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4287554025650024
36932 2023-02-16,23:38:01.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3476942777633667
37035 2023-02-16,23:38:01.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3515808582305908
37152 2023-02-16,23:38:01.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3944587707519531
37533 2023-02-16,23:38:01.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.395569086074829
37757 2023-02-16,23:38:01.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.400168776512146
38255 2023-02-16,23:38:01.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38362 2023-02-16,23:38:01.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37268 2023-02-16,23:38:01.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3652129173278809
38487 2023-02-16,23:38:02.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
36814 2023-02-16,23:38:02.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3630611896514893
37152 2023-02-16,23:38:02.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3656551837921143
37393 2023-02-16,23:38:02.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3906153440475464
37533 2023-02-16,23:38:02.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3609739542007446
37757 2023-02-16,23:38:02.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.403120756149292
37862 2023-02-16,23:38:02.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3860269784927368
37983 2023-02-16,23:38:02.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3466603755950928
38108 2023-02-16,23:38:02.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
38255 2023-02-16,23:38:02.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
38362 2023-02-16,23:38:02.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
36932 2023-02-16,23:38:02.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3326654434204102
37035 2023-02-16,23:38:02.058 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4004226922988892
37268 2023-02-16,23:38:02.059 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4094736576080322
37983 2023-02-16,23:38:02.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3511791229248047
38255 2023-02-16,23:38:02.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
38487 2023-02-16,23:38:02.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
36814 2023-02-16,23:38:02.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3681294918060303
36932 2023-02-16,23:38:02.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.415034294128418
37035 2023-02-16,23:38:02.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3600702285766602
37152 2023-02-16,23:38:02.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3523746728897095
37268 2023-02-16,23:38:02.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4066916704177856
37393 2023-02-16,23:38:02.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4033012390136719
37533 2023-02-16,23:38:02.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.387412667274475
37757 2023-02-16,23:38:02.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3817765712738037
37862 2023-02-16,23:38:02.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4028364419937134
38108 2023-02-16,23:38:02.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
38362 2023-02-16,23:38:02.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37983 2023-02-16,23:38:02.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3886209726333618
37393 2023-02-16,23:38:02.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.336880087852478
37862 2023-02-16,23:38:02.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4078309535980225
38108 2023-02-16,23:38:02.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
38255 2023-02-16,23:38:02.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
38487 2023-02-16,23:38:02.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
36814 2023-02-16,23:38:02.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4061859846115112
36932 2023-02-16,23:38:02.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.3998743295669556
37035 2023-02-16,23:38:02.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3779292106628418
37152 2023-02-16,23:38:02.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.388235092163086
37268 2023-02-16,23:38:02.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.415067195892334
37533 2023-02-16,23:38:02.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4176653623580933
37757 2023-02-16,23:38:02.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3932020664215088
38362 2023-02-16,23:38:02.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37862 2023-02-16,23:38:02.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3977959156036377
38487 2023-02-16,23:38:02.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37035 2023-02-16,23:38:02.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4401334524154663
37152 2023-02-16,23:38:02.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3309576511383057
37268 2023-02-16,23:38:02.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3910017013549805
37393 2023-02-16,23:38:02.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3737238645553589
37533 2023-02-16,23:38:02.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4195005893707275
37757 2023-02-16,23:38:02.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3980591297149658
37983 2023-02-16,23:38:02.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441040515899658
38255 2023-02-16,23:38:02.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
38362 2023-02-16,23:38:02.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
36814 2023-02-16,23:38:02.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3257014751434326
36932 2023-02-16,23:38:02.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3718026876449585
38108 2023-02-16,23:38:02.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37862 2023-02-16,23:38:03.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4278903007507324
38487 2023-02-16,23:38:03.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37035 2023-02-16,23:38:03.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3789098262786865
37152 2023-02-16,23:38:03.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3707653284072876
37268 2023-02-16,23:38:03.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3316271305084229
37393 2023-02-16,23:38:03.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3828654289245605
37533 2023-02-16,23:38:03.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3540077209472656
37757 2023-02-16,23:38:03.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4343138933181763
37983 2023-02-16,23:38:03.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.368138074874878
38255 2023-02-16,23:38:03.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
38362 2023-02-16,23:38:03.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
36814 2023-02-16,23:38:03.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3788788318634033
36932 2023-02-16,23:38:03.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4158282279968262
38108 2023-02-16,23:38:03.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.395104169845581
38487 2023-02-16,23:38:03.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
37035 2023-02-16,23:38:03.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4114084243774414
37268 2023-02-16,23:38:03.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3884835243225098
37393 2023-02-16,23:38:03.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3728677034378052
37533 2023-02-16,23:38:03.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4243979454040527
37757 2023-02-16,23:38:03.496 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4016413688659668
37862 2023-02-16,23:38:03.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4399497509002686
37983 2023-02-16,23:38:03.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4137732982635498
38255 2023-02-16,23:38:03.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
38362 2023-02-16,23:38:03.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
36814 2023-02-16,23:38:03.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3671908378601074
36932 2023-02-16,23:38:03.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4337762594223022
37152 2023-02-16,23:38:03.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3622498512268066
38108 2023-02-16,23:38:03.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4115476608276367
37035 2023-02-16,23:38:03.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3876993656158447
37393 2023-02-16,23:38:03.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3817768096923828
37533 2023-02-16,23:38:03.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.389095664024353
37862 2023-02-16,23:38:03.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4190499782562256
37983 2023-02-16,23:38:03.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3540748357772827
38255 2023-02-16,23:38:03.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
38362 2023-02-16,23:38:03.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
38487 2023-02-16,23:38:03.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
36814 2023-02-16,23:38:03.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4136263132095337
36932 2023-02-16,23:38:03.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3810499906539917
37152 2023-02-16,23:38:03.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3705503940582275
37268 2023-02-16,23:38:03.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.389934778213501
37757 2023-02-16,23:38:03.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3858003616333008
38108 2023-02-16,23:38:03.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.423162579536438
37862 2023-02-16,23:38:04.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4269124269485474
37983 2023-02-16,23:38:04.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3591771125793457
38487 2023-02-16,23:38:04.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
36814 2023-02-16,23:38:04.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4245061874389648
36932 2023-02-16,23:38:04.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3599971532821655
37035 2023-02-16,23:38:04.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3863040208816528
37152 2023-02-16,23:38:04.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4444003105163574
37268 2023-02-16,23:38:04.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.424565315246582
37393 2023-02-16,23:38:04.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4117746353149414
37533 2023-02-16,23:38:04.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4008009433746338
37757 2023-02-16,23:38:04.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4460586309432983
38255 2023-02-16,23:38:04.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
38362 2023-02-16,23:38:04.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
38108 2023-02-16,23:38:04.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3742550611495972
38487 2023-02-16,23:38:04.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
36814 2023-02-16,23:38:04.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.331897258758545
37035 2023-02-16,23:38:04.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3790473937988281
37152 2023-02-16,23:38:04.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3982083797454834
37268 2023-02-16,23:38:04.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4023598432540894
37393 2023-02-16,23:38:04.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3557456731796265
37533 2023-02-16,23:38:04.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4072548151016235
37757 2023-02-16,23:38:04.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4120666980743408
37862 2023-02-16,23:38:04.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.394484043121338
37983 2023-02-16,23:38:04.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848052024841309
38255 2023-02-16,23:38:04.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
38362 2023-02-16,23:38:04.380 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
36932 2023-02-16,23:38:04.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4227726459503174
38108 2023-02-16,23:38:04.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.370295524597168
36814 2023-02-16,23:38:04.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.44566810131073
37035 2023-02-16,23:38:04.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3849799633026123
37393 2023-02-16,23:38:04.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4539607763290405
37533 2023-02-16,23:38:04.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4327107667922974
37757 2023-02-16,23:38:04.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4312657117843628
37862 2023-02-16,23:38:04.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4037628173828125
37983 2023-02-16,23:38:04.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3570467233657837
38255 2023-02-16,23:38:04.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
38362 2023-02-16,23:38:04.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38487 2023-02-16,23:38:04.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
36932 2023-02-16,23:38:04.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.39085054397583
37152 2023-02-16,23:38:04.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4199817180633545
37268 2023-02-16,23:38:04.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4079087972640991
38108 2023-02-16,23:38:04.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.3999576568603516
37862 2023-02-16,23:38:04.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.443584680557251
37983 2023-02-16,23:38:04.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.367922067642212
38255 2023-02-16,23:38:04.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38362 2023-02-16,23:38:04.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
38487 2023-02-16,23:38:04.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
36814 2023-02-16,23:38:04.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.376328468322754
36932 2023-02-16,23:38:04.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3861724138259888
37035 2023-02-16,23:38:04.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.439450740814209
37152 2023-02-16,23:38:04.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3786036968231201
37268 2023-02-16,23:38:04.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.388867735862732
37393 2023-02-16,23:38:04.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3828213214874268
37533 2023-02-16,23:38:04.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4246021509170532
37757 2023-02-16,23:38:04.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3982698917388916
38108 2023-02-16,23:38:04.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4019354581832886
37983 2023-02-16,23:38:05.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3613765239715576
38487 2023-02-16,23:38:05.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
36814 2023-02-16,23:38:05.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4000084400177002
37035 2023-02-16,23:38:05.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.380040168762207
37152 2023-02-16,23:38:05.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.365748643875122
37268 2023-02-16,23:38:05.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3817490339279175
37393 2023-02-16,23:38:05.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4068148136138916
37533 2023-02-16,23:38:05.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.37058687210083
37757 2023-02-16,23:38:05.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4532681703567505
37862 2023-02-16,23:38:05.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3770015239715576
38255 2023-02-16,23:38:05.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
38362 2023-02-16,23:38:05.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
36932 2023-02-16,23:38:05.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3214932680130005
38108 2023-02-16,23:38:05.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3845398426055908
37035 2023-02-16,23:38:05.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3935058116912842
37533 2023-02-16,23:38:05.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.39170503616333
37862 2023-02-16,23:38:05.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.407511591911316
37983 2023-02-16,23:38:05.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.420885443687439
38255 2023-02-16,23:38:05.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
38362 2023-02-16,23:38:05.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
38487 2023-02-16,23:38:05.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
36814 2023-02-16,23:38:05.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4333492517471313
36932 2023-02-16,23:38:05.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4083374738693237
37152 2023-02-16,23:38:05.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3875874280929565
37268 2023-02-16,23:38:05.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.349346399307251
37393 2023-02-16,23:38:05.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3949761390686035
37757 2023-02-16,23:38:05.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4390738010406494
38108 2023-02-16,23:38:05.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3784745931625366
38487 2023-02-16,23:38:05.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
37152 2023-02-16,23:38:05.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3603558540344238
37268 2023-02-16,23:38:05.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3809128999710083
37393 2023-02-16,23:38:05.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4248956441879272
37533 2023-02-16,23:38:05.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3871662616729736
37757 2023-02-16,23:38:05.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4019339084625244
37862 2023-02-16,23:38:05.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.366457462310791
37983 2023-02-16,23:38:05.828 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4326869249343872
38108 2023-02-16,23:38:05.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.441746473312378
38255 2023-02-16,23:38:05.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
38362 2023-02-16,23:38:05.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
36814 2023-02-16,23:38:05.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4200884103775024
36932 2023-02-16,23:38:05.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3769155740737915
37035 2023-02-16,23:38:05.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3744943141937256
37862 2023-02-16,23:38:06.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3709198236465454
38255 2023-02-16,23:38:06.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
38362 2023-02-16,23:38:06.109 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38487 2023-02-16,23:38:06.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
36814 2023-02-16,23:38:06.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4014984369277954
36932 2023-02-16,23:38:06.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4112204313278198
37035 2023-02-16,23:38:06.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3723831176757812
37152 2023-02-16,23:38:06.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3718540668487549
37268 2023-02-16,23:38:06.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4017434120178223
37393 2023-02-16,23:38:06.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.3998671770095825
37533 2023-02-16,23:38:06.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3506557941436768
37757 2023-02-16,23:38:06.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3928298950195312
37983 2023-02-16,23:38:06.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3972141742706299
38108 2023-02-16,23:38:06.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.356771469116211
38487 2023-02-16,23:38:06.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37035 2023-02-16,23:38:06.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3877969980239868
37152 2023-02-16,23:38:06.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3535492420196533
37393 2023-02-16,23:38:06.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3966271877288818
37533 2023-02-16,23:38:06.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.411616563796997
37757 2023-02-16,23:38:06.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4077036380767822
37862 2023-02-16,23:38:06.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3987003564834595
37983 2023-02-16,23:38:06.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3705224990844727
38108 2023-02-16,23:38:06.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4244897365570068
38255 2023-02-16,23:38:06.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
38362 2023-02-16,23:38:06.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
36814 2023-02-16,23:38:06.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4456735849380493
36932 2023-02-16,23:38:06.420 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3839457035064697
37268 2023-02-16,23:38:06.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3617579936981201
37035 2023-02-16,23:38:06.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3894155025482178
37862 2023-02-16,23:38:06.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3798846006393433
38362 2023-02-16,23:38:06.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
38487 2023-02-16,23:38:06.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
36814 2023-02-16,23:38:06.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4026983976364136
37152 2023-02-16,23:38:06.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372084140777588
37268 2023-02-16,23:38:06.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.3955233097076416
37393 2023-02-16,23:38:06.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4240063428878784
37533 2023-02-16,23:38:06.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4389365911483765
37757 2023-02-16,23:38:06.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4130841493606567
37983 2023-02-16,23:38:06.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.428670048713684
38108 2023-02-16,23:38:06.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3931113481521606
38255 2023-02-16,23:38:06.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
36932 2023-02-16,23:38:06.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3607118129730225
36814 2023-02-16,23:38:06.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.366788625717163
37268 2023-02-16,23:38:06.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3781203031539917
37757 2023-02-16,23:38:06.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3757883310317993
37862 2023-02-16,23:38:06.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3651738166809082
37983 2023-02-16,23:38:06.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4252865314483643
38255 2023-02-16,23:38:06.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
38108 2023-02-16,23:38:06.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4014852046966553
38362 2023-02-16,23:38:06.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
38487 2023-02-16,23:38:07.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37035 2023-02-16,23:38:07.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3818821907043457
37152 2023-02-16,23:38:07.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3612713813781738
37393 2023-02-16,23:38:07.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3870103359222412
37533 2023-02-16,23:38:07.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3881773948669434
36932 2023-02-16,23:38:07.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4258873462677002
38255 2023-02-16,23:38:07.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
38108 2023-02-16,23:38:07.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4183006286621094
38487 2023-02-16,23:38:07.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
36814 2023-02-16,23:38:07.279 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3719096183776855
37152 2023-02-16,23:38:07.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4100974798202515
37268 2023-02-16,23:38:07.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.393111228942871
37393 2023-02-16,23:38:07.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3833236694335938
37533 2023-02-16,23:38:07.290 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3520400524139404
37862 2023-02-16,23:38:07.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3582676649093628
37983 2023-02-16,23:38:07.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3970141410827637
38362 2023-02-16,23:38:07.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
36932 2023-02-16,23:38:07.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4213736057281494
37035 2023-02-16,23:38:07.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3614815473556519
37757 2023-02-16,23:38:07.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890578746795654
38108 2023-02-16,23:38:07.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3967031240463257
38255 2023-02-16,23:38:07.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
38487 2023-02-16,23:38:07.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
36814 2023-02-16,23:38:07.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.415926456451416
36932 2023-02-16,23:38:07.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4090806245803833
37035 2023-02-16,23:38:07.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3676389455795288
37152 2023-02-16,23:38:07.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4218381643295288
37268 2023-02-16,23:38:07.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4185502529144287
37393 2023-02-16,23:38:07.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4230444431304932
37533 2023-02-16,23:38:07.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4121872186660767
37757 2023-02-16,23:38:07.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3738999366760254
37862 2023-02-16,23:38:07.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3541364669799805
37983 2023-02-16,23:38:07.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4128329753875732
38362 2023-02-16,23:38:07.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
38108 2023-02-16,23:38:07.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372609615325928
36814 2023-02-16,23:38:07.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3861736059188843
37035 2023-02-16,23:38:07.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3485521078109741
37152 2023-02-16,23:38:07.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4401319026947021
37268 2023-02-16,23:38:07.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4010761976242065
37393 2023-02-16,23:38:07.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4111586809158325
37533 2023-02-16,23:38:07.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3987163305282593
37757 2023-02-16,23:38:07.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3947503566741943
37862 2023-02-16,23:38:07.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4306561946868896
37983 2023-02-16,23:38:07.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3716776371002197
38255 2023-02-16,23:38:07.883 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
38487 2023-02-16,23:38:07.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
36932 2023-02-16,23:38:07.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4520237445831299
38362 2023-02-16,23:38:07.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
37533 2023-02-16,23:38:08.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4106894731521606
37757 2023-02-16,23:38:08.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.404190182685852
37862 2023-02-16,23:38:08.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4323463439941406
37983 2023-02-16,23:38:08.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3750768899917603
38108 2023-02-16,23:38:08.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3640556335449219
38255 2023-02-16,23:38:08.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
38487 2023-02-16,23:38:08.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
36814 2023-02-16,23:38:08.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4195090532302856
36932 2023-02-16,23:38:08.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.403357982635498
37035 2023-02-16,23:38:08.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4118891954421997
37152 2023-02-16,23:38:08.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3693039417266846
37268 2023-02-16,23:38:08.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3908629417419434
37393 2023-02-16,23:38:08.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4080121517181396
38362 2023-02-16,23:38:08.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37533 2023-02-16,23:38:08.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4111559391021729
37757 2023-02-16,23:38:08.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.372127652168274
38108 2023-02-16,23:38:08.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3467367887496948
38255 2023-02-16,23:38:08.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
38487 2023-02-16,23:38:08.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
36814 2023-02-16,23:38:08.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4082826375961304
37035 2023-02-16,23:38:08.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3799221515655518
37152 2023-02-16,23:38:08.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3956350088119507
37268 2023-02-16,23:38:08.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.397973895072937
37983 2023-02-16,23:38:08.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3916137218475342
38362 2023-02-16,23:38:08.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
36932 2023-02-16,23:38:08.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3914235830307007
37862 2023-02-16,23:38:08.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3834084272384644
37393 2023-02-16,23:38:08.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4026340246200562
37533 2023-02-16,23:38:08.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3446577787399292
37757 2023-02-16,23:38:08.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3959343433380127
38108 2023-02-16,23:38:08.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.351211667060852
38255 2023-02-16,23:38:08.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
38362 2023-02-16,23:38:08.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
38487 2023-02-16,23:38:08.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
36814 2023-02-16,23:38:08.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4055886268615723
36932 2023-02-16,23:38:08.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.3997464179992676
37035 2023-02-16,23:38:08.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.436023473739624
37152 2023-02-16,23:38:08.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4120562076568604
37268 2023-02-16,23:38:08.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3854163885116577
37862 2023-02-16,23:38:08.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.374237060546875
37983 2023-02-16,23:38:08.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.386082410812378
37393 2023-02-16,23:38:08.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.417773962020874
38108 2023-02-16,23:38:09.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3886033296585083
38255 2023-02-16,23:38:09.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
38362 2023-02-16,23:38:09.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
38487 2023-02-16,23:38:09.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
36814 2023-02-16,23:38:09.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3889509439468384
36932 2023-02-16,23:38:09.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4052119255065918
37035 2023-02-16,23:38:09.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4322789907455444
37152 2023-02-16,23:38:09.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4049413204193115
37268 2023-02-16,23:38:09.057 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3701738119125366
37533 2023-02-16,23:38:09.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3785371780395508
37757 2023-02-16,23:38:09.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3606061935424805
37862 2023-02-16,23:38:09.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4002381563186646
37983 2023-02-16,23:38:09.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4029585123062134
37393 2023-02-16,23:38:09.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3653225898742676
36814 2023-02-16,23:38:09.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4102929830551147
37533 2023-02-16,23:38:09.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.374010443687439
38108 2023-02-16,23:38:09.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441240787506104
38255 2023-02-16,23:38:09.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
38362 2023-02-16,23:38:09.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38487 2023-02-16,23:38:09.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
36932 2023-02-16,23:38:09.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4042394161224365
37035 2023-02-16,23:38:09.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.431086778640747
37152 2023-02-16,23:38:09.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4031211137771606
37268 2023-02-16,23:38:09.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4061577320098877
37862 2023-02-16,23:38:09.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4031111001968384
37983 2023-02-16,23:38:09.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.407892107963562
37393 2023-02-16,23:38:09.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4087077379226685
37757 2023-02-16,23:38:09.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3873710632324219
38108 2023-02-16,23:38:09.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.368263840675354
36814 2023-02-16,23:38:09.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3865596055984497
37533 2023-02-16,23:38:09.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3463174104690552
37862 2023-02-16,23:38:09.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3818042278289795
37983 2023-02-16,23:38:09.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3977878093719482
38255 2023-02-16,23:38:09.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
38362 2023-02-16,23:38:09.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
38487 2023-02-16,23:38:09.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
36932 2023-02-16,23:38:09.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.3996926546096802
37035 2023-02-16,23:38:09.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3660470247268677
37152 2023-02-16,23:38:09.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.395073413848877
37268 2023-02-16,23:38:09.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3929473161697388
37393 2023-02-16,23:38:09.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4083633422851562
37757 2023-02-16,23:38:09.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4183686971664429
37533 2023-02-16,23:38:09.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3994694948196411
37862 2023-02-16,23:38:09.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3931519985198975
37983 2023-02-16,23:38:09.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.428017258644104
38108 2023-02-16,23:38:09.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4137537479400635
38362 2023-02-16,23:38:09.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
38487 2023-02-16,23:38:09.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
36814 2023-02-16,23:38:09.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.33194899559021
36932 2023-02-16,23:38:09.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3971173763275146
37035 2023-02-16,23:38:09.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3761844635009766
37152 2023-02-16,23:38:09.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4421989917755127
37268 2023-02-16,23:38:09.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3727613687515259
37393 2023-02-16,23:38:09.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4154481887817383
37757 2023-02-16,23:38:09.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4199471473693848
38255 2023-02-16,23:38:09.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37533 2023-02-16,23:38:10.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3884234428405762
37862 2023-02-16,23:38:10.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3980774879455566
37983 2023-02-16,23:38:10.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4401623010635376
38108 2023-02-16,23:38:10.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3540748357772827
38487 2023-02-16,23:38:10.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
36814 2023-02-16,23:38:10.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4096273183822632
36932 2023-02-16,23:38:10.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4328762292861938
37035 2023-02-16,23:38:10.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4186978340148926
37152 2023-02-16,23:38:10.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3968636989593506
37268 2023-02-16,23:38:10.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3545925617218018
37393 2023-02-16,23:38:10.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3917800188064575
37757 2023-02-16,23:38:10.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3539525270462036
38362 2023-02-16,23:38:10.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
38255 2023-02-16,23:38:10.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3952723741531372
37862 2023-02-16,23:38:10.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4342997074127197
38108 2023-02-16,23:38:10.567 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.359212875366211
38487 2023-02-16,23:38:10.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
36814 2023-02-16,23:38:10.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3841544389724731
37035 2023-02-16,23:38:10.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3458582162857056
37152 2023-02-16,23:38:10.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.350564956665039
37268 2023-02-16,23:38:10.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.38336980342865
37393 2023-02-16,23:38:10.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3320624828338623
37533 2023-02-16,23:38:10.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3496662378311157
37757 2023-02-16,23:38:10.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4249523878097534
37983 2023-02-16,23:38:10.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4192718267440796
38362 2023-02-16,23:38:10.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
38255 2023-02-16,23:38:10.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4116195440292358
36932 2023-02-16,23:38:10.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4394527673721313
37862 2023-02-16,23:38:10.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.401641607284546
38108 2023-02-16,23:38:10.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848687410354614
38362 2023-02-16,23:38:10.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
38487 2023-02-16,23:38:10.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37035 2023-02-16,23:38:10.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3319380283355713
37152 2023-02-16,23:38:10.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3487977981567383
37268 2023-02-16,23:38:10.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4376074075698853
37393 2023-02-16,23:38:10.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3895738124847412
37533 2023-02-16,23:38:10.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.359492301940918
37757 2023-02-16,23:38:10.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3896186351776123
37983 2023-02-16,23:38:10.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4270614385604858
38255 2023-02-16,23:38:10.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4231711626052856
36814 2023-02-16,23:38:10.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3774733543395996
36932 2023-02-16,23:38:10.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4071153402328491
37862 2023-02-16,23:38:11.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3857977390289307
38108 2023-02-16,23:38:11.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3571324348449707
38255 2023-02-16,23:38:11.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3743627071380615
38487 2023-02-16,23:38:11.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
36932 2023-02-16,23:38:11.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4333019256591797
37152 2023-02-16,23:38:11.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.419589638710022
37268 2023-02-16,23:38:11.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3711497783660889
37393 2023-02-16,23:38:11.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3895623683929443
37533 2023-02-16,23:38:11.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3681375980377197
37757 2023-02-16,23:38:11.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4011483192443848
37983 2023-02-16,23:38:11.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3945575952529907
38362 2023-02-16,23:38:11.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
36814 2023-02-16,23:38:11.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.41426682472229
37035 2023-02-16,23:38:11.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4151551723480225
37862 2023-02-16,23:38:11.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4459598064422607
38108 2023-02-16,23:38:11.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3679147958755493
38255 2023-02-16,23:38:11.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3703393936157227
38487 2023-02-16,23:38:11.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
36932 2023-02-16,23:38:11.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4372355937957764
37152 2023-02-16,23:38:11.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4319634437561035
37268 2023-02-16,23:38:11.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3927257061004639
37393 2023-02-16,23:38:11.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.425003170967102
37533 2023-02-16,23:38:11.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4001339673995972
37757 2023-02-16,23:38:11.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4069358110427856
37983 2023-02-16,23:38:11.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.403903603553772
38362 2023-02-16,23:38:11.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
36814 2023-02-16,23:38:11.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3675098419189453
37035 2023-02-16,23:38:11.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.39910089969635
37757 2023-02-16,23:38:11.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.432995319366455
38108 2023-02-16,23:38:11.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3613895177841187
38255 2023-02-16,23:38:11.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.4000990390777588
36932 2023-02-16,23:38:11.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3726794719696045
37152 2023-02-16,23:38:11.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3877966403961182
37268 2023-02-16,23:38:11.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3243474960327148
37393 2023-02-16,23:38:11.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4023199081420898
37533 2023-02-16,23:38:11.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3865052461624146
37862 2023-02-16,23:38:11.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4121382236480713
37983 2023-02-16,23:38:11.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.443735122680664
38362 2023-02-16,23:38:11.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
38487 2023-02-16,23:38:11.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
36814 2023-02-16,23:38:11.817 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.392972707748413
37035 2023-02-16,23:38:11.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3717739582061768
37533 2023-02-16,23:38:12.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4554003477096558
36932 2023-02-16,23:38:12.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.402125597000122
37757 2023-02-16,23:38:12.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4254858493804932
38108 2023-02-16,23:38:12.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4208457469940186
38362 2023-02-16,23:38:12.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
38255 2023-02-16,23:38:12.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.401881217956543
38487 2023-02-16,23:38:12.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
36814 2023-02-16,23:38:12.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3811440467834473
37152 2023-02-16,23:38:12.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4291715621948242
37268 2023-02-16,23:38:12.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3712399005889893
37393 2023-02-16,23:38:12.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4087148904800415
37862 2023-02-16,23:38:12.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4311614036560059
37983 2023-02-16,23:38:12.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.377051591873169
37035 2023-02-16,23:38:12.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4161068201065063
37533 2023-02-16,23:38:12.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4056140184402466
36932 2023-02-16,23:38:12.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4201921224594116
37152 2023-02-16,23:38:12.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3567243814468384
37757 2023-02-16,23:38:12.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3704029321670532
37862 2023-02-16,23:38:12.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3982925415039062
37983 2023-02-16,23:38:12.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4075822830200195
38108 2023-02-16,23:38:12.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4326115846633911
38362 2023-02-16,23:38:12.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
38255 2023-02-16,23:38:12.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3843530416488647
38487 2023-02-16,23:38:12.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
36814 2023-02-16,23:38:12.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3867323398590088
37035 2023-02-16,23:38:12.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4333696365356445
37268 2023-02-16,23:38:12.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4154679775238037
37393 2023-02-16,23:38:12.420 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3897210359573364
37152 2023-02-16,23:38:12.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.383229374885559
36932 2023-02-16,23:38:12.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4176294803619385
37533 2023-02-16,23:38:12.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849390745162964
37862 2023-02-16,23:38:12.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4530352354049683
37757 2023-02-16,23:38:12.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3921561241149902
38108 2023-02-16,23:38:12.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3971775770187378
38255 2023-02-16,23:38:12.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3785347938537598
38362 2023-02-16,23:38:12.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
38487 2023-02-16,23:38:12.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
36814 2023-02-16,23:38:12.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3546254634857178
37035 2023-02-16,23:38:12.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.380420207977295
37268 2023-02-16,23:38:12.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3441309928894043
37393 2023-02-16,23:38:12.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.380629539489746
37983 2023-02-16,23:38:12.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3664261102676392
37152 2023-02-16,23:38:12.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3778027296066284
37533 2023-02-16,23:38:13.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3917553424835205
37757 2023-02-16,23:38:13.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.387026309967041
37862 2023-02-16,23:38:13.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4390537738800049
37983 2023-02-16,23:38:13.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3709893226623535
38108 2023-02-16,23:38:13.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3705189228057861
38255 2023-02-16,23:38:13.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4418137073516846
38362 2023-02-16,23:38:13.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
38487 2023-02-16,23:38:13.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
36814 2023-02-16,23:38:13.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4244818687438965
37035 2023-02-16,23:38:13.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.359220027923584
36932 2023-02-16,23:38:13.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4346399307250977
37268 2023-02-16,23:38:13.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3747293949127197
37393 2023-02-16,23:38:13.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3498058319091797
37152 2023-02-16,23:38:13.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3771899938583374
37533 2023-02-16,23:38:13.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.339189887046814
37757 2023-02-16,23:38:13.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3506747484207153
37862 2023-02-16,23:38:13.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4019365310668945
37983 2023-02-16,23:38:13.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.398790717124939
38108 2023-02-16,23:38:13.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4285573959350586
38255 2023-02-16,23:38:13.323 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3564867973327637
38362 2023-02-16,23:38:13.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
38487 2023-02-16,23:38:13.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
36814 2023-02-16,23:38:13.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4051711559295654
36932 2023-02-16,23:38:13.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4003628492355347
37035 2023-02-16,23:38:13.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.422222375869751
37268 2023-02-16,23:38:13.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3794159889221191
37393 2023-02-16,23:38:13.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3809926509857178
37152 2023-02-16,23:38:13.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3920893669128418
37533 2023-02-16,23:38:13.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3772966861724854
37757 2023-02-16,23:38:13.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.41201651096344
37862 2023-02-16,23:38:13.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3928624391555786
37983 2023-02-16,23:38:13.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3798260688781738
38108 2023-02-16,23:38:13.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4252883195877075
38255 2023-02-16,23:38:13.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4245249032974243
38362 2023-02-16,23:38:13.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
38487 2023-02-16,23:38:13.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
36814 2023-02-16,23:38:13.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3646697998046875
36932 2023-02-16,23:38:13.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4200811386108398
37035 2023-02-16,23:38:13.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3905587196350098
37268 2023-02-16,23:38:13.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.376285433769226
37393 2023-02-16,23:38:13.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4010334014892578
37152 2023-02-16,23:38:13.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.391837477684021
37533 2023-02-16,23:38:13.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4095978736877441
37757 2023-02-16,23:38:13.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4397221803665161
37862 2023-02-16,23:38:13.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4076635837554932
37983 2023-02-16,23:38:13.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3651033639907837
38108 2023-02-16,23:38:13.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3970599174499512
38255 2023-02-16,23:38:13.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.393129587173462
38362 2023-02-16,23:38:13.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
38487 2023-02-16,23:38:13.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
36814 2023-02-16,23:38:13.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4182734489440918
36932 2023-02-16,23:38:13.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.393808364868164
37035 2023-02-16,23:38:13.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3865400552749634
37268 2023-02-16,23:38:13.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4125232696533203
37393 2023-02-16,23:38:13.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3620166778564453
37152 2023-02-16,23:38:14.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3658801317214966
37533 2023-02-16,23:38:14.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3808103799819946
37757 2023-02-16,23:38:14.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3884503841400146
37862 2023-02-16,23:38:14.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.413038969039917
37983 2023-02-16,23:38:14.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3582113981246948
38255 2023-02-16,23:38:14.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4015377759933472
37035 2023-02-16,23:38:14.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3198071718215942
37268 2023-02-16,23:38:14.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3798109292984009
38108 2023-02-16,23:38:14.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.412837028503418
38362 2023-02-16,23:38:14.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
38487 2023-02-16,23:38:14.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
36814 2023-02-16,23:38:14.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3889211416244507
36932 2023-02-16,23:38:14.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.454307198524475
37393 2023-02-16,23:38:14.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.395689845085144
37152 2023-02-16,23:38:14.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.409903883934021
37533 2023-02-16,23:38:14.529 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3834093809127808
37757 2023-02-16,23:38:14.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3519623279571533
37862 2023-02-16,23:38:14.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.375709891319275
37983 2023-02-16,23:38:14.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3540089130401611
38108 2023-02-16,23:38:14.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3717076778411865
38255 2023-02-16,23:38:14.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4182610511779785
38487 2023-02-16,23:38:14.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
36814 2023-02-16,23:38:14.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.476920247077942
36932 2023-02-16,23:38:14.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3539299964904785
37035 2023-02-16,23:38:14.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.408444881439209
37268 2023-02-16,23:38:14.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4472157955169678
37393 2023-02-16,23:38:14.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3783361911773682
38362 2023-02-16,23:38:14.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
37862 2023-02-16,23:38:14.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890362977981567
37983 2023-02-16,23:38:14.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4308313131332397
37152 2023-02-16,23:38:14.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3859577178955078
37533 2023-02-16,23:38:14.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3762471675872803
38108 2023-02-16,23:38:14.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3750669956207275
38255 2023-02-16,23:38:14.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3966922760009766
38487 2023-02-16,23:38:14.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
36814 2023-02-16,23:38:14.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.368656039237976
36932 2023-02-16,23:38:14.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4145983457565308
37035 2023-02-16,23:38:14.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3752992153167725
37268 2023-02-16,23:38:14.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4449812173843384
37393 2023-02-16,23:38:14.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3936926126480103
37757 2023-02-16,23:38:14.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.412637710571289
38362 2023-02-16,23:38:14.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3951568603515625
37152 2023-02-16,23:38:15.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3953453302383423
37393 2023-02-16,23:38:15.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4176838397979736
37862 2023-02-16,23:38:15.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3739192485809326
37983 2023-02-16,23:38:15.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.43244469165802
36932 2023-02-16,23:38:15.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3775641918182373
37268 2023-02-16,23:38:15.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3437341451644897
37533 2023-02-16,23:38:15.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3829097747802734
37757 2023-02-16,23:38:15.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.399112343788147
38108 2023-02-16,23:38:15.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.391675591468811
38255 2023-02-16,23:38:15.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3371946811676025
38362 2023-02-16,23:38:15.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.411604642868042
38487 2023-02-16,23:38:15.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
36814 2023-02-16,23:38:15.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3780051469802856
37035 2023-02-16,23:38:15.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4118032455444336
36932 2023-02-16,23:38:15.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3167986869812012
37862 2023-02-16,23:38:15.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3946971893310547
37983 2023-02-16,23:38:15.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3833708763122559
38108 2023-02-16,23:38:15.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3860379457473755
38255 2023-02-16,23:38:15.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.364164113998413
38362 2023-02-16,23:38:15.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.423201322555542
38487 2023-02-16,23:38:15.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
36814 2023-02-16,23:38:15.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.372114658355713
37035 2023-02-16,23:38:15.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3827039003372192
37152 2023-02-16,23:38:15.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.3991341590881348
37268 2023-02-16,23:38:15.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4056286811828613
37393 2023-02-16,23:38:15.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4006540775299072
37533 2023-02-16,23:38:15.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4286972284317017
37757 2023-02-16,23:38:15.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4110437631607056
37862 2023-02-16,23:38:15.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4041638374328613
38108 2023-02-16,23:38:15.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4029701948165894
38362 2023-02-16,23:38:15.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3743383884429932
38487 2023-02-16,23:38:15.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
36814 2023-02-16,23:38:15.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4477211236953735
36932 2023-02-16,23:38:15.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3645001649856567
37035 2023-02-16,23:38:15.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3600093126296997
37152 2023-02-16,23:38:15.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.429261326789856
37268 2023-02-16,23:38:15.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4698973894119263
37393 2023-02-16,23:38:15.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3913605213165283
37533 2023-02-16,23:38:15.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.380078911781311
37757 2023-02-16,23:38:15.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4114433526992798
37983 2023-02-16,23:38:15.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3742566108703613
38255 2023-02-16,23:38:15.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.3465877771377563
37862 2023-02-16,23:38:16.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3720678091049194
38108 2023-02-16,23:38:16.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4078832864761353
38362 2023-02-16,23:38:16.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3703172206878662
38487 2023-02-16,23:38:16.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
36814 2023-02-16,23:38:16.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3737126588821411
36932 2023-02-16,23:38:16.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4359254837036133
37035 2023-02-16,23:38:16.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4265241622924805
37152 2023-02-16,23:38:16.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3786031007766724
37268 2023-02-16,23:38:16.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3867824077606201
37393 2023-02-16,23:38:16.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3976612091064453
37533 2023-02-16,23:38:16.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3371119499206543
37757 2023-02-16,23:38:16.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.344351053237915
37983 2023-02-16,23:38:16.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4002522230148315
38255 2023-02-16,23:38:16.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.351117491722107
38108 2023-02-16,23:38:16.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3977495431900024
38362 2023-02-16,23:38:16.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.4000349044799805
38487 2023-02-16,23:38:16.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
36814 2023-02-16,23:38:16.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3808176517486572
36932 2023-02-16,23:38:16.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3945043087005615
37035 2023-02-16,23:38:16.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4222973585128784
37152 2023-02-16,23:38:16.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4242461919784546
37268 2023-02-16,23:38:16.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3802553415298462
37393 2023-02-16,23:38:16.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3858460187911987
37533 2023-02-16,23:38:16.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.381670594215393
37757 2023-02-16,23:38:16.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3786120414733887
37862 2023-02-16,23:38:16.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3958967924118042
37983 2023-02-16,23:38:16.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.403172254562378
38255 2023-02-16,23:38:16.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3886300325393677
38108 2023-02-16,23:38:16.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4280067682266235
37862 2023-02-16,23:38:16.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3606725931167603
38362 2023-02-16,23:38:16.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.401932954788208
36814 2023-02-16,23:38:16.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4031238555908203
36932 2023-02-16,23:38:16.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3417315483093262
37533 2023-02-16,23:38:16.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4187464714050293
37393 2023-02-16,23:38:16.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3705743551254272
37757 2023-02-16,23:38:16.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3738373517990112
37983 2023-02-16,23:38:16.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3818268775939941
38255 2023-02-16,23:38:16.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441089391708374
38487 2023-02-16,23:38:16.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
37035 2023-02-16,23:38:16.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4102333784103394
37152 2023-02-16,23:38:16.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3760095834732056
37268 2023-02-16,23:38:16.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4004042148590088
38108 2023-02-16,23:38:16.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4401240348815918
36932 2023-02-16,23:38:16.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3916289806365967
37152 2023-02-16,23:38:16.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3968507051467896
37393 2023-02-16,23:38:16.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4057201147079468
37533 2023-02-16,23:38:16.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3975836038589478
37757 2023-02-16,23:38:16.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3458633422851562
37862 2023-02-16,23:38:16.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.387452483177185
37983 2023-02-16,23:38:16.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.393236756324768
38255 2023-02-16,23:38:16.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3681014776229858
38362 2023-02-16,23:38:16.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.3844660520553589
38487 2023-02-16,23:38:16.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
36814 2023-02-16,23:38:16.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3677771091461182
37035 2023-02-16,23:38:16.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4517953395843506
37268 2023-02-16,23:38:16.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3734883069992065
38108 2023-02-16,23:38:17.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4191750288009644
36932 2023-02-16,23:38:17.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3861055374145508
37152 2023-02-16,23:38:17.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3571240901947021
37393 2023-02-16,23:38:17.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3932502269744873
37533 2023-02-16,23:38:17.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3640676736831665
37757 2023-02-16,23:38:17.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.399936318397522
37862 2023-02-16,23:38:17.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.418285846710205
37983 2023-02-16,23:38:17.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3980445861816406
38255 2023-02-16,23:38:17.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4137638807296753
38362 2023-02-16,23:38:17.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3785136938095093
38487 2023-02-16,23:38:17.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
36814 2023-02-16,23:38:17.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.316859245300293
37035 2023-02-16,23:38:17.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.403735876083374
37268 2023-02-16,23:38:17.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.392293930053711
38108 2023-02-16,23:38:17.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4269239902496338
36932 2023-02-16,23:38:17.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3820852041244507
37533 2023-02-16,23:38:17.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3736883401870728
37757 2023-02-16,23:38:17.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.388728141784668
37862 2023-02-16,23:38:17.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4200087785720825
37983 2023-02-16,23:38:17.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4344751834869385
38255 2023-02-16,23:38:17.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.354180932044983
38362 2023-02-16,23:38:17.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.441800832748413
38487 2023-02-16,23:38:17.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
36814 2023-02-16,23:38:17.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3972834348678589
37035 2023-02-16,23:38:17.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3911705017089844
37152 2023-02-16,23:38:17.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4253709316253662
37268 2023-02-16,23:38:17.567 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4173791408538818
37393 2023-02-16,23:38:17.567 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3733726739883423
38108 2023-02-16,23:38:17.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3945558071136475
37533 2023-02-16,23:38:17.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3974790573120117
37757 2023-02-16,23:38:17.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3493750095367432
37862 2023-02-16,23:38:17.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3539237976074219
37983 2023-02-16,23:38:17.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4016886949539185
38362 2023-02-16,23:38:17.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3566598892211914
38255 2023-02-16,23:38:17.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3591612577438354
38487 2023-02-16,23:38:17.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
36814 2023-02-16,23:38:17.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.379564881324768
36932 2023-02-16,23:38:17.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4122931957244873
37035 2023-02-16,23:38:17.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4016190767288208
37152 2023-02-16,23:38:17.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.38435959815979
37268 2023-02-16,23:38:17.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4145567417144775
37393 2023-02-16,23:38:17.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3542060852050781
37533 2023-02-16,23:38:18.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.425055742263794
37757 2023-02-16,23:38:18.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3597499132156372
37862 2023-02-16,23:38:18.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4249279499053955
37983 2023-02-16,23:38:18.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.385880708694458
38108 2023-02-16,23:38:18.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4038934707641602
38255 2023-02-16,23:38:18.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848090171813965
38362 2023-02-16,23:38:18.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.4245973825454712
38487 2023-02-16,23:38:18.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
36814 2023-02-16,23:38:18.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4384801387786865
36932 2023-02-16,23:38:18.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4106849431991577
37035 2023-02-16,23:38:18.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.405788779258728
37152 2023-02-16,23:38:18.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3955897092819214
37268 2023-02-16,23:38:18.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3798977136611938
37393 2023-02-16,23:38:18.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3839826583862305
37533 2023-02-16,23:38:18.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4121131896972656
37757 2023-02-16,23:38:18.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3680815696716309
37862 2023-02-16,23:38:18.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.389587640762329
37983 2023-02-16,23:38:18.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4461514949798584
38108 2023-02-16,23:38:18.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4436976909637451
38255 2023-02-16,23:38:18.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3570575714111328
38362 2023-02-16,23:38:18.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3931676149368286
38487 2023-02-16,23:38:18.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
36814 2023-02-16,23:38:18.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3141793012619019
36932 2023-02-16,23:38:18.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4091105461120605
37035 2023-02-16,23:38:18.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4029520750045776
37152 2023-02-16,23:38:18.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3953373432159424
37268 2023-02-16,23:38:18.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4073728322982788
37393 2023-02-16,23:38:18.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4390273094177246
37533 2023-02-16,23:38:18.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4322052001953125
37757 2023-02-16,23:38:18.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4007734060287476
37862 2023-02-16,23:38:18.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4010021686553955
37983 2023-02-16,23:38:18.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4122706651687622
38108 2023-02-16,23:38:18.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3770405054092407
38255 2023-02-16,23:38:18.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3678914308547974
38362 2023-02-16,23:38:18.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.401531457901001
38487 2023-02-16,23:38:18.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
36814 2023-02-16,23:38:18.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.336593747138977
36932 2023-02-16,23:38:18.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4192912578582764
37035 2023-02-16,23:38:18.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4002388715744019
37152 2023-02-16,23:38:18.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.385668396949768
37268 2023-02-16,23:38:18.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.462737798690796
37393 2023-02-16,23:38:18.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.371409296989441
37533 2023-02-16,23:38:19.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.358860969543457
37757 2023-02-16,23:38:19.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3868918418884277
37862 2023-02-16,23:38:19.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4070383310317993
38108 2023-02-16,23:38:19.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.407557487487793
38255 2023-02-16,23:38:19.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3613343238830566
38362 2023-02-16,23:38:19.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4183076620101929
38487 2023-02-16,23:38:19.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 186/15000, loss = 1.3885318040847778
36814 2023-02-16,23:38:19.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3689203262329102
36932 2023-02-16,23:38:19.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4202462434768677
37035 2023-02-16,23:38:19.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.39714777469635
37152 2023-02-16,23:38:19.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4073259830474854
37268 2023-02-16,23:38:19.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4368129968643188
37393 2023-02-16,23:38:19.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3935643434524536
37983 2023-02-16,23:38:19.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.431260347366333
37533 2023-02-16,23:38:19.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.322434663772583
37757 2023-02-16,23:38:19.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4562127590179443
37862 2023-02-16,23:38:19.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4330568313598633
38108 2023-02-16,23:38:19.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3664836883544922
38255 2023-02-16,23:38:19.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.42081618309021
38362 2023-02-16,23:38:19.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.396733045578003
38487 2023-02-16,23:38:19.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 187/15000, loss = 1.3677974939346313
36814 2023-02-16,23:38:19.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3396546840667725
36932 2023-02-16,23:38:19.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.3992950916290283
37035 2023-02-16,23:38:19.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4334180355072021
37152 2023-02-16,23:38:19.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4024373292922974
37268 2023-02-16,23:38:19.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3940579891204834
37393 2023-02-16,23:38:19.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3243345022201538
37983 2023-02-16,23:38:19.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.398348331451416
38487 2023-02-16,23:38:19.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 188/15000, loss = 1.3843942880630493
37533 2023-02-16,23:38:19.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3793599605560303
37757 2023-02-16,23:38:19.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4062933921813965
37862 2023-02-16,23:38:19.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4253089427947998
38108 2023-02-16,23:38:19.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3710089921951294
38255 2023-02-16,23:38:19.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4326286315917969
38362 2023-02-16,23:38:19.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372673988342285
36814 2023-02-16,23:38:19.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4033939838409424
36932 2023-02-16,23:38:19.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4054043292999268
37152 2023-02-16,23:38:19.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4120464324951172
37268 2023-02-16,23:38:19.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3658757209777832
37393 2023-02-16,23:38:19.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3730469942092896
37983 2023-02-16,23:38:19.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4532719850540161
37035 2023-02-16,23:38:19.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.440415620803833
37533 2023-02-16,23:38:19.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3315058946609497
38487 2023-02-16,23:38:20.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 189/15000, loss = 1.4133137464523315
37152 2023-02-16,23:38:20.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4109671115875244
38108 2023-02-16,23:38:20.035 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3988583087921143
38255 2023-02-16,23:38:20.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3970794677734375
38362 2023-02-16,23:38:20.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3641327619552612
36814 2023-02-16,23:38:20.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.421184778213501
37268 2023-02-16,23:38:20.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3534454107284546
37393 2023-02-16,23:38:20.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4161686897277832
37757 2023-02-16,23:38:20.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849622011184692
37862 2023-02-16,23:38:20.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3705607652664185
37983 2023-02-16,23:38:20.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4391756057739258
36932 2023-02-16,23:38:20.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3666961193084717
37035 2023-02-16,23:38:20.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4057388305664062
37152 2023-02-16,23:38:20.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.423550009727478
37533 2023-02-16,23:38:20.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.386177659034729
37757 2023-02-16,23:38:20.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.39228093624115
38108 2023-02-16,23:38:20.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.379837989807129
38255 2023-02-16,23:38:20.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3705192804336548
38362 2023-02-16,23:38:20.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.346651554107666
38487 2023-02-16,23:38:20.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 190/15000, loss = 1.3670493364334106
36814 2023-02-16,23:38:20.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.416343092918396
36932 2023-02-16,23:38:20.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4159331321716309
37035 2023-02-16,23:38:20.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.433223843574524
37268 2023-02-16,23:38:20.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3894144296646118
37393 2023-02-16,23:38:20.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3448445796966553
37862 2023-02-16,23:38:20.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.392214059829712
37983 2023-02-16,23:38:20.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4019922018051147
37533 2023-02-16,23:38:20.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4062446355819702
37757 2023-02-16,23:38:20.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3387629985809326
37983 2023-02-16,23:38:20.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3929177522659302
38108 2023-02-16,23:38:20.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3651182651519775
38255 2023-02-16,23:38:20.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4285809993743896
38362 2023-02-16,23:38:20.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.351271390914917
38487 2023-02-16,23:38:20.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 191/15000, loss = 1.3320152759552002
36814 2023-02-16,23:38:20.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3894459009170532
36932 2023-02-16,23:38:20.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4356120824813843
37035 2023-02-16,23:38:20.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.43707275390625
37152 2023-02-16,23:38:20.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3775683641433716
37268 2023-02-16,23:38:20.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.331058382987976
37393 2023-02-16,23:38:20.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3761099576950073
37862 2023-02-16,23:38:20.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3870410919189453
37533 2023-02-16,23:38:20.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.390005111694336
37757 2023-02-16,23:38:20.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3769789934158325
37983 2023-02-16,23:38:20.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4076919555664062
38108 2023-02-16,23:38:20.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3582274913787842
38362 2023-02-16,23:38:20.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.388628602027893
38487 2023-02-16,23:38:20.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 192/15000, loss = 1.3817389011383057
36814 2023-02-16,23:38:20.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3593512773513794
36932 2023-02-16,23:38:20.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4163928031921387
37035 2023-02-16,23:38:20.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3718624114990234
37152 2023-02-16,23:38:20.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4068549871444702
37268 2023-02-16,23:38:20.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3718472719192505
37862 2023-02-16,23:38:20.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.350595474243164
38255 2023-02-16,23:38:20.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4252185821533203
37393 2023-02-16,23:38:20.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3789539337158203
38487 2023-02-16,23:38:21.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 193/15000, loss = 1.3128269910812378
37152 2023-02-16,23:38:21.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3762600421905518
37533 2023-02-16,23:38:21.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3846912384033203
37757 2023-02-16,23:38:21.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098221063613892
37983 2023-02-16,23:38:21.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4131104946136475
38362 2023-02-16,23:38:21.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441433906555176
36814 2023-02-16,23:38:21.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4050507545471191
36932 2023-02-16,23:38:21.301 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4094016551971436
37035 2023-02-16,23:38:21.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4012503623962402
37268 2023-02-16,23:38:21.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.362692952156067
37393 2023-02-16,23:38:21.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3756312131881714
37862 2023-02-16,23:38:21.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.412001371383667
38108 2023-02-16,23:38:21.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3540867567062378
38255 2023-02-16,23:38:21.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3969613313674927
37983 2023-02-16,23:38:21.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3757306337356567
36932 2023-02-16,23:38:21.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3795596361160278
37152 2023-02-16,23:38:21.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.373306393623352
37393 2023-02-16,23:38:21.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.413027048110962
37533 2023-02-16,23:38:21.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3636773824691772
37757 2023-02-16,23:38:21.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3810330629348755
37862 2023-02-16,23:38:21.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4396381378173828
38108 2023-02-16,23:38:21.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.430796504020691
38362 2023-02-16,23:38:21.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.368226408958435
38487 2023-02-16,23:38:21.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 194/15000, loss = 1.422794222831726
36814 2023-02-16,23:38:21.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.398414134979248
37035 2023-02-16,23:38:21.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4193778038024902
37268 2023-02-16,23:38:21.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3705387115478516
38255 2023-02-16,23:38:21.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.4128507375717163
37152 2023-02-16,23:38:21.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3797123432159424
37533 2023-02-16,23:38:21.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.356076955795288
37757 2023-02-16,23:38:21.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3834792375564575
37862 2023-02-16,23:38:21.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3883976936340332
37983 2023-02-16,23:38:21.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890016078948975
38108 2023-02-16,23:38:21.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.432305097579956
38255 2023-02-16,23:38:21.919 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3717190027236938
38362 2023-02-16,23:38:21.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.413758397102356
38487 2023-02-16,23:38:21.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 195/15000, loss = 1.3997400999069214
36814 2023-02-16,23:38:21.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3499281406402588
36932 2023-02-16,23:38:21.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3891068696975708
37035 2023-02-16,23:38:21.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4160804748535156
37268 2023-02-16,23:38:21.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4453363418579102
37393 2023-02-16,23:38:21.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3812568187713623
37533 2023-02-16,23:38:22.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.364764928817749
37152 2023-02-16,23:38:22.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3256206512451172
37757 2023-02-16,23:38:22.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3758479356765747
37862 2023-02-16,23:38:22.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3520251512527466
37983 2023-02-16,23:38:22.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3738982677459717
38108 2023-02-16,23:38:22.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3833613395690918
38255 2023-02-16,23:38:22.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.375108003616333
38362 2023-02-16,23:38:22.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3541133403778076
38487 2023-02-16,23:38:22.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 196/15000, loss = 1.3768079280853271
36814 2023-02-16,23:38:22.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3728039264678955
36932 2023-02-16,23:38:22.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3797513246536255
37035 2023-02-16,23:38:22.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4341843128204346
37268 2023-02-16,23:38:22.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3974733352661133
37393 2023-02-16,23:38:22.237 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4483987092971802
37152 2023-02-16,23:38:22.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3788368701934814
37533 2023-02-16,23:38:22.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3600646257400513
37862 2023-02-16,23:38:22.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4125828742980957
37983 2023-02-16,23:38:22.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3946726322174072
38108 2023-02-16,23:38:22.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.374273419380188
38255 2023-02-16,23:38:22.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3915389776229858
38362 2023-02-16,23:38:22.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.3592126369476318
38487 2023-02-16,23:38:22.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 197/15000, loss = 1.4382110834121704
36814 2023-02-16,23:38:22.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.3710706233978271
36932 2023-02-16,23:38:22.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3696372509002686
37268 2023-02-16,23:38:22.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.418872594833374
37393 2023-02-16,23:38:22.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.444932222366333
37757 2023-02-16,23:38:22.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3825454711914062
37035 2023-02-16,23:38:22.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.3984711170196533
37533 2023-02-16,23:38:22.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3705028295516968
37862 2023-02-16,23:38:22.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3991093635559082
38255 2023-02-16,23:38:22.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3860639333724976
38362 2023-02-16,23:38:22.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.3848307132720947
38487 2023-02-16,23:38:22.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 198/15000, loss = 1.4218688011169434
36814 2023-02-16,23:38:22.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3811233043670654
36932 2023-02-16,23:38:22.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.419991374015808
37152 2023-02-16,23:38:22.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3468011617660522
37268 2023-02-16,23:38:22.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3766595125198364
37393 2023-02-16,23:38:22.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3449257612228394
37757 2023-02-16,23:38:22.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4298428297042847
37983 2023-02-16,23:38:22.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4042072296142578
38108 2023-02-16,23:38:22.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4003020524978638
37035 2023-02-16,23:38:22.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4189884662628174
37533 2023-02-16,23:38:23.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.386817216873169
38108 2023-02-16,23:38:23.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.403059720993042
38255 2023-02-16,23:38:23.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4028629064559937
38362 2023-02-16,23:38:23.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3571217060089111
36814 2023-02-16,23:38:23.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.3992412090301514
36932 2023-02-16,23:38:23.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.395479679107666
37035 2023-02-16,23:38:23.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3928958177566528
37152 2023-02-16,23:38:23.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4002925157546997
37268 2023-02-16,23:38:23.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3657152652740479
37393 2023-02-16,23:38:23.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.406378984451294
37757 2023-02-16,23:38:23.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3799147605895996
37862 2023-02-16,23:38:23.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.411173701286316
37983 2023-02-16,23:38:23.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3720066547393799
38487 2023-02-16,23:38:23.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 199/15000, loss = 1.3778547048568726
38108 2023-02-16,23:38:23.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.381799578666687
38362 2023-02-16,23:38:23.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.367919921875
36814 2023-02-16,23:38:23.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4564332962036133
36932 2023-02-16,23:38:23.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4004300832748413
37035 2023-02-16,23:38:23.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4550693035125732
37152 2023-02-16,23:38:23.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4193005561828613
37268 2023-02-16,23:38:23.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3871866464614868
37393 2023-02-16,23:38:23.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4697036743164062
37533 2023-02-16,23:38:23.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4579840898513794
37757 2023-02-16,23:38:23.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3369871377944946
37862 2023-02-16,23:38:23.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4113837480545044
37983 2023-02-16,23:38:23.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.395961046218872
38255 2023-02-16,23:38:23.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4078233242034912
38487 2023-02-16,23:38:23.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 200/15000, loss = 1.3952486515045166
38108 2023-02-16,23:38:23.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.393187403678894
38362 2023-02-16,23:38:23.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.3614131212234497
36814 2023-02-16,23:38:23.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.343299150466919
36932 2023-02-16,23:38:23.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3669966459274292
37035 2023-02-16,23:38:23.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.352640151977539
37268 2023-02-16,23:38:23.782 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.361326813697815
37393 2023-02-16,23:38:23.783 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.387120008468628
37533 2023-02-16,23:38:23.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.398436188697815
37757 2023-02-16,23:38:23.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.381872534751892
37862 2023-02-16,23:38:23.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3443313837051392
37983 2023-02-16,23:38:23.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.360573410987854
38255 2023-02-16,23:38:23.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.397810459136963
38487 2023-02-16,23:38:23.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 201/15000, loss = 1.4115831851959229
37152 2023-02-16,23:38:23.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4097659587860107
38108 2023-02-16,23:38:24.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3980205059051514
38362 2023-02-16,23:38:24.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4209017753601074
36814 2023-02-16,23:38:24.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3842849731445312
36932 2023-02-16,23:38:24.089 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3704209327697754
37035 2023-02-16,23:38:24.090 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4153786897659302
37152 2023-02-16,23:38:24.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3934931755065918
37268 2023-02-16,23:38:24.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3715022802352905
37393 2023-02-16,23:38:24.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3814663887023926
37533 2023-02-16,23:38:24.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3969123363494873
37757 2023-02-16,23:38:24.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4195815324783325
37862 2023-02-16,23:38:24.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3786853551864624
37983 2023-02-16,23:38:24.098 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.387363314628601
38255 2023-02-16,23:38:24.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.428025245666504
38487 2023-02-16,23:38:24.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 202/15000, loss = 1.4232038259506226
36814 2023-02-16,23:38:24.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3976106643676758
36932 2023-02-16,23:38:24.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.3805983066558838
38108 2023-02-16,23:38:24.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4343382120132446
38362 2023-02-16,23:38:24.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4326469898223877
37035 2023-02-16,23:38:24.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3777438402175903
37152 2023-02-16,23:38:24.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3930883407592773
37268 2023-02-16,23:38:24.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3552285432815552
37393 2023-02-16,23:38:24.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4008632898330688
37533 2023-02-16,23:38:24.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410589694976807
37757 2023-02-16,23:38:24.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3978548049926758
37862 2023-02-16,23:38:24.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3739625215530396
37983 2023-02-16,23:38:24.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4183948040008545
38255 2023-02-16,23:38:24.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4400333166122437
38487 2023-02-16,23:38:24.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 203/15000, loss = 1.3743749856948853
36814 2023-02-16,23:38:24.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4034273624420166
36932 2023-02-16,23:38:24.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.399183988571167
38362 2023-02-16,23:38:24.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3971971273422241
37035 2023-02-16,23:38:24.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3165675401687622
37152 2023-02-16,23:38:24.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3908718824386597
37268 2023-02-16,23:38:24.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3716295957565308
37393 2023-02-16,23:38:24.712 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3738703727722168
37533 2023-02-16,23:38:24.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4160311222076416
37757 2023-02-16,23:38:24.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3643532991409302
37862 2023-02-16,23:38:24.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3458467721939087
37983 2023-02-16,23:38:24.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4200870990753174
38108 2023-02-16,23:38:24.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.401697039604187
38255 2023-02-16,23:38:24.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4191608428955078
38487 2023-02-16,23:38:24.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 204/15000, loss = 1.3703465461730957
38362 2023-02-16,23:38:25.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.370522379875183
36814 2023-02-16,23:38:25.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.3814961910247803
36932 2023-02-16,23:38:25.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.458815097808838
37393 2023-02-16,23:38:25.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3930346965789795
37533 2023-02-16,23:38:25.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3836359977722168
37757 2023-02-16,23:38:25.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3735431432724
37862 2023-02-16,23:38:25.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3998996019363403
37983 2023-02-16,23:38:25.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3538706302642822
38108 2023-02-16,23:38:25.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3858109712600708
38255 2023-02-16,23:38:25.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.4270401000976562
38487 2023-02-16,23:38:25.026 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 205/15000, loss = 1.400054931640625
37035 2023-02-16,23:38:25.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.365983486175537
37152 2023-02-16,23:38:25.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4426310062408447
37268 2023-02-16,23:38:25.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3609555959701538
38362 2023-02-16,23:38:25.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.4286178350448608
36814 2023-02-16,23:38:25.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3648405075073242
36932 2023-02-16,23:38:25.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.406444787979126
37393 2023-02-16,23:38:25.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4174916744232178
37757 2023-02-16,23:38:25.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3977203369140625
37862 2023-02-16,23:38:25.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3888589143753052
37983 2023-02-16,23:38:25.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4249892234802246
38108 2023-02-16,23:38:25.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4460560083389282
38255 2023-02-16,23:38:25.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3945835828781128
38487 2023-02-16,23:38:25.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 206/15000, loss = 1.4019691944122314
37035 2023-02-16,23:38:25.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4344990253448486
37152 2023-02-16,23:38:25.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3762003183364868
37268 2023-02-16,23:38:25.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4105827808380127
37533 2023-02-16,23:38:25.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.411947250366211
37862 2023-02-16,23:38:25.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3493783473968506
37983 2023-02-16,23:38:25.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3896814584732056
38108 2023-02-16,23:38:25.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4122915267944336
38362 2023-02-16,23:38:25.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.425299882888794
38487 2023-02-16,23:38:25.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 207/15000, loss = 1.384529948234558
36814 2023-02-16,23:38:25.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.422925353050232
36932 2023-02-16,23:38:25.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4089077711105347
37035 2023-02-16,23:38:25.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3945578336715698
37152 2023-02-16,23:38:25.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3530737161636353
37268 2023-02-16,23:38:25.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4198074340820312
37393 2023-02-16,23:38:25.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4151268005371094
37533 2023-02-16,23:38:25.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3547260761260986
37757 2023-02-16,23:38:25.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4259790182113647
38255 2023-02-16,23:38:25.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.403868556022644
37862 2023-02-16,23:38:25.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3596816062927246
37983 2023-02-16,23:38:25.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4011139869689941
38108 2023-02-16,23:38:25.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4312069416046143
38362 2023-02-16,23:38:25.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3970603942871094
38487 2023-02-16,23:38:25.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 208/15000, loss = 1.3785301446914673
36814 2023-02-16,23:38:25.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4159059524536133
36932 2023-02-16,23:38:25.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3719907999038696
37035 2023-02-16,23:38:25.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.340248465538025
37152 2023-02-16,23:38:25.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4010167121887207
37268 2023-02-16,23:38:25.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4425442218780518
37393 2023-02-16,23:38:25.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3796658515930176
37533 2023-02-16,23:38:25.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3793867826461792
37757 2023-02-16,23:38:25.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4129102230072021
38255 2023-02-16,23:38:25.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.443585753440857
37862 2023-02-16,23:38:26.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3681201934814453
37983 2023-02-16,23:38:26.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4069743156433105
38108 2023-02-16,23:38:26.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.398403525352478
38487 2023-02-16,23:38:26.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 209/15000, loss = 1.4417493343353271
36814 2023-02-16,23:38:26.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.40770423412323
36932 2023-02-16,23:38:26.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3712373971939087
37035 2023-02-16,23:38:26.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3906874656677246
37152 2023-02-16,23:38:26.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3597456216812134
37268 2023-02-16,23:38:26.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3684816360473633
37393 2023-02-16,23:38:26.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4072860479354858
37533 2023-02-16,23:38:26.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3481884002685547
37757 2023-02-16,23:38:26.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4330295324325562
38255 2023-02-16,23:38:26.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.3770784139633179
38362 2023-02-16,23:38:26.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.41287100315094
36814 2023-02-16,23:38:26.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3721340894699097
36932 2023-02-16,23:38:26.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4058260917663574
37862 2023-02-16,23:38:26.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4007842540740967
38108 2023-02-16,23:38:26.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.453221082687378
38487 2023-02-16,23:38:26.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 210/15000, loss = 1.3566938638687134
37035 2023-02-16,23:38:26.570 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3861827850341797
37152 2023-02-16,23:38:26.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3769400119781494
37268 2023-02-16,23:38:26.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3967102766036987
37393 2023-02-16,23:38:26.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4634265899658203
37533 2023-02-16,23:38:26.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4008580446243286
37757 2023-02-16,23:38:26.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.358521580696106
37983 2023-02-16,23:38:26.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4330939054489136
38255 2023-02-16,23:38:26.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4076341390609741
38362 2023-02-16,23:38:26.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.371717929840088
37862 2023-02-16,23:38:26.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3868366479873657
36814 2023-02-16,23:38:26.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3284258842468262
36932 2023-02-16,23:38:26.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3970710039138794
37035 2023-02-16,23:38:26.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3825198411941528
37152 2023-02-16,23:38:26.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4408669471740723
37268 2023-02-16,23:38:26.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4129408597946167
37393 2023-02-16,23:38:26.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4369698762893677
37533 2023-02-16,23:38:26.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3741031885147095
37757 2023-02-16,23:38:26.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3216902017593384
37983 2023-02-16,23:38:26.883 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4254865646362305
38108 2023-02-16,23:38:26.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4391624927520752
38255 2023-02-16,23:38:26.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3664478063583374
38362 2023-02-16,23:38:26.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.375124454498291
38487 2023-02-16,23:38:26.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 211/15000, loss = 1.424546241760254
36814 2023-02-16,23:38:27.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.3635457754135132
36932 2023-02-16,23:38:27.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3795437812805176
37035 2023-02-16,23:38:27.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4121809005737305
37152 2023-02-16,23:38:27.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3795853853225708
37268 2023-02-16,23:38:27.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.405200481414795
37393 2023-02-16,23:38:27.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3950555324554443
37533 2023-02-16,23:38:27.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4606214761734009
37757 2023-02-16,23:38:27.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3793158531188965
37862 2023-02-16,23:38:27.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4560976028442383
37983 2023-02-16,23:38:27.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3705222606658936
38108 2023-02-16,23:38:27.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4019700288772583
38255 2023-02-16,23:38:27.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3709346055984497
38362 2023-02-16,23:38:27.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3916667699813843
38487 2023-02-16,23:38:27.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 212/15000, loss = 1.3931593894958496
38108 2023-02-16,23:38:27.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3929166793823242
38362 2023-02-16,23:38:27.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3861263990402222
36932 2023-02-16,23:38:27.496 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4301420450210571
37035 2023-02-16,23:38:27.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4114878177642822
37152 2023-02-16,23:38:27.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4090347290039062
37268 2023-02-16,23:38:27.500 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4016698598861694
37393 2023-02-16,23:38:27.500 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3669352531433105
37533 2023-02-16,23:38:27.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3476771116256714
37757 2023-02-16,23:38:27.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3312790393829346
37862 2023-02-16,23:38:27.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4062013626098633
37983 2023-02-16,23:38:27.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3922420740127563
38255 2023-02-16,23:38:27.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3987170457839966
38487 2023-02-16,23:38:27.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 213/15000, loss = 1.4015839099884033
36814 2023-02-16,23:38:27.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.3743664026260376
38362 2023-02-16,23:38:27.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4029649496078491
38487 2023-02-16,23:38:27.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 214/15000, loss = 1.4182897806167603
36814 2023-02-16,23:38:27.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.3960026502609253
36932 2023-02-16,23:38:27.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3915175199508667
37035 2023-02-16,23:38:27.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4086604118347168
37152 2023-02-16,23:38:27.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3871724605560303
37268 2023-02-16,23:38:27.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3968344926834106
37393 2023-02-16,23:38:27.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3535890579223633
37533 2023-02-16,23:38:27.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4077123403549194
37757 2023-02-16,23:38:27.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.386154294013977
37862 2023-02-16,23:38:27.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849818706512451
37983 2023-02-16,23:38:27.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3870766162872314
38108 2023-02-16,23:38:27.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4076460599899292
38255 2023-02-16,23:38:27.817 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3798856735229492
38108 2023-02-16,23:38:28.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4130827188491821
38362 2023-02-16,23:38:28.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.4079420566558838
38487 2023-02-16,23:38:28.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 215/15000, loss = 1.3967678546905518
36814 2023-02-16,23:38:28.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3616548776626587
36932 2023-02-16,23:38:28.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4093905687332153
37035 2023-02-16,23:38:28.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4182722568511963
37152 2023-02-16,23:38:28.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3867621421813965
37268 2023-02-16,23:38:28.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4426021575927734
37393 2023-02-16,23:38:28.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3888647556304932
37533 2023-02-16,23:38:28.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.401944637298584
37757 2023-02-16,23:38:28.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4065120220184326
37862 2023-02-16,23:38:28.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3922159671783447
37983 2023-02-16,23:38:28.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3505560159683228
38255 2023-02-16,23:38:28.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.365098476409912
38108 2023-02-16,23:38:28.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3756816387176514
38362 2023-02-16,23:38:28.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.397810697555542
38487 2023-02-16,23:38:28.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 216/15000, loss = 1.3372586965560913
36814 2023-02-16,23:38:28.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3598241806030273
36932 2023-02-16,23:38:28.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4297645092010498
37035 2023-02-16,23:38:28.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4192142486572266
37152 2023-02-16,23:38:28.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3784220218658447
37268 2023-02-16,23:38:28.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3973966836929321
37393 2023-02-16,23:38:28.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3315157890319824
37533 2023-02-16,23:38:28.433 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3909165859222412
37757 2023-02-16,23:38:28.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.389927864074707
37862 2023-02-16,23:38:28.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.33888578414917
37983 2023-02-16,23:38:28.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4121342897415161
38255 2023-02-16,23:38:28.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3582099676132202
39095 2023-02-16,23:38:28.659 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
38108 2023-02-16,23:38:28.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890091180801392
38362 2023-02-16,23:38:28.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4280909299850464
38487 2023-02-16,23:38:28.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 217/15000, loss = 1.3641612529754639
36814 2023-02-16,23:38:28.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.394452691078186
36932 2023-02-16,23:38:28.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3617267608642578
37035 2023-02-16,23:38:28.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.3989613056182861
37152 2023-02-16,23:38:28.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3864744901657104
37268 2023-02-16,23:38:28.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3502496480941772
37393 2023-02-16,23:38:28.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3710578680038452
37533 2023-02-16,23:38:28.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.402896523475647
37757 2023-02-16,23:38:28.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3848923444747925
37862 2023-02-16,23:38:28.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3770031929016113
37983 2023-02-16,23:38:28.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4398057460784912
38255 2023-02-16,23:38:28.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3540079593658447
38108 2023-02-16,23:38:29.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3739063739776611
36814 2023-02-16,23:38:29.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3593721389770508
37152 2023-02-16,23:38:29.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.439255714416504
37393 2023-02-16,23:38:29.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3622684478759766
37533 2023-02-16,23:38:29.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.337108850479126
37862 2023-02-16,23:38:29.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098163843154907
37983 2023-02-16,23:38:29.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3884470462799072
38255 2023-02-16,23:38:29.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4308362007141113
38362 2023-02-16,23:38:29.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.4402117729187012
38487 2023-02-16,23:38:29.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 218/15000, loss = 1.346674919128418
36932 2023-02-16,23:38:29.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3675466775894165
37035 2023-02-16,23:38:29.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4040316343307495
37268 2023-02-16,23:38:29.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3487496376037598
37757 2023-02-16,23:38:29.058 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3635385036468506
37152 2023-02-16,23:38:29.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3815628290176392
37268 2023-02-16,23:38:29.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4203344583511353
37393 2023-02-16,23:38:29.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3710954189300537
37862 2023-02-16,23:38:29.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3810452222824097
37983 2023-02-16,23:38:29.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.352006196975708
38108 2023-02-16,23:38:29.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3946341276168823
38362 2023-02-16,23:38:29.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.419244647026062
36814 2023-02-16,23:38:29.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.360884189605713
36932 2023-02-16,23:38:29.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4072537422180176
37533 2023-02-16,23:38:29.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3736770153045654
37035 2023-02-16,23:38:29.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3666893243789673
37757 2023-02-16,23:38:29.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3563023805618286
38255 2023-02-16,23:38:29.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.432396650314331
38487 2023-02-16,23:38:29.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 219/15000, loss = 1.3512213230133057
37152 2023-02-16,23:38:29.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3953640460968018
37268 2023-02-16,23:38:29.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.431167483329773
38108 2023-02-16,23:38:29.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.40423583984375
38362 2023-02-16,23:38:29.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.427004337310791
36814 2023-02-16,23:38:29.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.389310598373413
36932 2023-02-16,23:38:29.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.325190782546997
37035 2023-02-16,23:38:29.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4162945747375488
37393 2023-02-16,23:38:29.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4457687139511108
37757 2023-02-16,23:38:29.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3648089170455933
37983 2023-02-16,23:38:29.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4126503467559814
37533 2023-02-16,23:38:29.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3835532665252686
37862 2023-02-16,23:38:29.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.383458137512207
38255 2023-02-16,23:38:29.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3834103345870972
38487 2023-02-16,23:38:29.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 220/15000, loss = 1.3886916637420654
38108 2023-02-16,23:38:29.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3719393014907837
37152 2023-02-16,23:38:29.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.374460220336914
37268 2023-02-16,23:38:29.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3875313997268677
38362 2023-02-16,23:38:29.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3945587873458862
36814 2023-02-16,23:38:29.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3757407665252686
36932 2023-02-16,23:38:29.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.378434181213379
37035 2023-02-16,23:38:29.982 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4348316192626953
37393 2023-02-16,23:38:29.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.398335576057434
37533 2023-02-16,23:38:29.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.372797966003418
37757 2023-02-16,23:38:29.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3600751161575317
37862 2023-02-16,23:38:29.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3759595155715942
37983 2023-02-16,23:38:29.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3990665674209595
38255 2023-02-16,23:38:29.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.3740923404693604
38487 2023-02-16,23:38:29.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 221/15000, loss = 1.3441524505615234
38108 2023-02-16,23:38:30.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.395899772644043
36932 2023-02-16,23:38:30.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3666396141052246
37035 2023-02-16,23:38:30.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4159162044525146
37152 2023-02-16,23:38:30.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3740265369415283
37268 2023-02-16,23:38:30.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4312721490859985
37533 2023-02-16,23:38:30.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825085163116455
37757 2023-02-16,23:38:30.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.370022177696228
37862 2023-02-16,23:38:30.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.382563591003418
37983 2023-02-16,23:38:30.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4112541675567627
38255 2023-02-16,23:38:30.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4001826047897339
38362 2023-02-16,23:38:30.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.403921127319336
38487 2023-02-16,23:38:30.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 222/15000, loss = 1.3682374954223633
36814 2023-02-16,23:38:30.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4045875072479248
37393 2023-02-16,23:38:30.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4202885627746582
38108 2023-02-16,23:38:30.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3605878353118896
36932 2023-02-16,23:38:30.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4148881435394287
37152 2023-02-16,23:38:30.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3873215913772583
37268 2023-02-16,23:38:30.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.356846809387207
38255 2023-02-16,23:38:30.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4032087326049805
38362 2023-02-16,23:38:30.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4437335729599
38487 2023-02-16,23:38:30.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 223/15000, loss = 1.4137847423553467
36814 2023-02-16,23:38:30.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.4039068222045898
37035 2023-02-16,23:38:30.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.40847647190094
37393 2023-02-16,23:38:30.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.378064751625061
37533 2023-02-16,23:38:30.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4121124744415283
37757 2023-02-16,23:38:30.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.387082576751709
37862 2023-02-16,23:38:30.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4297572374343872
37983 2023-02-16,23:38:30.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4114981889724731
36932 2023-02-16,23:38:30.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4258527755737305
37393 2023-02-16,23:38:30.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3656551837921143
37757 2023-02-16,23:38:30.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4587312936782837
37862 2023-02-16,23:38:30.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3799128532409668
38362 2023-02-16,23:38:30.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.377049446105957
38487 2023-02-16,23:38:30.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 224/15000, loss = 1.3542098999023438
36814 2023-02-16,23:38:30.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.4067373275756836
37035 2023-02-16,23:38:30.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3769738674163818
37152 2023-02-16,23:38:30.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3912124633789062
37268 2023-02-16,23:38:30.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3838706016540527
37533 2023-02-16,23:38:30.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3564246892929077
37983 2023-02-16,23:38:30.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3443164825439453
38108 2023-02-16,23:38:30.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3873039484024048
38255 2023-02-16,23:38:30.919 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3817925453186035
37862 2023-02-16,23:38:31.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3370355367660522
38108 2023-02-16,23:38:31.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4183852672576904
38255 2023-02-16,23:38:31.219 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.3932400941848755
38362 2023-02-16,23:38:31.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4075853824615479
38487 2023-02-16,23:38:31.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 225/15000, loss = 1.359204888343811
36814 2023-02-16,23:38:31.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.348877191543579
36932 2023-02-16,23:38:31.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3303223848342896
37035 2023-02-16,23:38:31.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3889398574829102
37152 2023-02-16,23:38:31.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3838486671447754
37268 2023-02-16,23:38:31.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3776330947875977
37393 2023-02-16,23:38:31.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3882800340652466
37533 2023-02-16,23:38:31.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4530972242355347
37983 2023-02-16,23:38:31.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3786818981170654
37757 2023-02-16,23:38:31.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3985062837600708
39195 2023-02-16,23:38:31.390 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37862 2023-02-16,23:38:31.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3817752599716187
38108 2023-02-16,23:38:31.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4200986623764038
38255 2023-02-16,23:38:31.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3980313539505005
38362 2023-02-16,23:38:31.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3664507865905762
38487 2023-02-16,23:38:31.529 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 226/15000, loss = 1.384870171546936
36814 2023-02-16,23:38:31.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.401321530342102
36932 2023-02-16,23:38:31.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.446828842163086
37035 2023-02-16,23:38:31.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3797115087509155
37152 2023-02-16,23:38:31.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3623350858688354
37268 2023-02-16,23:38:31.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3775122165679932
37393 2023-02-16,23:38:31.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3619191646575928
37533 2023-02-16,23:38:31.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.382604718208313
37757 2023-02-16,23:38:31.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3973127603530884
37983 2023-02-16,23:38:31.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3738934993743896
37862 2023-02-16,23:38:31.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4196423292160034
38108 2023-02-16,23:38:31.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3539484739303589
38255 2023-02-16,23:38:31.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4344134330749512
38487 2023-02-16,23:38:31.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 227/15000, loss = 1.3571571111679077
36814 2023-02-16,23:38:31.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.3585877418518066
36932 2023-02-16,23:38:31.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3760478496551514
37035 2023-02-16,23:38:31.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.369450330734253
37152 2023-02-16,23:38:31.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3666529655456543
37268 2023-02-16,23:38:31.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3918850421905518
37393 2023-02-16,23:38:31.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3724387884140015
37533 2023-02-16,23:38:31.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.407225251197815
37757 2023-02-16,23:38:31.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410789966583252
37983 2023-02-16,23:38:31.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3457518815994263
38362 2023-02-16,23:38:31.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.371032476425171
38108 2023-02-16,23:38:32.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4249743223190308
37152 2023-02-16,23:38:32.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3500930070877075
37862 2023-02-16,23:38:32.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.397792935371399
38255 2023-02-16,23:38:32.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4015766382217407
38487 2023-02-16,23:38:32.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 228/15000, loss = 1.3679759502410889
36814 2023-02-16,23:38:32.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.41660737991333
36932 2023-02-16,23:38:32.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.399982213973999
37035 2023-02-16,23:38:32.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.4177186489105225
37268 2023-02-16,23:38:32.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3931831121444702
37393 2023-02-16,23:38:32.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3546043634414673
37533 2023-02-16,23:38:32.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3946045637130737
37757 2023-02-16,23:38:32.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4159283638000488
37983 2023-02-16,23:38:32.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.4000365734100342
38362 2023-02-16,23:38:32.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.3988460302352905
37035 2023-02-16,23:38:32.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3954817056655884
37862 2023-02-16,23:38:32.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3643510341644287
38108 2023-02-16,23:38:32.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3896949291229248
38255 2023-02-16,23:38:32.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3858481645584106
36932 2023-02-16,23:38:32.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4345053434371948
37152 2023-02-16,23:38:32.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4121434688568115
37268 2023-02-16,23:38:32.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3659288883209229
37533 2023-02-16,23:38:32.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4250407218933105
37757 2023-02-16,23:38:32.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3837087154388428
37983 2023-02-16,23:38:32.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3888200521469116
38362 2023-02-16,23:38:32.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3798718452453613
38487 2023-02-16,23:38:32.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 229/15000, loss = 1.361421823501587
36814 2023-02-16,23:38:32.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.362342119216919
37393 2023-02-16,23:38:32.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3721331357955933
37035 2023-02-16,23:38:32.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4027634859085083
38108 2023-02-16,23:38:32.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4010529518127441
37533 2023-02-16,23:38:32.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.399808645248413
37862 2023-02-16,23:38:32.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3736114501953125
37983 2023-02-16,23:38:32.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3492547273635864
38255 2023-02-16,23:38:32.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4460973739624023
38362 2023-02-16,23:38:32.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.3651310205459595
38487 2023-02-16,23:38:32.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 230/15000, loss = 1.4208451509475708
36814 2023-02-16,23:38:32.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.378143548965454
36932 2023-02-16,23:38:32.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4213013648986816
37268 2023-02-16,23:38:32.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4087599515914917
37757 2023-02-16,23:38:32.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4127534627914429
37152 2023-02-16,23:38:32.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.380177617073059
37393 2023-02-16,23:38:32.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3610920906066895
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38108 2023-02-16,23:38:33.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.406976342201233
37533 2023-02-16,23:38:33.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3968441486358643
37862 2023-02-16,23:38:33.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3975920677185059
37983 2023-02-16,23:38:33.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3597205877304077
38255 2023-02-16,23:38:33.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4120713472366333
38362 2023-02-16,23:38:33.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3581995964050293
38487 2023-02-16,23:38:33.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 231/15000, loss = 1.4326229095458984
36814 2023-02-16,23:38:33.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.3838955163955688
36932 2023-02-16,23:38:33.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4027714729309082
37268 2023-02-16,23:38:33.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.384618878364563
37393 2023-02-16,23:38:33.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4109656810760498
37757 2023-02-16,23:38:33.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3541510105133057
37152 2023-02-16,23:38:33.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.435339331626892
37035 2023-02-16,23:38:33.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3660650253295898
36932 2023-02-16,23:38:33.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4448171854019165
37393 2023-02-16,23:38:33.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4221582412719727
37533 2023-02-16,23:38:33.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4231406450271606
37862 2023-02-16,23:38:33.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.425909161567688
37983 2023-02-16,23:38:33.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3680179119110107
38108 2023-02-16,23:38:33.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4330360889434814
38255 2023-02-16,23:38:33.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4313669204711914
38362 2023-02-16,23:38:33.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3540953397750854
38487 2023-02-16,23:38:33.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 232/15000, loss = 1.3971328735351562
36814 2023-02-16,23:38:33.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.4380584955215454
37035 2023-02-16,23:38:33.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3709007501602173
37152 2023-02-16,23:38:33.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4312856197357178
37268 2023-02-16,23:38:33.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3943251371383667
37757 2023-02-16,23:38:33.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3794366121292114
36932 2023-02-16,23:38:33.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4027960300445557
38108 2023-02-16,23:38:33.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4254069328308105
38487 2023-02-16,23:38:33.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 233/15000, loss = 1.3705663681030273
37035 2023-02-16,23:38:33.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.379886507987976
37152 2023-02-16,23:38:33.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4296152591705322
37268 2023-02-16,23:38:33.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.400738000869751
37393 2023-02-16,23:38:33.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4414420127868652
37533 2023-02-16,23:38:33.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3878718614578247
37757 2023-02-16,23:38:33.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3481783866882324
37862 2023-02-16,23:38:33.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4128726720809937
37983 2023-02-16,23:38:33.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4008347988128662
38255 2023-02-16,23:38:33.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.398305892944336
38362 2023-02-16,23:38:33.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4308768510818481
36814 2023-02-16,23:38:33.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.3444775342941284
37393 2023-02-16,23:38:33.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3689305782318115
38487 2023-02-16,23:38:33.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 234/15000, loss = 1.428599238395691
36932 2023-02-16,23:38:34.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.366733431816101
37152 2023-02-16,23:38:34.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3661162853240967
37533 2023-02-16,23:38:34.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3837252855300903
37862 2023-02-16,23:38:34.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4329473972320557
37983 2023-02-16,23:38:34.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.386836051940918
38108 2023-02-16,23:38:34.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.370528221130371
38255 2023-02-16,23:38:34.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4532935619354248
38362 2023-02-16,23:38:34.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4324209690093994
36814 2023-02-16,23:38:34.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.3763799667358398
37035 2023-02-16,23:38:34.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3981976509094238
37268 2023-02-16,23:38:34.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4289300441741943
37757 2023-02-16,23:38:34.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4008463621139526
37393 2023-02-16,23:38:34.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3970024585723877
37533 2023-02-16,23:38:34.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.423325777053833
37862 2023-02-16,23:38:34.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3585156202316284
38255 2023-02-16,23:38:34.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4391059875488281
38487 2023-02-16,23:38:34.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 235/15000, loss = 1.4252680540084839
36814 2023-02-16,23:38:34.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.4237816333770752
36932 2023-02-16,23:38:34.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3697413206100464
37152 2023-02-16,23:38:34.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3781503438949585
37268 2023-02-16,23:38:34.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3791117668151855
37757 2023-02-16,23:38:34.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.374091386795044
37983 2023-02-16,23:38:34.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4562796354293823
38108 2023-02-16,23:38:34.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3922755718231201
38362 2023-02-16,23:38:34.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.383418083190918
37035 2023-02-16,23:38:34.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4559376239776611
39288 2023-02-16,23:38:34.506 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37393 2023-02-16,23:38:34.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4133312702178955
36814 2023-02-16,23:38:34.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.37811279296875
37533 2023-02-16,23:38:34.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4102144241333008
37757 2023-02-16,23:38:34.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.461520791053772
37862 2023-02-16,23:38:34.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3216338157653809
37983 2023-02-16,23:38:34.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4063305854797363
38255 2023-02-16,23:38:34.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4019414186477661
38362 2023-02-16,23:38:34.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.374181866645813
38487 2023-02-16,23:38:34.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 236/15000, loss = 1.3970065116882324
36932 2023-02-16,23:38:34.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4158140420913696
37035 2023-02-16,23:38:34.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.405592679977417
37152 2023-02-16,23:38:34.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4191348552703857
37268 2023-02-16,23:38:34.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4226655960083008
38108 2023-02-16,23:38:34.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3870477676391602
38487 2023-02-16,23:38:34.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 237/15000, loss = 1.412880539894104
36814 2023-02-16,23:38:34.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.4366166591644287
36932 2023-02-16,23:38:34.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3841092586517334
37152 2023-02-16,23:38:34.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3464316129684448
37268 2023-02-16,23:38:34.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.37709641456604
37393 2023-02-16,23:38:34.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.405768871307373
37533 2023-02-16,23:38:34.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4073803424835205
37862 2023-02-16,23:38:34.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3793654441833496
37983 2023-02-16,23:38:34.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849270343780518
38255 2023-02-16,23:38:34.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3928302526474
38362 2023-02-16,23:38:34.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4003252983093262
37035 2023-02-16,23:38:34.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4073911905288696
37757 2023-02-16,23:38:34.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3472095727920532
38108 2023-02-16,23:38:34.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3505363464355469
37152 2023-02-16,23:38:35.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.333256483078003
37268 2023-02-16,23:38:35.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3974077701568604
37757 2023-02-16,23:38:35.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.407715916633606
37862 2023-02-16,23:38:35.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3312581777572632
37983 2023-02-16,23:38:35.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3923168182373047
37393 2023-02-16,23:38:35.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4027634859085083
38255 2023-02-16,23:38:35.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4076926708221436
38362 2023-02-16,23:38:35.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4031202793121338
38487 2023-02-16,23:38:35.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 238/15000, loss = 1.3717427253723145
36932 2023-02-16,23:38:35.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.421743392944336
36814 2023-02-16,23:38:35.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3949280977249146
37035 2023-02-16,23:38:35.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.370640516281128
37533 2023-02-16,23:38:35.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4021800756454468
38108 2023-02-16,23:38:35.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4120620489120483
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38487 2023-02-16,23:38:35.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 239/15000, loss = 1.3751277923583984
36814 2023-02-16,23:38:35.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.362083911895752
36932 2023-02-16,23:38:35.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4090017080307007
37035 2023-02-16,23:38:35.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3727818727493286
37152 2023-02-16,23:38:35.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4152312278747559
37268 2023-02-16,23:38:35.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3549647331237793
37533 2023-02-16,23:38:35.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.417534351348877
37757 2023-02-16,23:38:35.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4021921157836914
37862 2023-02-16,23:38:35.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3861855268478394
37983 2023-02-16,23:38:35.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3386986255645752
38255 2023-02-16,23:38:35.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.413076639175415
38362 2023-02-16,23:38:35.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3818256855010986
37393 2023-02-16,23:38:35.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3965089321136475
38108 2023-02-16,23:38:35.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4398077726364136
37862 2023-02-16,23:38:35.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.406594157218933
36814 2023-02-16,23:38:35.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4085774421691895
37757 2023-02-16,23:38:35.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3910657167434692
36932 2023-02-16,23:38:35.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4058616161346436
37035 2023-02-16,23:38:35.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4060693979263306
37268 2023-02-16,23:38:35.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.42422354221344
37393 2023-02-16,23:38:35.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.443017840385437
37983 2023-02-16,23:38:35.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3768742084503174
38255 2023-02-16,23:38:35.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.375769853591919
38362 2023-02-16,23:38:35.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.393275260925293
37152 2023-02-16,23:38:35.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.3998057842254639
37533 2023-02-16,23:38:35.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.365715742111206
38487 2023-02-16,23:38:35.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 240/15000, loss = 1.3916571140289307
38108 2023-02-16,23:38:35.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3883436918258667
37757 2023-02-16,23:38:36.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4032859802246094
37862 2023-02-16,23:38:36.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3899805545806885
37393 2023-02-16,23:38:36.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3977925777435303
37983 2023-02-16,23:38:36.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098751544952393
38108 2023-02-16,23:38:36.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3519761562347412
38255 2023-02-16,23:38:36.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890504837036133
38362 2023-02-16,23:38:36.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.3980488777160645
38487 2023-02-16,23:38:36.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 241/15000, loss = 1.3860883712768555
36814 2023-02-16,23:38:36.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.3961267471313477
36932 2023-02-16,23:38:36.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3884327411651611
37035 2023-02-16,23:38:36.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3984649181365967
37152 2023-02-16,23:38:36.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3743003606796265
37268 2023-02-16,23:38:36.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.383108139038086
37533 2023-02-16,23:38:36.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4098132848739624
37393 2023-02-16,23:38:36.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3512295484542847
37757 2023-02-16,23:38:36.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3371045589447021
37862 2023-02-16,23:38:36.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.38485848903656
38108 2023-02-16,23:38:36.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4126092195510864
38255 2023-02-16,23:38:36.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3739131689071655
38362 2023-02-16,23:38:36.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.4344203472137451
38487 2023-02-16,23:38:36.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 242/15000, loss = 1.4029258489608765
36814 2023-02-16,23:38:36.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.4223213195800781
36932 2023-02-16,23:38:36.650 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4107431173324585
37035 2023-02-16,23:38:36.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3790677785873413
37152 2023-02-16,23:38:36.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4155772924423218
37268 2023-02-16,23:38:36.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3955068588256836
37533 2023-02-16,23:38:36.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4070271253585815
37983 2023-02-16,23:38:36.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.38106107711792
36932 2023-02-16,23:38:36.981 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3869036436080933
37035 2023-02-16,23:38:36.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4280810356140137
37393 2023-02-16,23:38:36.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3492193222045898
37757 2023-02-16,23:38:36.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3741989135742188
37862 2023-02-16,23:38:36.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3635457754135132
37983 2023-02-16,23:38:36.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.383518099784851
38108 2023-02-16,23:38:36.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3990755081176758
38255 2023-02-16,23:38:36.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3947745561599731
38362 2023-02-16,23:38:36.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4016969203948975
38487 2023-02-16,23:38:36.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 243/15000, loss = 1.407841444015503
36814 2023-02-16,23:38:37.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.405550241470337
37152 2023-02-16,23:38:37.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4321852922439575
37268 2023-02-16,23:38:37.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3944411277770996
37533 2023-02-16,23:38:37.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4154458045959473
36932 2023-02-16,23:38:37.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3333591222763062
37035 2023-02-16,23:38:37.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3911603689193726
37393 2023-02-16,23:38:37.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4200434684753418
37533 2023-02-16,23:38:37.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3913390636444092
37862 2023-02-16,23:38:37.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.356215476989746
37983 2023-02-16,23:38:37.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.375914454460144
38108 2023-02-16,23:38:37.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4113610982894897
38255 2023-02-16,23:38:37.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4041668176651
38362 2023-02-16,23:38:37.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.3858816623687744
38487 2023-02-16,23:38:37.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 244/15000, loss = 1.3978394269943237
36814 2023-02-16,23:38:37.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.3777624368667603
37152 2023-02-16,23:38:37.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3813438415527344
37268 2023-02-16,23:38:37.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3862624168395996
37757 2023-02-16,23:38:37.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.383379578590393
37533 2023-02-16,23:38:37.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3333778381347656
37862 2023-02-16,23:38:37.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3649592399597168
37983 2023-02-16,23:38:37.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3825569152832031
38108 2023-02-16,23:38:37.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4114673137664795
38362 2023-02-16,23:38:37.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4461355209350586
38487 2023-02-16,23:38:37.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 245/15000, loss = 1.4280247688293457
36932 2023-02-16,23:38:37.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4096473455429077
37035 2023-02-16,23:38:37.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4095200300216675
37152 2023-02-16,23:38:37.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.360849380493164
37268 2023-02-16,23:38:37.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4059054851531982
37393 2023-02-16,23:38:37.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4318926334381104
38255 2023-02-16,23:38:37.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3720600605010986
36814 2023-02-16,23:38:37.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3517072200775146
37757 2023-02-16,23:38:37.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3730508089065552
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39095 2023-02-16,23:38:37.744 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
39095 2023-02-16,23:38:37.809 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37035 2023-02-16,23:38:37.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4280130863189697
37268 2023-02-16,23:38:37.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4050049781799316
37393 2023-02-16,23:38:37.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3882819414138794
37862 2023-02-16,23:38:37.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3600099086761475
38362 2023-02-16,23:38:37.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4123116731643677
37152 2023-02-16,23:38:38.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4217640161514282
37533 2023-02-16,23:38:38.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.389312982559204
37983 2023-02-16,23:38:38.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4299510717391968
38108 2023-02-16,23:38:38.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3444125652313232
38255 2023-02-16,23:38:38.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.395947813987732
38487 2023-02-16,23:38:38.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 246/15000, loss = 1.440071702003479
36814 2023-02-16,23:38:38.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.410075306892395
36932 2023-02-16,23:38:38.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3836277723312378
37757 2023-02-16,23:38:38.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825640678405762
37035 2023-02-16,23:38:38.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3608747720718384
37393 2023-02-16,23:38:38.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4306190013885498
39438 2023-02-16,23:38:38.286 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
38108 2023-02-16,23:38:38.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3786959648132324
37152 2023-02-16,23:38:38.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3914014101028442
37533 2023-02-16,23:38:38.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3910773992538452
36814 2023-02-16,23:38:38.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.433785080909729
36932 2023-02-16,23:38:38.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.377456784248352
37757 2023-02-16,23:38:38.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4124674797058105
37862 2023-02-16,23:38:38.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.370155930519104
38255 2023-02-16,23:38:38.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.360565423965454
38362 2023-02-16,23:38:38.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4312800168991089
38487 2023-02-16,23:38:38.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 247/15000, loss = 1.4191832542419434
37983 2023-02-16,23:38:38.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3799309730529785
37268 2023-02-16,23:38:38.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4137535095214844
37035 2023-02-16,23:38:38.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3674355745315552
37393 2023-02-16,23:38:38.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3569810390472412
38108 2023-02-16,23:38:38.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.373978614807129
36814 2023-02-16,23:38:38.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4297372102737427
36932 2023-02-16,23:38:38.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4156707525253296
37152 2023-02-16,23:38:38.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3872549533843994
37757 2023-02-16,23:38:38.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3562220335006714
37862 2023-02-16,23:38:38.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.387071132659912
38255 2023-02-16,23:38:38.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3874053955078125
38362 2023-02-16,23:38:38.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.398440957069397
38487 2023-02-16,23:38:38.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 248/15000, loss = 1.427038550376892
37983 2023-02-16,23:38:38.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3369649648666382
37268 2023-02-16,23:38:38.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.412217378616333
37533 2023-02-16,23:38:38.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4248058795928955
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37035 2023-02-16,23:38:38.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4045329093933105
37393 2023-02-16,23:38:38.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3840339183807373
38108 2023-02-16,23:38:38.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3457586765289307
36814 2023-02-16,23:38:38.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.407800555229187
37152 2023-02-16,23:38:38.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3195017576217651
37757 2023-02-16,23:38:38.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4543211460113525
37862 2023-02-16,23:38:38.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4587016105651855
38255 2023-02-16,23:38:38.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4183250665664673
38362 2023-02-16,23:38:38.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4533509016036987
38487 2023-02-16,23:38:38.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 249/15000, loss = 1.3945646286010742
36932 2023-02-16,23:38:38.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3665720224380493
37983 2023-02-16,23:38:38.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.381813645362854
37268 2023-02-16,23:38:38.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4240317344665527
37533 2023-02-16,23:38:38.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4018032550811768
37035 2023-02-16,23:38:39.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3240529298782349
37393 2023-02-16,23:38:39.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3779046535491943
38108 2023-02-16,23:38:39.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.3999874591827393
36814 2023-02-16,23:38:39.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4528923034667969
37152 2023-02-16,23:38:39.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.40781569480896
37757 2023-02-16,23:38:39.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3827126026153564
37862 2023-02-16,23:38:39.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3986127376556396
38255 2023-02-16,23:38:39.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4199788570404053
38362 2023-02-16,23:38:39.301 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.439226746559143
38487 2023-02-16,23:38:39.301 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 250/15000, loss = 1.4038615226745605
36932 2023-02-16,23:38:39.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.393752098083496
37983 2023-02-16,23:38:39.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4197734594345093
37268 2023-02-16,23:38:39.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.37992262840271
37533 2023-02-16,23:38:39.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4078675508499146
37035 2023-02-16,23:38:39.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.378392219543457
37393 2023-02-16,23:38:39.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3781710863113403
38108 2023-02-16,23:38:39.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3887932300567627
36814 2023-02-16,23:38:39.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4120895862579346
37152 2023-02-16,23:38:39.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3761539459228516
37757 2023-02-16,23:38:39.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4069808721542358
37862 2023-02-16,23:38:39.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3972076177597046
38255 2023-02-16,23:38:39.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3538718223571777
38362 2023-02-16,23:38:39.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.401980996131897
38487 2023-02-16,23:38:39.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 251/15000, loss = 1.4436590671539307
37983 2023-02-16,23:38:39.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.39780855178833
36932 2023-02-16,23:38:39.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3813189268112183
37268 2023-02-16,23:38:39.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4075411558151245
37533 2023-02-16,23:38:39.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3898719549179077
37035 2023-02-16,23:38:39.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.368325114250183
37393 2023-02-16,23:38:39.795 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3928221464157104
38108 2023-02-16,23:38:39.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3493329286575317
36814 2023-02-16,23:38:39.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.360001802444458
37757 2023-02-16,23:38:39.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3952500820159912
37862 2023-02-16,23:38:39.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.341038465499878
38255 2023-02-16,23:38:39.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.424941062927246
38362 2023-02-16,23:38:39.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3929656744003296
38487 2023-02-16,23:38:39.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 252/15000, loss = 1.377074122428894
37152 2023-02-16,23:38:39.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4122072458267212
37983 2023-02-16,23:38:39.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3642971515655518
36932 2023-02-16,23:38:39.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3870832920074463
37268 2023-02-16,23:38:39.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.376832127571106
37533 2023-02-16,23:38:39.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3820335865020752
37035 2023-02-16,23:38:40.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4149466753005981
37393 2023-02-16,23:38:40.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3932983875274658
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39195 2023-02-16,23:38:40.184 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
39195 2023-02-16,23:38:40.243 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
38108 2023-02-16,23:38:40.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3597700595855713
36814 2023-02-16,23:38:40.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3896045684814453
37757 2023-02-16,23:38:40.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4255198240280151
37862 2023-02-16,23:38:40.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4159789085388184
38255 2023-02-16,23:38:40.281 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3896230459213257
38362 2023-02-16,23:38:40.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.407710313796997
38487 2023-02-16,23:38:40.283 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 253/15000, loss = 1.4076182842254639
37152 2023-02-16,23:38:40.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3832930326461792
37983 2023-02-16,23:38:40.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3735294342041016
36932 2023-02-16,23:38:40.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3548978567123413
37268 2023-02-16,23:38:40.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3737995624542236
37533 2023-02-16,23:38:40.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.349726915359497
37035 2023-02-16,23:38:40.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.425012469291687
37393 2023-02-16,23:38:40.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3659911155700684
38487 2023-02-16,23:38:40.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 254/15000, loss = 1.3664779663085938
36814 2023-02-16,23:38:40.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.3739283084869385
37757 2023-02-16,23:38:40.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.400406837463379
37862 2023-02-16,23:38:40.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3838568925857544
38108 2023-02-16,23:38:40.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.368088722229004
38255 2023-02-16,23:38:40.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4011216163635254
38362 2023-02-16,23:38:40.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4130922555923462
37152 2023-02-16,23:38:40.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3601621389389038
37983 2023-02-16,23:38:40.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.397652268409729
36932 2023-02-16,23:38:40.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4262524843215942
37268 2023-02-16,23:38:40.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3814013004302979
37533 2023-02-16,23:38:40.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3815346956253052
37393 2023-02-16,23:38:40.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4097150564193726
37035 2023-02-16,23:38:40.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.329983115196228
38487 2023-02-16,23:38:40.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 255/15000, loss = 1.3709810972213745
36814 2023-02-16,23:38:40.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.4144783020019531
37757 2023-02-16,23:38:40.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.396785020828247
37862 2023-02-16,23:38:40.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.412676453590393
38108 2023-02-16,23:38:40.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4007964134216309
38255 2023-02-16,23:38:40.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.406997561454773
37152 2023-02-16,23:38:40.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4250757694244385
37983 2023-02-16,23:38:40.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4260311126708984
38362 2023-02-16,23:38:40.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3757164478302002
36932 2023-02-16,23:38:40.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4066197872161865
37268 2023-02-16,23:38:40.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3250631093978882
37533 2023-02-16,23:38:40.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4018992185592651
37393 2023-02-16,23:38:41.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3858705759048462
37035 2023-02-16,23:38:41.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4451919794082642
38487 2023-02-16,23:38:41.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 256/15000, loss = 1.398800015449524
36814 2023-02-16,23:38:41.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.3702495098114014
37757 2023-02-16,23:38:41.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4240391254425049
37862 2023-02-16,23:38:41.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3542373180389404
38108 2023-02-16,23:38:41.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3868001699447632
38255 2023-02-16,23:38:41.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4331021308898926
37152 2023-02-16,23:38:41.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4208199977874756
37983 2023-02-16,23:38:41.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4130088090896606
38362 2023-02-16,23:38:41.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.389018177986145
36932 2023-02-16,23:38:41.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3650761842727661
37268 2023-02-16,23:38:41.281 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3773884773254395
37533 2023-02-16,23:38:41.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3626577854156494
37393 2023-02-16,23:38:41.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3961513042449951
37035 2023-02-16,23:38:41.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3768069744110107
39532 2023-02-16,23:38:41.521 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
38487 2023-02-16,23:38:41.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 257/15000, loss = 1.3799241781234741
36814 2023-02-16,23:38:41.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.3949096202850342
37757 2023-02-16,23:38:41.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.387649655342102
37862 2023-02-16,23:38:41.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.379462480545044
38108 2023-02-16,23:38:41.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4561681747436523
38255 2023-02-16,23:38:41.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4255166053771973
37152 2023-02-16,23:38:41.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4087672233581543
37983 2023-02-16,23:38:41.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4330158233642578
36932 2023-02-16,23:38:41.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.419567346572876
37268 2023-02-16,23:38:41.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3467721939086914
38362 2023-02-16,23:38:41.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.3739609718322754
37533 2023-02-16,23:38:41.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.396096110343933
37393 2023-02-16,23:38:41.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4007381200790405
37035 2023-02-16,23:38:41.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4001097679138184
36814 2023-02-16,23:38:41.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.430156946182251
37983 2023-02-16,23:38:41.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3584315776824951
36932 2023-02-16,23:38:41.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3889296054840088
37152 2023-02-16,23:38:41.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4517853260040283
37757 2023-02-16,23:38:41.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3840969800949097
38108 2023-02-16,23:38:41.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4063314199447632
38362 2023-02-16,23:38:41.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.3947160243988037
37393 2023-02-16,23:38:41.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4298129081726074
38255 2023-02-16,23:38:41.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3704451322555542
37268 2023-02-16,23:38:41.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4004218578338623
37035 2023-02-16,23:38:41.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.434637427330017
37533 2023-02-16,23:38:41.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3780138492584229
38487 2023-02-16,23:38:41.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 258/15000, loss = 1.365151047706604
37862 2023-02-16,23:38:41.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3480733633041382
36814 2023-02-16,23:38:42.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.3992890119552612
37983 2023-02-16,23:38:42.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3213940858840942
36932 2023-02-16,23:38:42.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4778164625167847
37152 2023-02-16,23:38:42.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4030086994171143
37757 2023-02-16,23:38:42.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.423540711402893
38108 2023-02-16,23:38:42.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.38493013381958
38362 2023-02-16,23:38:42.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4042717218399048
37393 2023-02-16,23:38:42.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3783389329910278
38255 2023-02-16,23:38:42.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3921698331832886
37035 2023-02-16,23:38:42.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4217373132705688
37268 2023-02-16,23:38:42.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4185552597045898
37533 2023-02-16,23:38:42.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3935420513153076
38487 2023-02-16,23:38:42.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 259/15000, loss = 1.3582764863967896
37862 2023-02-16,23:38:42.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.40079665184021
37983 2023-02-16,23:38:42.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3792963027954102
36814 2023-02-16,23:38:42.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.352078914642334
37152 2023-02-16,23:38:42.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.392047643661499
36932 2023-02-16,23:38:42.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3667620420455933
37757 2023-02-16,23:38:42.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.410740613937378
38108 2023-02-16,23:38:42.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3922039270401
38362 2023-02-16,23:38:42.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.372049331665039
37393 2023-02-16,23:38:42.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4240739345550537
37533 2023-02-16,23:38:42.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4185209274291992
38255 2023-02-16,23:38:42.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3870580196380615
37035 2023-02-16,23:38:42.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4027091264724731
37268 2023-02-16,23:38:42.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4103832244873047
38487 2023-02-16,23:38:42.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 260/15000, loss = 1.3541311025619507
37862 2023-02-16,23:38:42.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.374068021774292
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37983 2023-02-16,23:38:42.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3311508893966675
36814 2023-02-16,23:38:42.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.4205113649368286
37152 2023-02-16,23:38:42.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4010534286499023
36932 2023-02-16,23:38:42.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3783293962478638
37757 2023-02-16,23:38:42.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4083727598190308
38362 2023-02-16,23:38:42.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3959765434265137
37393 2023-02-16,23:38:42.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.377199649810791
37533 2023-02-16,23:38:42.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4019967317581177
38255 2023-02-16,23:38:42.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3506195545196533
37035 2023-02-16,23:38:42.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4452797174453735
37268 2023-02-16,23:38:42.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3939876556396484
37862 2023-02-16,23:38:42.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4615273475646973
38108 2023-02-16,23:38:42.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3388214111328125
38487 2023-02-16,23:38:42.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 261/15000, loss = 1.4307851791381836
37983 2023-02-16,23:38:43.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3861522674560547
36814 2023-02-16,23:38:43.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3939845561981201
36932 2023-02-16,23:38:43.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.3717890977859497
37152 2023-02-16,23:38:43.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4062979221343994
37393 2023-02-16,23:38:43.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3972067832946777
37533 2023-02-16,23:38:43.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.391737699508667
38362 2023-02-16,23:38:43.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3606185913085938
38255 2023-02-16,23:38:43.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.412048578262329
37035 2023-02-16,23:38:43.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4017345905303955
37268 2023-02-16,23:38:43.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3936430215835571
37757 2023-02-16,23:38:43.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.402435541152954
38487 2023-02-16,23:38:43.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 262/15000, loss = 1.4324021339416504
37862 2023-02-16,23:38:43.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3473106622695923
38108 2023-02-16,23:38:43.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3768463134765625
37983 2023-02-16,23:38:43.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.406661033630371
36932 2023-02-16,23:38:43.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4474910497665405
36814 2023-02-16,23:38:43.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.336586356163025
37152 2023-02-16,23:38:43.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4033935070037842
37533 2023-02-16,23:38:43.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.398393988609314
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39288 2023-02-16,23:38:43.543 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37393 2023-02-16,23:38:43.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3559783697128296
38362 2023-02-16,23:38:43.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.387359619140625
38255 2023-02-16,23:38:43.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4397633075714111
37035 2023-02-16,23:38:43.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3671642541885376
37268 2023-02-16,23:38:43.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.392327070236206
39288 2023-02-16,23:38:43.597 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37757 2023-02-16,23:38:43.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4184951782226562
38487 2023-02-16,23:38:43.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 263/15000, loss = 1.3834284543991089
37862 2023-02-16,23:38:43.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4079222679138184
38108 2023-02-16,23:38:43.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098200798034668
37983 2023-02-16,23:38:43.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3900341987609863
36932 2023-02-16,23:38:43.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3737831115722656
36814 2023-02-16,23:38:43.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.3672438859939575
37152 2023-02-16,23:38:43.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.399182915687561
37533 2023-02-16,23:38:43.817 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3863133192062378
37393 2023-02-16,23:38:43.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4258465766906738
38362 2023-02-16,23:38:43.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4184422492980957
38255 2023-02-16,23:38:43.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3885014057159424
37268 2023-02-16,23:38:43.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4450099468231201
38487 2023-02-16,23:38:43.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 264/15000, loss = 1.374261498451233
37035 2023-02-16,23:38:43.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3695669174194336
37757 2023-02-16,23:38:43.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3664071559906006
37862 2023-02-16,23:38:43.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4022297859191895
38108 2023-02-16,23:38:43.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3810049295425415
37983 2023-02-16,23:38:44.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.384934902191162
36932 2023-02-16,23:38:44.035 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3814455270767212
37152 2023-02-16,23:38:44.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.39783775806427
36814 2023-02-16,23:38:44.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3876148462295532
37533 2023-02-16,23:38:44.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3708088397979736
37393 2023-02-16,23:38:44.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3837107419967651
38362 2023-02-16,23:38:44.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4201205968856812
38255 2023-02-16,23:38:44.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3520019054412842
38487 2023-02-16,23:38:44.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 265/15000, loss = 1.4003058671951294
37268 2023-02-16,23:38:44.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3753554821014404
37035 2023-02-16,23:38:44.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.413931965827942
37757 2023-02-16,23:38:44.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4097927808761597
37862 2023-02-16,23:38:44.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3910167217254639
38108 2023-02-16,23:38:44.280 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3835058212280273
39647 2023-02-16,23:38:44.312 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37983 2023-02-16,23:38:44.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.363503098487854
36932 2023-02-16,23:38:44.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4029541015625
37152 2023-02-16,23:38:44.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4322993755340576
36814 2023-02-16,23:38:44.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.3591630458831787
37533 2023-02-16,23:38:44.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4065545797348022
37393 2023-02-16,23:38:44.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3967266082763672
38362 2023-02-16,23:38:44.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3539371490478516
38255 2023-02-16,23:38:44.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4125992059707642
38487 2023-02-16,23:38:44.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 266/15000, loss = 1.4031875133514404
37268 2023-02-16,23:38:44.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3532274961471558
37035 2023-02-16,23:38:44.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3831771612167358
37757 2023-02-16,23:38:44.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.408494472503662
38108 2023-02-16,23:38:44.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3759963512420654
37862 2023-02-16,23:38:44.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4033114910125732
37983 2023-02-16,23:38:44.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3562790155410767
36932 2023-02-16,23:38:44.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3675792217254639
36814 2023-02-16,23:38:44.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.3888483047485352
37533 2023-02-16,23:38:44.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.394029140472412
37152 2023-02-16,23:38:44.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4390770196914673
37393 2023-02-16,23:38:44.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3959251642227173
38362 2023-02-16,23:38:44.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4250034093856812
37268 2023-02-16,23:38:44.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.399377465248108
38255 2023-02-16,23:38:44.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3990463018417358
38487 2023-02-16,23:38:44.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 267/15000, loss = 1.3818182945251465
38108 2023-02-16,23:38:44.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.382523775100708
37035 2023-02-16,23:38:44.902 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.418232798576355
37757 2023-02-16,23:38:44.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.415968656539917
37862 2023-02-16,23:38:44.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.336961030960083
37983 2023-02-16,23:38:45.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3649792671203613
36932 2023-02-16,23:38:45.026 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.3156968355178833
37533 2023-02-16,23:38:45.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3736746311187744
36814 2023-02-16,23:38:45.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.3409004211425781
37152 2023-02-16,23:38:45.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4079010486602783
37393 2023-02-16,23:38:45.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3863219022750854
38362 2023-02-16,23:38:45.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3897007703781128
37268 2023-02-16,23:38:45.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3602410554885864
38255 2023-02-16,23:38:45.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4110785722732544
38487 2023-02-16,23:38:45.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 268/15000, loss = 1.393231749534607
38108 2023-02-16,23:38:45.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.429911732673645
37035 2023-02-16,23:38:45.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4071396589279175
37757 2023-02-16,23:38:45.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3921204805374146
37862 2023-02-16,23:38:45.237 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3740628957748413
37983 2023-02-16,23:38:45.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3600126504898071
37533 2023-02-16,23:38:45.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3555055856704712
36932 2023-02-16,23:38:45.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3986767530441284
36814 2023-02-16,23:38:45.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.39504873752594
37152 2023-02-16,23:38:45.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4338340759277344
37393 2023-02-16,23:38:45.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4073973894119263
38362 2023-02-16,23:38:45.420 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4011002779006958
37268 2023-02-16,23:38:45.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.378859519958496
38108 2023-02-16,23:38:45.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3799471855163574
38255 2023-02-16,23:38:45.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4114545583724976
38487 2023-02-16,23:38:45.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 269/15000, loss = 1.398115873336792
37035 2023-02-16,23:38:45.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4042975902557373
37757 2023-02-16,23:38:45.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.33231782913208
37862 2023-02-16,23:38:45.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3833789825439453
37983 2023-02-16,23:38:45.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3700788021087646
37533 2023-02-16,23:38:45.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3842217922210693
36932 2023-02-16,23:38:45.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3797154426574707
37152 2023-02-16,23:38:45.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.437507152557373
37393 2023-02-16,23:38:45.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.404374599456787
38362 2023-02-16,23:38:45.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.407020092010498
36814 2023-02-16,23:38:45.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.3900315761566162
38108 2023-02-16,23:38:45.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3370145559310913
37268 2023-02-16,23:38:45.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4409375190734863
38255 2023-02-16,23:38:45.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3443760871887207
38487 2023-02-16,23:38:45.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 270/15000, loss = 1.434395432472229
37757 2023-02-16,23:38:45.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3900604248046875
37035 2023-02-16,23:38:45.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3882508277893066
37862 2023-02-16,23:38:45.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3730716705322266
37983 2023-02-16,23:38:45.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3871383666992188
37533 2023-02-16,23:38:45.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4367287158966064
37152 2023-02-16,23:38:45.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.371199369430542
37393 2023-02-16,23:38:45.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.413828730583191
36932 2023-02-16,23:38:46.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4394550323486328
38362 2023-02-16,23:38:46.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.433109164237976
36814 2023-02-16,23:38:46.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.355631947517395
38108 2023-02-16,23:38:46.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3818007707595825
38255 2023-02-16,23:38:46.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.3785979747772217
38487 2023-02-16,23:38:46.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 271/15000, loss = 1.4016722440719604
37268 2023-02-16,23:38:46.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3790918588638306
37035 2023-02-16,23:38:46.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.410841941833496
37757 2023-02-16,23:38:46.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3906292915344238
37862 2023-02-16,23:38:46.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825328350067139
37983 2023-02-16,23:38:46.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4588377475738525
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37533 2023-02-16,23:38:46.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3718219995498657
37393 2023-02-16,23:38:46.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4119846820831299
36932 2023-02-16,23:38:46.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3118786811828613
37152 2023-02-16,23:38:46.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4035764932632446
36814 2023-02-16,23:38:46.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.3710989952087402
38362 2023-02-16,23:38:46.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.425536870956421
38108 2023-02-16,23:38:46.425 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4197629690170288
38487 2023-02-16,23:38:46.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 272/15000, loss = 1.385865330696106
38255 2023-02-16,23:38:46.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.373866319656372
37268 2023-02-16,23:38:46.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4117169380187988
37035 2023-02-16,23:38:46.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3875762224197388
37757 2023-02-16,23:38:46.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4258172512054443
37862 2023-02-16,23:38:46.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4124287366867065
37983 2023-02-16,23:38:46.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3986917734146118
37533 2023-02-16,23:38:46.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3932100534439087
37393 2023-02-16,23:38:46.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4241634607315063
37152 2023-02-16,23:38:46.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4213207960128784
36932 2023-02-16,23:38:46.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3362761735916138
36814 2023-02-16,23:38:46.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.393944501876831
38362 2023-02-16,23:38:46.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.370553970336914
38108 2023-02-16,23:38:46.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3977813720703125
38487 2023-02-16,23:38:46.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 273/15000, loss = 1.4460594654083252
38255 2023-02-16,23:38:46.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3458421230316162
37268 2023-02-16,23:38:46.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3877983093261719
37035 2023-02-16,23:38:46.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.332775592803955
37757 2023-02-16,23:38:46.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4031293392181396
37862 2023-02-16,23:38:46.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3561928272247314
37983 2023-02-16,23:38:46.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3972457647323608
37393 2023-02-16,23:38:46.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3782762289047241
37533 2023-02-16,23:38:46.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3257431983947754
37152 2023-02-16,23:38:46.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4171139001846313
36932 2023-02-16,23:38:46.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3682764768600464
38362 2023-02-16,23:38:47.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3922864198684692
36814 2023-02-16,23:38:47.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.4368150234222412
38108 2023-02-16,23:38:47.040 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3643317222595215
39803 2023-02-16,23:38:47.053 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4,5', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
38487 2023-02-16,23:38:47.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 274/15000, loss = 1.4122151136398315
38255 2023-02-16,23:38:47.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.400001883506775
37268 2023-02-16,23:38:47.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3867937326431274
37035 2023-02-16,23:38:47.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4083551168441772
37757 2023-02-16,23:38:47.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4097909927368164
37862 2023-02-16,23:38:47.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.45420241355896
37983 2023-02-16,23:38:47.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3409795761108398
37533 2023-02-16,23:38:47.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3727935552597046
37393 2023-02-16,23:38:47.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.407668948173523
37152 2023-02-16,23:38:47.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.434442400932312
38362 2023-02-16,23:38:47.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.387032389640808
36932 2023-02-16,23:38:47.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.339377522468567
38108 2023-02-16,23:38:47.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3735312223434448
36814 2023-02-16,23:38:47.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.428297519683838
38487 2023-02-16,23:38:47.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 275/15000, loss = 1.4313132762908936
38255 2023-02-16,23:38:47.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3887882232666016
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39438 2023-02-16,23:38:47.460 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37268 2023-02-16,23:38:47.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3794574737548828
39438 2023-02-16,23:38:47.498 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37862 2023-02-16,23:38:47.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3825995922088623
37035 2023-02-16,23:38:47.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.383235216140747
37533 2023-02-16,23:38:47.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4152352809906006
37757 2023-02-16,23:38:47.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3900071382522583
37983 2023-02-16,23:38:47.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4160035848617554
37393 2023-02-16,23:38:47.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.377690076828003
38362 2023-02-16,23:38:47.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.3506044149398804
37152 2023-02-16,23:38:47.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4016457796096802
38108 2023-02-16,23:38:47.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3975746631622314
36932 2023-02-16,23:38:47.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4046907424926758
36814 2023-02-16,23:38:47.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 805/15000, loss = 1.3882826566696167
38487 2023-02-16,23:38:47.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 276/15000, loss = 1.3983614444732666
38255 2023-02-16,23:38:47.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3492377996444702
37268 2023-02-16,23:38:47.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3853554725646973
37035 2023-02-16,23:38:47.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3756595849990845
37393 2023-02-16,23:38:47.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3746260404586792
37983 2023-02-16,23:38:47.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3838545083999634
37862 2023-02-16,23:38:47.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4071359634399414
37757 2023-02-16,23:38:47.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3825238943099976
37533 2023-02-16,23:38:47.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3455946445465088
38362 2023-02-16,23:38:47.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4121036529541016
37152 2023-02-16,23:38:47.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4202587604522705
38108 2023-02-16,23:38:47.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4260996580123901
36932 2023-02-16,23:38:47.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4213345050811768
38487 2023-02-16,23:38:48.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 277/15000, loss = 1.4532115459442139
36814 2023-02-16,23:38:48.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 806/15000, loss = 1.3830615282058716
38255 2023-02-16,23:38:48.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.35969078540802
37268 2023-02-16,23:38:48.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4391748905181885
37035 2023-02-16,23:38:48.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4133856296539307
37393 2023-02-16,23:38:48.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3798177242279053
37533 2023-02-16,23:38:48.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.375978946685791
37757 2023-02-16,23:38:48.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3498167991638184
37862 2023-02-16,23:38:48.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3952447175979614
37983 2023-02-16,23:38:48.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.412827491760254
38362 2023-02-16,23:38:48.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.4398390054702759
38108 2023-02-16,23:38:48.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.412989854812622
36932 2023-02-16,23:38:48.328 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.4172685146331787
37152 2023-02-16,23:38:48.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3966798782348633
38487 2023-02-16,23:38:48.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 278/15000, loss = 1.4391446113586426
36814 2023-02-16,23:38:48.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 807/15000, loss = 1.3719673156738281
38255 2023-02-16,23:38:48.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3680673837661743
37268 2023-02-16,23:38:48.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3796954154968262
37035 2023-02-16,23:38:48.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.365773320198059
37533 2023-02-16,23:38:48.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3799208402633667
37393 2023-02-16,23:38:48.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3255127668380737
37757 2023-02-16,23:38:48.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3817775249481201
37862 2023-02-16,23:38:48.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4255303144454956
37983 2023-02-16,23:38:48.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.354124665260315
38362 2023-02-16,23:38:48.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3884503841400146
38108 2023-02-16,23:38:48.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.432971477508545
37152 2023-02-16,23:38:48.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.454925537109375
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38487 2023-02-16,23:38:48.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 279/15000, loss = 1.4020228385925293
36932 2023-02-16,23:38:48.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3895888328552246
38255 2023-02-16,23:38:48.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4007859230041504
36814 2023-02-16,23:38:48.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 808/15000, loss = 1.4151698350906372
37268 2023-02-16,23:38:48.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3944613933563232
37035 2023-02-16,23:38:48.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3926844596862793
37533 2023-02-16,23:38:48.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3766329288482666
37393 2023-02-16,23:38:48.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3791639804840088
37983 2023-02-16,23:38:48.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3794530630111694
37757 2023-02-16,23:38:48.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4022217988967896
37862 2023-02-16,23:38:48.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.4002537727355957
38362 2023-02-16,23:38:48.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3519705533981323
38108 2023-02-16,23:38:48.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.358412742614746
37152 2023-02-16,23:38:48.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.352915644645691
38487 2023-02-16,23:38:49.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 280/15000, loss = 1.3929290771484375
36932 2023-02-16,23:38:49.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3584342002868652
38255 2023-02-16,23:38:49.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.3868801593780518
36814 2023-02-16,23:38:49.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 809/15000, loss = 1.4121302366256714
37268 2023-02-16,23:38:49.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3740476369857788
37035 2023-02-16,23:38:49.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3834024667739868
37533 2023-02-16,23:38:49.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4120800495147705
37393 2023-02-16,23:38:49.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3473058938980103
37983 2023-02-16,23:38:49.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3480370044708252
37757 2023-02-16,23:38:49.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621997833251953
37862 2023-02-16,23:38:49.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3967595100402832
38362 2023-02-16,23:38:49.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.412684679031372
38108 2023-02-16,23:38:49.280 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3214964866638184
38487 2023-02-16,23:38:49.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 281/15000, loss = 1.4076951742172241
37152 2023-02-16,23:38:49.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4169235229492188
36932 2023-02-16,23:38:49.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4043585062026978
38255 2023-02-16,23:38:49.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4563151597976685
36814 2023-02-16,23:38:49.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 810/15000, loss = 1.3889623880386353
37268 2023-02-16,23:38:49.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3735103607177734
37035 2023-02-16,23:38:49.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.386671543121338
37533 2023-02-16,23:38:49.445 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3806127309799194
37393 2023-02-16,23:38:49.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4008147716522217
37983 2023-02-16,23:38:49.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4008482694625854
37862 2023-02-16,23:38:49.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.423905849456787
37757 2023-02-16,23:38:49.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.396227240562439
38362 2023-02-16,23:38:49.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3990873098373413
38108 2023-02-16,23:38:49.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3792693614959717
38487 2023-02-16,23:38:49.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 282/15000, loss = 1.4131046533584595
37152 2023-02-16,23:38:49.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3758496046066284
38255 2023-02-16,23:38:49.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.406286597251892
36932 2023-02-16,23:38:49.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.3995583057403564
37268 2023-02-16,23:38:49.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3872679471969604
36814 2023-02-16,23:38:49.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 811/15000, loss = 1.2967207431793213
37035 2023-02-16,23:38:49.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.354461669921875
37533 2023-02-16,23:38:49.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4462040662765503
37393 2023-02-16,23:38:49.783 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4198848009109497
37983 2023-02-16,23:38:49.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3740648031234741
37862 2023-02-16,23:38:49.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3877471685409546
38362 2023-02-16,23:38:49.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4112861156463623
37757 2023-02-16,23:38:49.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3787596225738525
38108 2023-02-16,23:38:49.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3312005996704102
38487 2023-02-16,23:38:49.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 283/15000, loss = 1.3757933378219604
37152 2023-02-16,23:38:49.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3173415660858154
38255 2023-02-16,23:38:49.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3848752975463867
37268 2023-02-16,23:38:49.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3904173374176025
36932 2023-02-16,23:38:50.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.349534273147583
36814 2023-02-16,23:38:50.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 812/15000, loss = 1.3425495624542236
37035 2023-02-16,23:38:50.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4252293109893799
37533 2023-02-16,23:38:50.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4444197416305542
37393 2023-02-16,23:38:50.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4107023477554321
37983 2023-02-16,23:38:50.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4616667032241821
37862 2023-02-16,23:38:50.127 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.383989691734314
38362 2023-02-16,23:38:50.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.41152822971344
37757 2023-02-16,23:38:50.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3941853046417236
38108 2023-02-16,23:38:50.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3860695362091064
38487 2023-02-16,23:38:50.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 284/15000, loss = 1.3890907764434814
39928 2023-02-16,23:38:50.281 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
38255 2023-02-16,23:38:50.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3923486471176147
37152 2023-02-16,23:38:50.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3641176223754883
37268 2023-02-16,23:38:50.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3827898502349854
36932 2023-02-16,23:38:50.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3738948106765747
36814 2023-02-16,23:38:50.380 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 813/15000, loss = 1.38309645652771
37533 2023-02-16,23:38:50.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3446851968765259
37035 2023-02-16,23:38:50.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4039216041564941
37393 2023-02-16,23:38:50.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3942396640777588
37862 2023-02-16,23:38:50.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4233993291854858
37983 2023-02-16,23:38:50.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3471283912658691
38362 2023-02-16,23:38:50.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3443443775177002
37757 2023-02-16,23:38:50.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4193922281265259
38108 2023-02-16,23:38:50.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4066627025604248
38487 2023-02-16,23:38:50.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 285/15000, loss = 1.373966932296753
37152 2023-02-16,23:38:50.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4379910230636597
38255 2023-02-16,23:38:50.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3387738466262817
37268 2023-02-16,23:38:50.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3620188236236572
36932 2023-02-16,23:38:50.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.371915578842163
36814 2023-02-16,23:38:50.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 814/15000, loss = 1.3740644454956055
37393 2023-02-16,23:38:50.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3941255807876587
37533 2023-02-16,23:38:50.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.406129002571106
37035 2023-02-16,23:38:50.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.363558053970337
37983 2023-02-16,23:38:50.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.40788996219635
37862 2023-02-16,23:38:50.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4107489585876465
38362 2023-02-16,23:38:50.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.378661036491394
37757 2023-02-16,23:38:50.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4016573429107666
38108 2023-02-16,23:38:50.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.390045166015625
38487 2023-02-16,23:38:50.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 286/15000, loss = 1.394729495048523
37152 2023-02-16,23:38:50.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3946589231491089
38255 2023-02-16,23:38:50.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3769570589065552
37268 2023-02-16,23:38:50.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3672325611114502
36932 2023-02-16,23:38:51.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3798717260360718
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37533 2023-02-16,23:38:51.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4691346883773804
39532 2023-02-16,23:38:51.030 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
36814 2023-02-16,23:38:51.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 815/15000, loss = 1.412057876586914
37393 2023-02-16,23:38:51.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3914296627044678
37035 2023-02-16,23:38:51.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4175055027008057
39532 2023-02-16,23:38:51.085 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37983 2023-02-16,23:38:51.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4023046493530273
37862 2023-02-16,23:38:51.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4081692695617676
38362 2023-02-16,23:38:51.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3739547729492188
38108 2023-02-16,23:38:51.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3848748207092285
37757 2023-02-16,23:38:51.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3918302059173584
38487 2023-02-16,23:38:51.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 287/15000, loss = 1.4042186737060547
38255 2023-02-16,23:38:51.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098358154296875
37152 2023-02-16,23:38:51.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.342720866203308
37268 2023-02-16,23:38:51.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3495157957077026
37533 2023-02-16,23:38:51.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3880902528762817
36932 2023-02-16,23:38:51.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.3996174335479736
37393 2023-02-16,23:38:51.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4440724849700928
36814 2023-02-16,23:38:51.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 816/15000, loss = 1.4217742681503296
37035 2023-02-16,23:38:51.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3874608278274536
37983 2023-02-16,23:38:51.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3910237550735474
38362 2023-02-16,23:38:51.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3457390069961548
37862 2023-02-16,23:38:51.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4024701118469238
38108 2023-02-16,23:38:51.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.363544225692749
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37757 2023-02-16,23:38:51.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3990026712417603
38255 2023-02-16,23:38:51.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3810889720916748
38487 2023-02-16,23:38:51.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 288/15000, loss = 1.3720695972442627
37152 2023-02-16,23:38:51.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.392561435699463
37268 2023-02-16,23:38:51.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4118435382843018
37533 2023-02-16,23:38:51.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3807716369628906
36932 2023-02-16,23:38:51.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4568605422973633
37393 2023-02-16,23:38:51.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.375332236289978
36814 2023-02-16,23:38:51.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 817/15000, loss = 1.4178396463394165
37983 2023-02-16,23:38:51.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4033915996551514
38362 2023-02-16,23:38:51.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.4000579118728638
37035 2023-02-16,23:38:51.739 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4784046411514282
37862 2023-02-16,23:38:51.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4187554121017456
38108 2023-02-16,23:38:51.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3563029766082764
38255 2023-02-16,23:38:51.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3834798336029053
38487 2023-02-16,23:38:51.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 289/15000, loss = 1.3959741592407227
37152 2023-02-16,23:38:51.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.388878345489502
37757 2023-02-16,23:38:51.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3857682943344116
37268 2023-02-16,23:38:51.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.380002737045288
37533 2023-02-16,23:38:51.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4011008739471436
37393 2023-02-16,23:38:51.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3532192707061768
36932 2023-02-16,23:38:52.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.3432888984680176
37983 2023-02-16,23:38:52.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3369239568710327
38362 2023-02-16,23:38:52.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.388816475868225
36814 2023-02-16,23:38:52.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 818/15000, loss = 1.386553168296814
37035 2023-02-16,23:38:52.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3669942617416382
37862 2023-02-16,23:38:52.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3663835525512695
38255 2023-02-16,23:38:52.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3758506774902344
38487 2023-02-16,23:38:52.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 290/15000, loss = 1.3606809377670288
38108 2023-02-16,23:38:52.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.365051031112671
37152 2023-02-16,23:38:52.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3822505474090576
37757 2023-02-16,23:38:52.219 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3712289333343506
37268 2023-02-16,23:38:52.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4351894855499268
37533 2023-02-16,23:38:52.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3746447563171387
37393 2023-02-16,23:38:52.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.401473879814148
36932 2023-02-16,23:38:52.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3847967386245728
37983 2023-02-16,23:38:52.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3741124868392944
38362 2023-02-16,23:38:52.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.349328875541687
36814 2023-02-16,23:38:52.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 819/15000, loss = 1.4180238246917725
37035 2023-02-16,23:38:52.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.378340482711792
37862 2023-02-16,23:38:52.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4098995923995972
38255 2023-02-16,23:38:52.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3825132846832275
37152 2023-02-16,23:38:52.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.413487434387207
38108 2023-02-16,23:38:52.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.360052227973938
38487 2023-02-16,23:38:52.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 291/15000, loss = 1.3874213695526123
37757 2023-02-16,23:38:52.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.406916856765747
37268 2023-02-16,23:38:52.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4324207305908203
37533 2023-02-16,23:38:52.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3932881355285645
37393 2023-02-16,23:38:52.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3597925901412964
36932 2023-02-16,23:38:52.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.396121859550476
37983 2023-02-16,23:38:52.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.383384346961975
38362 2023-02-16,23:38:52.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3597795963287354
37035 2023-02-16,23:38:52.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.3720340728759766
36814 2023-02-16,23:38:52.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 820/15000, loss = 1.3852134943008423
37862 2023-02-16,23:38:52.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4085224866867065
38255 2023-02-16,23:38:52.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4298731088638306
37152 2023-02-16,23:38:52.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4097830057144165
38108 2023-02-16,23:38:52.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3701545000076294
38487 2023-02-16,23:38:52.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 292/15000, loss = 1.4183595180511475
37268 2023-02-16,23:38:52.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.429896354675293
37533 2023-02-16,23:38:52.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4179205894470215
37757 2023-02-16,23:38:52.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3938654661178589
37393 2023-02-16,23:38:52.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3780533075332642
38362 2023-02-16,23:38:53.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3680994510650635
36932 2023-02-16,23:38:53.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4047516584396362
37035 2023-02-16,23:38:53.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4466147422790527
37983 2023-02-16,23:38:53.033 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3730337619781494
36814 2023-02-16,23:38:53.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 821/15000, loss = 1.3598631620407104
37862 2023-02-16,23:38:53.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4159387350082397
38255 2023-02-16,23:38:53.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3799827098846436
38108 2023-02-16,23:38:53.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3871077299118042
38487 2023-02-16,23:38:53.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 293/15000, loss = 1.4200537204742432
37152 2023-02-16,23:38:53.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4097181558609009
37533 2023-02-16,23:38:53.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4146857261657715
37268 2023-02-16,23:38:53.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.366064429283142
37757 2023-02-16,23:38:53.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3739910125732422
37393 2023-02-16,23:38:53.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4412788152694702
37035 2023-02-16,23:38:53.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3733162879943848
37983 2023-02-16,23:38:53.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825693130493164
38362 2023-02-16,23:38:53.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4008893966674805
36932 2023-02-16,23:38:53.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.3813245296478271
36814 2023-02-16,23:38:53.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 822/15000, loss = 1.4137049913406372
37862 2023-02-16,23:38:53.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3921784162521362
38255 2023-02-16,23:38:53.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3369368314743042
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39647 2023-02-16,23:38:53.448 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
38108 2023-02-16,23:38:53.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4588367938995361
37152 2023-02-16,23:38:53.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4186851978302002
38487 2023-02-16,23:38:53.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 294/15000, loss = 1.3539466857910156
39647 2023-02-16,23:38:53.503 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37268 2023-02-16,23:38:53.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3766953945159912
37533 2023-02-16,23:38:53.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3806185722351074
37757 2023-02-16,23:38:53.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3548794984817505
37393 2023-02-16,23:38:53.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3794156312942505
40058 2023-02-16,23:38:53.640 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37035 2023-02-16,23:38:53.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.381337285041809
37983 2023-02-16,23:38:53.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4124619960784912
38362 2023-02-16,23:38:53.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.386816143989563
36814 2023-02-16,23:38:53.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 823/15000, loss = 1.43183171749115
36932 2023-02-16,23:38:53.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3651143312454224
37862 2023-02-16,23:38:53.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.332289457321167
38255 2023-02-16,23:38:53.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3818418979644775
37152 2023-02-16,23:38:53.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4227102994918823
38108 2023-02-16,23:38:53.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3985968828201294
38487 2023-02-16,23:38:53.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 295/15000, loss = 1.4249908924102783
37268 2023-02-16,23:38:53.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4183568954467773
37533 2023-02-16,23:38:53.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4076546430587769
37757 2023-02-16,23:38:53.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3842439651489258
37393 2023-02-16,23:38:53.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4106218814849854
38362 2023-02-16,23:38:53.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4563195705413818
37983 2023-02-16,23:38:54.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3562116622924805
37035 2023-02-16,23:38:54.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4022225141525269
37862 2023-02-16,23:38:54.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3899933099746704
38255 2023-02-16,23:38:54.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.4196354150772095
36814 2023-02-16,23:38:54.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 824/15000, loss = 1.4100732803344727
36932 2023-02-16,23:38:54.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4246283769607544
37152 2023-02-16,23:38:54.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4011647701263428
38108 2023-02-16,23:38:54.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3971812725067139
38487 2023-02-16,23:38:54.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 296/15000, loss = 1.3896528482437134
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37268 2023-02-16,23:38:54.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3466920852661133
37533 2023-02-16,23:38:54.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4622828960418701
37757 2023-02-16,23:38:54.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.439415454864502
37393 2023-02-16,23:38:54.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.388296127319336
38362 2023-02-16,23:38:54.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4063340425491333
37983 2023-02-16,23:38:54.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4544107913970947
37035 2023-02-16,23:38:54.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3696351051330566
37862 2023-02-16,23:38:54.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3907521963119507
38255 2023-02-16,23:38:54.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3978791236877441
36932 2023-02-16,23:38:54.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4169197082519531
36814 2023-02-16,23:38:54.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 825/15000, loss = 1.4010038375854492
37152 2023-02-16,23:38:54.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4046833515167236
38108 2023-02-16,23:38:54.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410640954971313
38487 2023-02-16,23:38:54.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 297/15000, loss = 1.4011194705963135
37268 2023-02-16,23:38:54.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3320915699005127
37533 2023-02-16,23:38:54.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4367179870605469
37393 2023-02-16,23:38:54.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.387279987335205
37757 2023-02-16,23:38:54.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3717797994613647
38362 2023-02-16,23:38:54.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849453926086426
37983 2023-02-16,23:38:54.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3826062679290771
37035 2023-02-16,23:38:54.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.317170262336731
37862 2023-02-16,23:38:54.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4260190725326538
38255 2023-02-16,23:38:54.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3643171787261963
36932 2023-02-16,23:38:54.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.4083236455917358
36814 2023-02-16,23:38:54.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 826/15000, loss = 1.4115973711013794
37152 2023-02-16,23:38:54.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.366471290588379
37533 2023-02-16,23:38:54.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3950934410095215
38108 2023-02-16,23:38:54.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4159520864486694
38487 2023-02-16,23:38:54.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 298/15000, loss = 1.4071179628372192
37268 2023-02-16,23:38:54.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4152604341506958
37393 2023-02-16,23:38:54.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3796745538711548
37757 2023-02-16,23:38:54.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3939356803894043
38362 2023-02-16,23:38:54.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3922994136810303
37983 2023-02-16,23:38:54.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.407211422920227
37035 2023-02-16,23:38:55.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.4001693725585938
37862 2023-02-16,23:38:55.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.403221845626831
38255 2023-02-16,23:38:55.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3734617233276367
36932 2023-02-16,23:38:55.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3719890117645264
36814 2023-02-16,23:38:55.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 827/15000, loss = 1.395861029624939
37152 2023-02-16,23:38:55.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4180043935775757
37533 2023-02-16,23:38:55.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3670340776443481
38108 2023-02-16,23:38:55.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3838984966278076
38487 2023-02-16,23:38:55.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 299/15000, loss = 1.4331603050231934
37268 2023-02-16,23:38:55.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4003795385360718
37393 2023-02-16,23:38:55.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.386152744293213
37757 2023-02-16,23:38:55.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3244547843933105
38362 2023-02-16,23:38:55.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3387866020202637
37983 2023-02-16,23:38:55.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3953044414520264
37862 2023-02-16,23:38:55.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.409726858139038
38255 2023-02-16,23:38:55.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3976538181304932
37035 2023-02-16,23:38:55.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3792550563812256
36932 2023-02-16,23:38:55.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.327933669090271
37533 2023-02-16,23:38:55.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3551191091537476
36814 2023-02-16,23:38:55.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 828/15000, loss = 1.4060062170028687
37152 2023-02-16,23:38:55.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.43430757522583
38108 2023-02-16,23:38:55.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4128397703170776
38487 2023-02-16,23:38:55.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 300/15000, loss = 1.4254968166351318
37268 2023-02-16,23:38:55.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3723244667053223
37393 2023-02-16,23:38:55.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4396369457244873
37757 2023-02-16,23:38:55.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.372654676437378
38362 2023-02-16,23:38:55.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3768730163574219
37983 2023-02-16,23:38:55.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.425616979598999
37862 2023-02-16,23:38:55.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3901004791259766
38255 2023-02-16,23:38:55.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4260247945785522
37035 2023-02-16,23:38:55.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4380762577056885
36932 2023-02-16,23:38:55.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.363219976425171
37533 2023-02-16,23:38:55.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3910136222839355
36814 2023-02-16,23:38:55.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 829/15000, loss = 1.396976351737976
37152 2023-02-16,23:38:55.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4181562662124634
38108 2023-02-16,23:38:55.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3541593551635742
38487 2023-02-16,23:38:55.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 301/15000, loss = 1.3705662488937378
37268 2023-02-16,23:38:55.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4163072109222412
37393 2023-02-16,23:38:55.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3808757066726685
38362 2023-02-16,23:38:55.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098951816558838
37757 2023-02-16,23:38:55.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4171993732452393
37983 2023-02-16,23:38:55.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.400274395942688
37862 2023-02-16,23:38:55.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.382373332977295
38255 2023-02-16,23:38:55.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4130504131317139
37035 2023-02-16,23:38:55.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3140268325805664
37533 2023-02-16,23:38:56.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.332208514213562
36932 2023-02-16,23:38:56.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.373197078704834
37152 2023-02-16,23:38:56.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4088488817214966
38108 2023-02-16,23:38:56.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.379391074180603
36814 2023-02-16,23:38:56.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 830/15000, loss = 1.4150159358978271
38487 2023-02-16,23:38:56.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 302/15000, loss = 1.3922303915023804
37268 2023-02-16,23:38:56.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4324063062667847
37393 2023-02-16,23:38:56.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3952759504318237
38362 2023-02-16,23:38:56.198 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.3810641765594482
37757 2023-02-16,23:38:56.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3446943759918213
37983 2023-02-16,23:38:56.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3966987133026123
37862 2023-02-16,23:38:56.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3498867750167847
38255 2023-02-16,23:38:56.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4329898357391357
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39803 2023-02-16,23:38:56.289 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37035 2023-02-16,23:38:56.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3378747701644897
37533 2023-02-16,23:38:56.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3724979162216187
39803 2023-02-16,23:38:56.382 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36932 2023-02-16,23:38:56.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.3949135541915894
37152 2023-02-16,23:38:56.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3788058757781982
38108 2023-02-16,23:38:56.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.348063588142395
36814 2023-02-16,23:38:56.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 831/15000, loss = 1.3693712949752808
38487 2023-02-16,23:38:56.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 303/15000, loss = 1.3870810270309448
37268 2023-02-16,23:38:56.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3805288076400757
37393 2023-02-16,23:38:56.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.373837947845459
40186 2023-02-16,23:38:56.534 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
38362 2023-02-16,23:38:56.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3835246562957764
37757 2023-02-16,23:38:56.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3762463331222534
37983 2023-02-16,23:38:56.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4240264892578125
37862 2023-02-16,23:38:56.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3816981315612793
38255 2023-02-16,23:38:56.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3584622144699097
37035 2023-02-16,23:38:56.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3686261177062988
37533 2023-02-16,23:38:56.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3631272315979004
36932 2023-02-16,23:38:56.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3618173599243164
37152 2023-02-16,23:38:56.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3880864381790161
36814 2023-02-16,23:38:56.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 832/15000, loss = 1.4427845478057861
38108 2023-02-16,23:38:56.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.400820255279541
38487 2023-02-16,23:38:56.829 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 304/15000, loss = 1.350630283355713
37268 2023-02-16,23:38:56.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3601605892181396
37393 2023-02-16,23:38:56.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3736144304275513
38362 2023-02-16,23:38:56.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3758963346481323
37983 2023-02-16,23:38:56.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3876872062683105
37757 2023-02-16,23:38:56.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3798279762268066
37862 2023-02-16,23:38:56.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4023158550262451
38255 2023-02-16,23:38:56.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.321479082107544
37035 2023-02-16,23:38:56.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3397005796432495
37533 2023-02-16,23:38:56.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3714230060577393
36932 2023-02-16,23:38:57.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3588720560073853
37152 2023-02-16,23:38:57.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3806630373001099
36814 2023-02-16,23:38:57.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 833/15000, loss = 1.3568613529205322
38108 2023-02-16,23:38:57.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3740863800048828
38487 2023-02-16,23:38:57.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 305/15000, loss = 1.4120844602584839
37268 2023-02-16,23:38:57.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4222606420516968
38362 2023-02-16,23:38:57.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3825570344924927
37393 2023-02-16,23:38:57.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3881922960281372
37757 2023-02-16,23:38:57.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3765192031860352
37983 2023-02-16,23:38:57.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3841004371643066
37862 2023-02-16,23:38:57.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621735572814941
38255 2023-02-16,23:38:57.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3792998790740967
37035 2023-02-16,23:38:57.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4029992818832397
37533 2023-02-16,23:38:57.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.444859266281128
37152 2023-02-16,23:38:57.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3705546855926514
36932 2023-02-16,23:38:57.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.394171953201294
38108 2023-02-16,23:38:57.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4615163803100586
36814 2023-02-16,23:38:57.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 834/15000, loss = 1.3716676235198975
37268 2023-02-16,23:38:57.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.391871690750122
38362 2023-02-16,23:38:57.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4299209117889404
38487 2023-02-16,23:38:57.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 306/15000, loss = 1.439786672592163
37393 2023-02-16,23:38:57.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3911012411117554
37757 2023-02-16,23:38:57.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4135228395462036
37983 2023-02-16,23:38:57.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4235434532165527
37862 2023-02-16,23:38:57.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.3962247371673584
38255 2023-02-16,23:38:57.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3312045335769653
37035 2023-02-16,23:38:57.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4200481176376343
37533 2023-02-16,23:38:57.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3978041410446167
37152 2023-02-16,23:38:57.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.42252516746521
36932 2023-02-16,23:38:57.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.358614206314087
38108 2023-02-16,23:38:57.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3470977544784546
36814 2023-02-16,23:38:57.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 835/15000, loss = 1.3627204895019531
37268 2023-02-16,23:38:57.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.385549783706665
38487 2023-02-16,23:38:57.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 307/15000, loss = 1.3884758949279785
38362 2023-02-16,23:38:57.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3799669742584229
37393 2023-02-16,23:38:57.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3835316896438599
37757 2023-02-16,23:38:57.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3809661865234375
37983 2023-02-16,23:38:57.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4109032154083252
37862 2023-02-16,23:38:57.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3788315057754517
37533 2023-02-16,23:38:57.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4190750122070312
38255 2023-02-16,23:38:57.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.386206865310669
37035 2023-02-16,23:38:57.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.414567470550537
36932 2023-02-16,23:38:58.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.3612020015716553
37152 2023-02-16,23:38:58.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3964569568634033
38108 2023-02-16,23:38:58.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4079099893569946
37268 2023-02-16,23:38:58.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.321253776550293
36814 2023-02-16,23:38:58.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 836/15000, loss = 1.4309427738189697
38487 2023-02-16,23:38:58.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 308/15000, loss = 1.3520350456237793
38362 2023-02-16,23:38:58.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3370158672332764
37393 2023-02-16,23:38:58.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3627331256866455
37757 2023-02-16,23:38:58.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4487779140472412
37983 2023-02-16,23:38:58.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4082754850387573
37862 2023-02-16,23:38:58.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3942266702651978
37533 2023-02-16,23:38:58.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.376732349395752
38255 2023-02-16,23:38:58.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4065494537353516
37035 2023-02-16,23:38:58.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3903440237045288
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36932 2023-02-16,23:38:58.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.3888700008392334
37152 2023-02-16,23:38:58.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.400976300239563
38108 2023-02-16,23:38:58.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4023367166519165
37268 2023-02-16,23:38:58.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4086592197418213
38487 2023-02-16,23:38:58.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 309/15000, loss = 1.4126521348953247
36814 2023-02-16,23:38:58.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 837/15000, loss = 1.341918706893921
38362 2023-02-16,23:38:58.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.381860613822937
37393 2023-02-16,23:38:58.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3673455715179443
37757 2023-02-16,23:38:58.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4462226629257202
37533 2023-02-16,23:38:58.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3670860528945923
37862 2023-02-16,23:38:58.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4192981719970703
37983 2023-02-16,23:38:58.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4025052785873413
38255 2023-02-16,23:38:58.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3898946046829224
37035 2023-02-16,23:38:58.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3582494258880615
36932 2023-02-16,23:38:58.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3775923252105713
37152 2023-02-16,23:38:58.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3660317659378052
38108 2023-02-16,23:38:58.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3909873962402344
37268 2023-02-16,23:38:58.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3768874406814575
38487 2023-02-16,23:38:58.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 310/15000, loss = 1.3990695476531982
36814 2023-02-16,23:38:58.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 838/15000, loss = 1.3615914583206177
38362 2023-02-16,23:38:58.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.41972815990448
37393 2023-02-16,23:38:58.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.350024938583374
37757 2023-02-16,23:38:58.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3448251485824585
37533 2023-02-16,23:38:58.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3881500959396362
37862 2023-02-16,23:38:58.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4016103744506836
37983 2023-02-16,23:38:58.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4188380241394043
38255 2023-02-16,23:38:58.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.384911060333252
37035 2023-02-16,23:38:58.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.402583122253418
36932 2023-02-16,23:38:59.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4046123027801514
37152 2023-02-16,23:38:59.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3710057735443115
38108 2023-02-16,23:38:59.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4033963680267334
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)39928 2023-02-16,23:38:59.040 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37268 2023-02-16,23:38:59.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4118726253509521
38487 2023-02-16,23:38:59.078 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 311/15000, loss = 1.4112204313278198
39928 2023-02-16,23:38:59.101 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:38:59.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 839/15000, loss = 1.4087871313095093
38362 2023-02-16,23:38:59.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3978750705718994
37393 2023-02-16,23:38:59.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4120242595672607
37533 2023-02-16,23:38:59.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3623130321502686
37757 2023-02-16,23:38:59.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4068001508712769
37862 2023-02-16,23:38:59.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3919402360916138
38255 2023-02-16,23:38:59.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3634943962097168
37983 2023-02-16,23:38:59.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3663480281829834
37035 2023-02-16,23:38:59.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.39750337600708
38108 2023-02-16,23:38:59.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.336957335472107
36932 2023-02-16,23:38:59.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.4040716886520386
37152 2023-02-16,23:38:59.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.380055546760559
38487 2023-02-16,23:38:59.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 312/15000, loss = 1.4115164279937744
37268 2023-02-16,23:38:59.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3828271627426147
36814 2023-02-16,23:38:59.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 840/15000, loss = 1.4117871522903442
37393 2023-02-16,23:38:59.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3811187744140625
38362 2023-02-16,23:38:59.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.364358901977539
37862 2023-02-16,23:38:59.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3990209102630615
37533 2023-02-16,23:38:59.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.372353196144104
37757 2023-02-16,23:38:59.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4708856344223022
37983 2023-02-16,23:38:59.570 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4099600315093994
38255 2023-02-16,23:38:59.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3562395572662354
37035 2023-02-16,23:38:59.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3496540784835815
37152 2023-02-16,23:38:59.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3965449333190918
38108 2023-02-16,23:38:59.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3741353750228882
36932 2023-02-16,23:38:59.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.4067878723144531
37268 2023-02-16,23:38:59.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3605718612670898
38487 2023-02-16,23:38:59.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 313/15000, loss = 1.3444123268127441
36814 2023-02-16,23:38:59.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 841/15000, loss = 1.3908827304840088
37393 2023-02-16,23:38:59.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4364286661148071
38362 2023-02-16,23:38:59.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3735417127609253
40293 2023-02-16,23:38:59.846 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37862 2023-02-16,23:38:59.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3857895135879517
38255 2023-02-16,23:38:59.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3648418188095093
37757 2023-02-16,23:38:59.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3876287937164307
37983 2023-02-16,23:38:59.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4084358215332031
37533 2023-02-16,23:38:59.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3559768199920654
37035 2023-02-16,23:38:59.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3737592697143555
37152 2023-02-16,23:38:59.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.459269642829895
38108 2023-02-16,23:38:59.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3833556175231934
36932 2023-02-16,23:39:00.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.3487186431884766
38487 2023-02-16,23:39:00.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 314/15000, loss = 1.378675103187561
37268 2023-02-16,23:39:00.058 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4264695644378662
37393 2023-02-16,23:39:00.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.432407021522522
36814 2023-02-16,23:39:00.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 842/15000, loss = 1.3830301761627197
38362 2023-02-16,23:39:00.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3976495265960693
37533 2023-02-16,23:39:00.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372251033782959
37862 2023-02-16,23:39:00.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3712679147720337
38255 2023-02-16,23:39:00.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.360033392906189
37757 2023-02-16,23:39:00.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3815332651138306
37983 2023-02-16,23:39:00.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4158298969268799
37035 2023-02-16,23:39:00.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.3717974424362183
37152 2023-02-16,23:39:00.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4071217775344849
38108 2023-02-16,23:39:00.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3730829954147339
36932 2023-02-16,23:39:00.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4024802446365356
38487 2023-02-16,23:39:00.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 315/15000, loss = 1.3739640712738037
37268 2023-02-16,23:39:00.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4211934804916382
37393 2023-02-16,23:39:00.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4303582906723022
36814 2023-02-16,23:39:00.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 843/15000, loss = 1.3499948978424072
38362 2023-02-16,23:39:00.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4261362552642822
37533 2023-02-16,23:39:00.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3618419170379639
38255 2023-02-16,23:39:00.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.37005615234375
39095 2023-02-16,23:39:00.574 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
39095 2023-02-16,23:39:00.576 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
39095 2023-02-16,23:39:00.577 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e']
39095 2023-02-16,23:39:00.578 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 42530308}
37757 2023-02-16,23:39:00.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4014256000518799
37862 2023-02-16,23:39:00.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4069305658340454
37983 2023-02-16,23:39:00.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.392004370689392
37035 2023-02-16,23:39:00.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.380692481994629
37152 2023-02-16,23:39:00.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.409441590309143
38108 2023-02-16,23:39:00.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825139999389648
38487 2023-02-16,23:39:00.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 316/15000, loss = 1.3458524942398071
36932 2023-02-16,23:39:00.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.3576358556747437
37268 2023-02-16,23:39:00.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4101723432540894
37393 2023-02-16,23:39:00.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3675471544265747
36814 2023-02-16,23:39:00.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 844/15000, loss = 1.3768494129180908
38362 2023-02-16,23:39:00.795 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4130569696426392
37533 2023-02-16,23:39:00.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.411148190498352
38255 2023-02-16,23:39:00.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.387118935585022
37757 2023-02-16,23:39:00.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3743128776550293
37862 2023-02-16,23:39:00.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3939176797866821
37983 2023-02-16,23:39:00.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3322306871414185
37035 2023-02-16,23:39:00.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.3987977504730225
37152 2023-02-16,23:39:00.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3733749389648438
38108 2023-02-16,23:39:00.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4123544692993164
38487 2023-02-16,23:39:01.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 317/15000, loss = 1.4000144004821777
36932 2023-02-16,23:39:01.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.4180949926376343
37393 2023-02-16,23:39:01.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3780795335769653
37268 2023-02-16,23:39:01.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4521158933639526
38362 2023-02-16,23:39:01.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4330439567565918
36814 2023-02-16,23:39:01.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 845/15000, loss = 1.3629109859466553
37533 2023-02-16,23:39:01.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4198733568191528
38255 2023-02-16,23:39:01.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4586987495422363
37983 2023-02-16,23:39:01.237 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3900984525680542
37862 2023-02-16,23:39:01.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.373905897140503
37757 2023-02-16,23:39:01.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3937451839447021
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37035 2023-02-16,23:39:01.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4566895961761475
37152 2023-02-16,23:39:01.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3729792833328247
38108 2023-02-16,23:39:01.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3561863899230957
37393 2023-02-16,23:39:01.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.419829249382019
38487 2023-02-16,23:39:01.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 318/15000, loss = 1.3888217210769653
36932 2023-02-16,23:39:01.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.363855242729187
37268 2023-02-16,23:39:01.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.404421329498291
38362 2023-02-16,23:39:01.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3584215641021729
36814 2023-02-16,23:39:01.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 846/15000, loss = 1.430632472038269
37533 2023-02-16,23:39:01.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.442776083946228
38255 2023-02-16,23:39:01.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3985860347747803
37983 2023-02-16,23:39:01.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3907445669174194
37757 2023-02-16,23:39:01.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4186996221542358
37862 2023-02-16,23:39:01.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3550236225128174
37035 2023-02-16,23:39:01.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.342397928237915
37152 2023-02-16,23:39:01.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.406501054763794
38108 2023-02-16,23:39:01.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4543235301971436
38487 2023-02-16,23:39:01.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 319/15000, loss = 1.3493571281433105
37393 2023-02-16,23:39:01.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3469200134277344
36932 2023-02-16,23:39:01.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.3776054382324219
37268 2023-02-16,23:39:01.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3915624618530273
38362 2023-02-16,23:39:01.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3214794397354126
36814 2023-02-16,23:39:01.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 847/15000, loss = 1.4217758178710938
37533 2023-02-16,23:39:01.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3690394163131714
38255 2023-02-16,23:39:01.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3973450660705566
37983 2023-02-16,23:39:01.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.426091194152832
37757 2023-02-16,23:39:01.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4158713817596436
37862 2023-02-16,23:39:01.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.384260654449463
38487 2023-02-16,23:39:01.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 320/15000, loss = 1.3597548007965088
37035 2023-02-16,23:39:01.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3856995105743408
37152 2023-02-16,23:39:01.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3974521160125732
38108 2023-02-16,23:39:02.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.382569670677185
37393 2023-02-16,23:39:02.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3327662944793701
36932 2023-02-16,23:39:02.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.385172724723816
37268 2023-02-16,23:39:02.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4014203548431396
38362 2023-02-16,23:39:02.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.379319429397583
36814 2023-02-16,23:39:02.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 848/15000, loss = 1.4090559482574463
37533 2023-02-16,23:39:02.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3973097801208496
38255 2023-02-16,23:39:02.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410438299179077
37983 2023-02-16,23:39:02.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4031699895858765
37862 2023-02-16,23:39:02.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.439543604850769
37757 2023-02-16,23:39:02.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3807337284088135
37035 2023-02-16,23:39:02.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3968015909194946
37152 2023-02-16,23:39:02.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3801149129867554
38487 2023-02-16,23:39:02.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 321/15000, loss = 1.3681385517120361
37393 2023-02-16,23:39:02.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4157428741455078
38108 2023-02-16,23:39:02.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4071712493896484
38362 2023-02-16,23:39:02.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3312222957611084
36932 2023-02-16,23:39:02.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.437673568725586
37268 2023-02-16,23:39:02.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4060808420181274
37533 2023-02-16,23:39:02.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4126206636428833
36814 2023-02-16,23:39:02.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 849/15000, loss = 1.4025225639343262
38255 2023-02-16,23:39:02.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4159573316574097
37983 2023-02-16,23:39:02.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.409787654876709
37862 2023-02-16,23:39:02.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.371798038482666
37757 2023-02-16,23:39:02.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4084675312042236
37035 2023-02-16,23:39:02.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4044209718704224
38487 2023-02-16,23:39:02.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 322/15000, loss = 1.4008004665374756
37152 2023-02-16,23:39:02.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4287058115005493
37393 2023-02-16,23:39:02.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.3994184732437134
38108 2023-02-16,23:39:02.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3953524827957153
36932 2023-02-16,23:39:02.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.3441485166549683
38362 2023-02-16,23:39:02.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3861262798309326
37533 2023-02-16,23:39:02.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4060267210006714
37268 2023-02-16,23:39:02.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.402234673500061
40433 2023-02-16,23:39:02.740 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36814 2023-02-16,23:39:02.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 850/15000, loss = 1.3934322595596313
38255 2023-02-16,23:39:02.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3837316036224365
37983 2023-02-16,23:39:02.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3900049924850464
37862 2023-02-16,23:39:02.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.393896460533142
37757 2023-02-16,23:39:02.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4648127555847168
37035 2023-02-16,23:39:02.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.3814809322357178
37152 2023-02-16,23:39:02.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3904439210891724
37393 2023-02-16,23:39:02.982 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3734264373779297
38108 2023-02-16,23:39:02.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4255118370056152
38362 2023-02-16,23:39:03.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4066619873046875
38487 2023-02-16,23:39:03.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 323/15000, loss = 1.386846899986267
37268 2023-02-16,23:39:03.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.3999311923980713
36932 2023-02-16,23:39:03.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.3764804601669312
37533 2023-02-16,23:39:03.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4020241498947144
36814 2023-02-16,23:39:03.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 851/15000, loss = 1.4515866041183472
38255 2023-02-16,23:39:03.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4127905368804932
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37983 2023-02-16,23:39:03.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3826102018356323
40058 2023-02-16,23:39:03.135 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37862 2023-02-16,23:39:03.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3245820999145508
37757 2023-02-16,23:39:03.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4379655122756958
40058 2023-02-16,23:39:03.209 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37035 2023-02-16,23:39:03.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3648983240127563
37152 2023-02-16,23:39:03.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4112149477005005
37393 2023-02-16,23:39:03.323 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4165785312652588
38108 2023-02-16,23:39:03.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.4002974033355713
38362 2023-02-16,23:39:03.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3899962902069092
38487 2023-02-16,23:39:03.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 324/15000, loss = 1.4562574625015259
37268 2023-02-16,23:39:03.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3977493047714233
37533 2023-02-16,23:39:03.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.397714376449585
36932 2023-02-16,23:39:03.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.4238423109054565
36814 2023-02-16,23:39:03.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 852/15000, loss = 1.393946647644043
37983 2023-02-16,23:39:03.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3497649431228638
38255 2023-02-16,23:39:03.463 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3541027307510376
39195 2023-02-16,23:39:03.492 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
39195 2023-02-16,23:39:03.495 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
39195 2023-02-16,23:39:03.495 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0']
39195 2023-02-16,23:39:03.497 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 35442436}
37862 2023-02-16,23:39:03.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3726661205291748
37757 2023-02-16,23:39:03.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3956220149993896
37035 2023-02-16,23:39:03.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4223304986953735
37152 2023-02-16,23:39:03.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4313206672668457
37393 2023-02-16,23:39:03.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4330979585647583
38108 2023-02-16,23:39:03.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.396746277809143
38362 2023-02-16,23:39:03.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3849468231201172
38487 2023-02-16,23:39:03.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 325/15000, loss = 1.4063140153884888
37533 2023-02-16,23:39:03.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4423038959503174
37268 2023-02-16,23:39:03.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4331562519073486
36932 2023-02-16,23:39:03.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.3774522542953491
37983 2023-02-16,23:39:03.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.381679892539978
38255 2023-02-16,23:39:03.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3794572353363037
36814 2023-02-16,23:39:03.783 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 853/15000, loss = 1.3785605430603027
37862 2023-02-16,23:39:03.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4172354936599731
37757 2023-02-16,23:39:03.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3666490316390991
37035 2023-02-16,23:39:03.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.41768217086792
37152 2023-02-16,23:39:03.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3634092807769775
37393 2023-02-16,23:39:03.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3817397356033325
38108 2023-02-16,23:39:03.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4239833354949951
38487 2023-02-16,23:39:03.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 326/15000, loss = 1.3849570751190186
37533 2023-02-16,23:39:04.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3984179496765137
38362 2023-02-16,23:39:04.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3635506629943848
37983 2023-02-16,23:39:04.101 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4022794961929321
38255 2023-02-16,23:39:04.105 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.348067283630371
36932 2023-02-16,23:39:04.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.4373421669006348
37268 2023-02-16,23:39:04.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.440316081047058
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
36814 2023-02-16,23:39:04.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 854/15000, loss = 1.3603394031524658
37862 2023-02-16,23:39:04.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3447167873382568
37757 2023-02-16,23:39:04.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3542449474334717
37035 2023-02-16,23:39:04.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.4072108268737793
37152 2023-02-16,23:39:04.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3683850765228271
37393 2023-02-16,23:39:04.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.361450433731079
38108 2023-02-16,23:39:04.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3876941204071045
38487 2023-02-16,23:39:04.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 327/15000, loss = 1.3923012018203735
37533 2023-02-16,23:39:04.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3505991697311401
38362 2023-02-16,23:39:04.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3563123941421509
36932 2023-02-16,23:39:04.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.392141342163086
37983 2023-02-16,23:39:04.452 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621774911880493
38255 2023-02-16,23:39:04.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4008146524429321
37268 2023-02-16,23:39:04.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4087517261505127
36814 2023-02-16,23:39:04.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 855/15000, loss = 1.3763740062713623
37862 2023-02-16,23:39:04.496 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.376238465309143
37757 2023-02-16,23:39:04.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.390304446220398
37035 2023-02-16,23:39:04.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.371767520904541
37152 2023-02-16,23:39:04.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4073749780654907
37393 2023-02-16,23:39:04.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4230536222457886
38108 2023-02-16,23:39:04.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3840495347976685
38487 2023-02-16,23:39:04.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 328/15000, loss = 1.3388487100601196
37533 2023-02-16,23:39:04.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3498480319976807
38362 2023-02-16,23:39:04.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3649684190750122
36932 2023-02-16,23:39:04.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.3628716468811035
37983 2023-02-16,23:39:04.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.396160364151001
38255 2023-02-16,23:39:04.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3740438222885132
36814 2023-02-16,23:39:04.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 856/15000, loss = 1.4243078231811523
37268 2023-02-16,23:39:04.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4340078830718994
37862 2023-02-16,23:39:04.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3798141479492188
37035 2023-02-16,23:39:04.862 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3278281688690186
37757 2023-02-16,23:39:04.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.331343412399292
37152 2023-02-16,23:39:04.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3250797986984253
37393 2023-02-16,23:39:04.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3911051750183105
38108 2023-02-16,23:39:04.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4234468936920166
38487 2023-02-16,23:39:04.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 329/15000, loss = 1.3769506216049194
37533 2023-02-16,23:39:04.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4204802513122559
38362 2023-02-16,23:39:04.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3600729703903198
36932 2023-02-16,23:39:05.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4074358940124512
37983 2023-02-16,23:39:05.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3787944316864014
38255 2023-02-16,23:39:05.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.461676001548767
36814 2023-02-16,23:39:05.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 857/15000, loss = 1.3881874084472656
37268 2023-02-16,23:39:05.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.437678575515747
37862 2023-02-16,23:39:05.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3766100406646729
37035 2023-02-16,23:39:05.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.3634287118911743
37393 2023-02-16,23:39:05.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3869218826293945
37757 2023-02-16,23:39:05.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.372109293937683
38108 2023-02-16,23:39:05.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.410902738571167
37152 2023-02-16,23:39:05.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3782658576965332
38487 2023-02-16,23:39:05.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 330/15000, loss = 1.4098830223083496
37533 2023-02-16,23:39:05.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4316411018371582
38362 2023-02-16,23:39:05.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3700902462005615
36932 2023-02-16,23:39:05.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.3943864107131958
38255 2023-02-16,23:39:05.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.347245693206787
37983 2023-02-16,23:39:05.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3943932056427002
40554 2023-02-16,23:39:05.467 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
36814 2023-02-16,23:39:05.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 858/15000, loss = 1.4122014045715332
37268 2023-02-16,23:39:05.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3726282119750977
37862 2023-02-16,23:39:05.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.413548231124878
37035 2023-02-16,23:39:05.529 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.372544765472412
37393 2023-02-16,23:39:05.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3207167387008667
37152 2023-02-16,23:39:05.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3660328388214111
37757 2023-02-16,23:39:05.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3633885383605957
38108 2023-02-16,23:39:05.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4082276821136475
38487 2023-02-16,23:39:05.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 331/15000, loss = 1.381072998046875
37533 2023-02-16,23:39:05.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3880196809768677
38362 2023-02-16,23:39:05.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3871338367462158
36932 2023-02-16,23:39:05.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.4241180419921875
37983 2023-02-16,23:39:05.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4194186925888062
38255 2023-02-16,23:39:05.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.407787561416626
37268 2023-02-16,23:39:05.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.404494285583496
37862 2023-02-16,23:39:05.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3810380697250366
36814 2023-02-16,23:39:05.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 859/15000, loss = 1.4087717533111572
37035 2023-02-16,23:39:05.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.3934338092803955
37393 2023-02-16,23:39:05.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4087127447128296
37152 2023-02-16,23:39:05.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4156970977783203
38487 2023-02-16,23:39:05.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 332/15000, loss = 1.3835595846176147
37533 2023-02-16,23:39:05.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.430781364440918
37757 2023-02-16,23:39:05.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3715665340423584
38362 2023-02-16,23:39:05.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4588100910186768
38108 2023-02-16,23:39:05.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4025654792785645
36932 2023-02-16,23:39:06.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.4055869579315186
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)40186 2023-02-16,23:39:06.098 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37983 2023-02-16,23:39:06.103 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4016237258911133
38255 2023-02-16,23:39:06.106 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.402167797088623
37268 2023-02-16,23:39:06.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4205927848815918
37862 2023-02-16,23:39:06.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4488606452941895
36814 2023-02-16,23:39:06.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 860/15000, loss = 1.4045406579971313
40186 2023-02-16,23:39:06.164 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37035 2023-02-16,23:39:06.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3602635860443115
37393 2023-02-16,23:39:06.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3764151334762573
38487 2023-02-16,23:39:06.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 333/15000, loss = 1.3759647607803345
37152 2023-02-16,23:39:06.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4251856803894043
37533 2023-02-16,23:39:06.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3582595586776733
38362 2023-02-16,23:39:06.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3986268043518066
37757 2023-02-16,23:39:06.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.446734070777893
38108 2023-02-16,23:39:06.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.418761134147644
36932 2023-02-16,23:39:06.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.3778551816940308
37983 2023-02-16,23:39:06.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3918402194976807
38255 2023-02-16,23:39:06.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3910222053527832
37268 2023-02-16,23:39:06.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4171857833862305
37862 2023-02-16,23:39:06.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4464075565338135
36814 2023-02-16,23:39:06.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 861/15000, loss = 1.4019865989685059
37393 2023-02-16,23:39:06.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4131072759628296
37035 2023-02-16,23:39:06.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3599834442138672
37533 2023-02-16,23:39:06.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3853225708007812
38362 2023-02-16,23:39:06.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3973467350006104
37152 2023-02-16,23:39:06.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3298230171203613
38487 2023-02-16,23:39:06.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 334/15000, loss = 1.3826159238815308
37757 2023-02-16,23:39:06.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.399249792098999
38108 2023-02-16,23:39:06.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3663166761398315
36932 2023-02-16,23:39:06.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3529143333435059
37983 2023-02-16,23:39:06.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3990213871002197
38255 2023-02-16,23:39:06.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4033188819885254
37268 2023-02-16,23:39:06.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4342899322509766
37862 2023-02-16,23:39:06.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3448693752288818
37393 2023-02-16,23:39:06.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3835909366607666
36814 2023-02-16,23:39:06.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 862/15000, loss = 1.3811033964157104
37533 2023-02-16,23:39:06.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3783211708068848
38362 2023-02-16,23:39:06.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410499095916748
37035 2023-02-16,23:39:06.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.3943825960159302
37152 2023-02-16,23:39:06.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4471125602722168
38487 2023-02-16,23:39:06.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 335/15000, loss = 1.4298477172851562
37757 2023-02-16,23:39:06.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4202289581298828
38108 2023-02-16,23:39:06.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4099159240722656
36932 2023-02-16,23:39:07.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4103522300720215
37983 2023-02-16,23:39:07.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3857641220092773
38255 2023-02-16,23:39:07.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.336999535560608
39288 2023-02-16,23:39:07.079 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
37268 2023-02-16,23:39:07.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.399834156036377
39288 2023-02-16,23:39:07.098 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
39288 2023-02-16,23:39:07.098 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1']
39288 2023-02-16,23:39:07.100 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 28354564}
37862 2023-02-16,23:39:07.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.406874656677246
37393 2023-02-16,23:39:07.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3606027364730835
36814 2023-02-16,23:39:07.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 863/15000, loss = 1.4128371477127075
37533 2023-02-16,23:39:07.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3782812356948853
37035 2023-02-16,23:39:07.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.359203815460205
37152 2023-02-16,23:39:07.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3759196996688843
38362 2023-02-16,23:39:07.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4159910678863525
38487 2023-02-16,23:39:07.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 336/15000, loss = 1.3800071477890015
38108 2023-02-16,23:39:07.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4084309339523315
37757 2023-02-16,23:39:07.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3780876398086548
36932 2023-02-16,23:39:07.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.433384656906128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37983 2023-02-16,23:39:07.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3712259531021118
38255 2023-02-16,23:39:07.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3741281032562256
37268 2023-02-16,23:39:07.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4196311235427856
37862 2023-02-16,23:39:07.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.471052885055542
37393 2023-02-16,23:39:07.471 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4263122081756592
37533 2023-02-16,23:39:07.529 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3929352760314941
36814 2023-02-16,23:39:07.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 864/15000, loss = 1.4081785678863525
38362 2023-02-16,23:39:07.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.3838458061218262
38487 2023-02-16,23:39:07.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 337/15000, loss = 1.3370418548583984
37035 2023-02-16,23:39:07.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.3608500957489014
37152 2023-02-16,23:39:07.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4003993272781372
38108 2023-02-16,23:39:07.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4158440828323364
37757 2023-02-16,23:39:07.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3665353059768677
36932 2023-02-16,23:39:07.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4292711019515991
37983 2023-02-16,23:39:07.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4070289134979248
38255 2023-02-16,23:39:07.693 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3833547830581665
37268 2023-02-16,23:39:07.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3943164348602295
37862 2023-02-16,23:39:07.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3876328468322754
37393 2023-02-16,23:39:07.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4220985174179077
37533 2023-02-16,23:39:07.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3940256834030151
38362 2023-02-16,23:39:07.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4128652811050415
38487 2023-02-16,23:39:07.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 338/15000, loss = 1.3818737268447876
36814 2023-02-16,23:39:07.899 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 865/15000, loss = 1.3913897275924683
37152 2023-02-16,23:39:07.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4333363771438599
37035 2023-02-16,23:39:07.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.3892544507980347
37757 2023-02-16,23:39:07.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3889195919036865
38108 2023-02-16,23:39:07.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.392142653465271
36932 2023-02-16,23:39:07.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.408028483390808
37983 2023-02-16,23:39:08.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3939087390899658
38255 2023-02-16,23:39:08.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3730599880218506
37268 2023-02-16,23:39:08.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4559409618377686
37862 2023-02-16,23:39:08.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.381669521331787
37393 2023-02-16,23:39:08.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4101581573486328
37533 2023-02-16,23:39:08.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3665577173233032
38362 2023-02-16,23:39:08.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3541306257247925
38487 2023-02-16,23:39:08.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 339/15000, loss = 1.419670581817627
36814 2023-02-16,23:39:08.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 866/15000, loss = 1.364692211151123
37035 2023-02-16,23:39:08.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3778363466262817
37152 2023-02-16,23:39:08.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4210264682769775
37757 2023-02-16,23:39:08.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.362559199333191
38108 2023-02-16,23:39:08.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3323088884353638
37983 2023-02-16,23:39:08.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3739184141159058
38255 2023-02-16,23:39:08.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825607299804688
36932 2023-02-16,23:39:08.346 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4526057243347168
37268 2023-02-16,23:39:08.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.352729082107544
37862 2023-02-16,23:39:08.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.401557445526123
37393 2023-02-16,23:39:08.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4528717994689941
37533 2023-02-16,23:39:08.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4093159437179565
38362 2023-02-16,23:39:08.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3794442415237427
38487 2023-02-16,23:39:08.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 340/15000, loss = 1.3978548049926758
36814 2023-02-16,23:39:08.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 867/15000, loss = 1.3599556684494019
37035 2023-02-16,23:39:08.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.404003381729126
37152 2023-02-16,23:39:08.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4029334783554077
38108 2023-02-16,23:39:08.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3900188207626343
37757 2023-02-16,23:39:08.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3725619316101074
38255 2023-02-16,23:39:08.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4125354290008545
40677 2023-02-16,23:39:08.685 - {main_tc.py (37)} - <module>(): Namespace(data_file_path='/data/wyz/fednlp_data/data_files/agnews_data.h5', dataset='agnews', do_lower_case=True, epochs=50, eval_batch_size=8, evaluate_during_training_steps=100, forward_mode=True, fp16=False, freeze_layers='e,0,1,2,3,4,5', gradient_accumulation_steps=1, is_debug_mode=0, learning_rate=0.0001, manual_seed=42, max_seq_length=64, model_name='distilbert-base-uncased', model_type='distilbert', n_gpu=1, output_dir='/tmp/', partition_file_path='/data/wyz/fednlp_data/partition_files/agnews_partition.h5', partition_method='uniform', reprocess_input_data=False, run_id=0, train_batch_size=8, use_adapter=False, use_quantize=False, weight_decay=0)
37983 2023-02-16,23:39:08.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3549304008483887
36932 2023-02-16,23:39:08.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4118624925613403
37862 2023-02-16,23:39:08.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3742561340332031
37268 2023-02-16,23:39:08.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4177565574645996
37393 2023-02-16,23:39:08.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4039655923843384
37533 2023-02-16,23:39:08.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.385180115699768
38362 2023-02-16,23:39:08.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3480850458145142
38487 2023-02-16,23:39:08.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 341/15000, loss = 1.3643463850021362
37035 2023-02-16,23:39:08.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.40387761592865
37152 2023-02-16,23:39:08.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4443776607513428
36814 2023-02-16,23:39:08.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 868/15000, loss = 1.4310784339904785
38108 2023-02-16,23:39:08.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3906279802322388
38255 2023-02-16,23:39:08.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3561739921569824
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)40293 2023-02-16,23:39:08.991 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
37757 2023-02-16,23:39:09.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3555700778961182
37983 2023-02-16,23:39:09.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3843648433685303
40293 2023-02-16,23:39:09.037 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36932 2023-02-16,23:39:09.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.3598747253417969
37268 2023-02-16,23:39:09.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3787751197814941
37393 2023-02-16,23:39:09.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.39216947555542
37862 2023-02-16,23:39:09.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3938064575195312
37533 2023-02-16,23:39:09.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3946821689605713
38362 2023-02-16,23:39:09.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4008188247680664
38487 2023-02-16,23:39:09.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 342/15000, loss = 1.3735830783843994
37035 2023-02-16,23:39:09.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.404847264289856
36814 2023-02-16,23:39:09.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 869/15000, loss = 1.3439404964447021
37152 2023-02-16,23:39:09.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4029157161712646
38108 2023-02-16,23:39:09.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4260172843933105
38255 2023-02-16,23:39:09.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.454334020614624
37983 2023-02-16,23:39:09.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4395498037338257
36932 2023-02-16,23:39:09.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3884022235870361
37268 2023-02-16,23:39:09.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3165614604949951
37393 2023-02-16,23:39:09.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.401097059249878
37757 2023-02-16,23:39:09.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372857689857483
37862 2023-02-16,23:39:09.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.418743371963501
37533 2023-02-16,23:39:09.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4010648727416992
38362 2023-02-16,23:39:09.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3740962743759155
38487 2023-02-16,23:39:09.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 343/15000, loss = 1.3976778984069824
37035 2023-02-16,23:39:09.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.3490879535675049
36814 2023-02-16,23:39:09.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 870/15000, loss = 1.3520407676696777
37152 2023-02-16,23:39:09.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3670181035995483
38108 2023-02-16,23:39:09.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4030842781066895
38255 2023-02-16,23:39:09.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3825929164886475
37983 2023-02-16,23:39:09.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3718042373657227
36932 2023-02-16,23:39:09.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.3718757629394531
37268 2023-02-16,23:39:09.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3641408681869507
37393 2023-02-16,23:39:09.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.406174659729004
37757 2023-02-16,23:39:09.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3619967699050903
37533 2023-02-16,23:39:09.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4289790391921997
38362 2023-02-16,23:39:09.799 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4616246223449707
38487 2023-02-16,23:39:09.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 344/15000, loss = 1.4260352849960327
37862 2023-02-16,23:39:09.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4159237146377563
37035 2023-02-16,23:39:09.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4014687538146973
36814 2023-02-16,23:39:09.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 871/15000, loss = 1.3644490242004395
37152 2023-02-16,23:39:09.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3743228912353516
38255 2023-02-16,23:39:09.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4071372747421265
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 164.12 MiB already allocated; 23.56 MiB free; 172.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

38108 2023-02-16,23:39:09.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4096550941467285
37983 2023-02-16,23:39:09.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3939917087554932
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37268 2023-02-16,23:39:10.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4368232488632202
37393 2023-02-16,23:39:10.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.403942584991455
36932 2023-02-16,23:39:10.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.413336157798767
37757 2023-02-16,23:39:10.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4125173091888428
37533 2023-02-16,23:39:10.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3799866437911987
38362 2023-02-16,23:39:10.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3471369743347168
38487 2023-02-16,23:39:10.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 345/15000, loss = 1.4130268096923828
37862 2023-02-16,23:39:10.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3806428909301758
37035 2023-02-16,23:39:10.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.35688316822052
38255 2023-02-16,23:39:10.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.395255446434021
36814 2023-02-16,23:39:10.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 872/15000, loss = 1.4013018608093262
37152 2023-02-16,23:39:10.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4171231985092163
37983 2023-02-16,23:39:10.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.324486494064331
38108 2023-02-16,23:39:10.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3901653289794922
37268 2023-02-16,23:39:10.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3952702283859253
37393 2023-02-16,23:39:10.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.3997350931167603
36932 2023-02-16,23:39:10.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.371525764465332
37757 2023-02-16,23:39:10.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4225519895553589
37533 2023-02-16,23:39:10.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4227815866470337
38362 2023-02-16,23:39:10.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4078822135925293
38487 2023-02-16,23:39:10.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 346/15000, loss = 1.4329969882965088
37862 2023-02-16,23:39:10.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4085454940795898
37035 2023-02-16,23:39:10.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.4179635047912598
38255 2023-02-16,23:39:10.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4256212711334229
36814 2023-02-16,23:39:10.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 873/15000, loss = 1.3803608417510986
37152 2023-02-16,23:39:10.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.386917233467102
37983 2023-02-16,23:39:10.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.372559905052185
38108 2023-02-16,23:39:10.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.382439136505127
37268 2023-02-16,23:39:10.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3419733047485352
37393 2023-02-16,23:39:10.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.397904396057129
36932 2023-02-16,23:39:10.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.394258737564087
37757 2023-02-16,23:39:10.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.443014144897461
37533 2023-02-16,23:39:10.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3785649538040161
38362 2023-02-16,23:39:10.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4023277759552002
38487 2023-02-16,23:39:10.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 347/15000, loss = 1.3585247993469238
37862 2023-02-16,23:39:10.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4648985862731934
37035 2023-02-16,23:39:10.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.362779140472412
37152 2023-02-16,23:39:10.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4186877012252808
38255 2023-02-16,23:39:10.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.4003037214279175
36814 2023-02-16,23:39:10.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 874/15000, loss = 1.4122658967971802
37983 2023-02-16,23:39:10.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.417270302772522
38108 2023-02-16,23:39:10.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3500280380249023
37393 2023-02-16,23:39:11.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4326804876327515
37268 2023-02-16,23:39:11.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3928366899490356
36932 2023-02-16,23:39:11.103 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.4317739009857178
37757 2023-02-16,23:39:11.109 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3697484731674194
38487 2023-02-16,23:39:11.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 348/15000, loss = 1.3215856552124023
37533 2023-02-16,23:39:11.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3978264331817627
38362 2023-02-16,23:39:11.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.3910391330718994
37862 2023-02-16,23:39:11.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4380437135696411
37035 2023-02-16,23:39:11.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.3787269592285156
37152 2023-02-16,23:39:11.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.410643458366394
38255 2023-02-16,23:39:11.265 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.396722674369812
36814 2023-02-16,23:39:11.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 875/15000, loss = 1.41119384765625
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 90.00 MiB already allocated; 23.56 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

37983 2023-02-16,23:39:11.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3447214365005493
38108 2023-02-16,23:39:11.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3816653490066528
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 100, in <module>
    trainer.train_model()
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 104, in train_model
    loss, jvp = fc.jvp(f, (self.params,), (v_params,))
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 788, in jvp
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/vmap.py", line 35, in fn
    return f(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 837, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 332, in functional_get_loss
    y = model(params,buffers, x)[0]
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/make_functional.py", line 282, in forward
    return self.stateless_model(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 779, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/context.py", line 108, in wrapper_func
    results = f(self, *args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 597, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 365, in forward
    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 306, in forward
    output_attentions=output_attentions,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 215, in forward
    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/lora.py", line 251, in forward
    return F.linear(x, T(self.weight), bias=self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 90.00 MiB already allocated; 23.56 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 100, in <module>
    trainer.train_model()
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 104, in train_model
    loss, jvp = fc.jvp(f, (self.params,), (v_params,))
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 788, in jvp
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/vmap.py", line 35, in fn
    return f(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 837, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 332, in functional_get_loss
    y = model(params,buffers, x)[0]
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/make_functional.py", line 282, in forward
    return self.stateless_model(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 779, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/context.py", line 108, in wrapper_func
    results = f(self, *args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 597, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 365, in forward
    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 306, in forward
    output_attentions=output_attentions,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 215, in forward
    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/lora.py", line 251, in forward
    return F.linear(x, T(self.weight), bias=self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

37268 2023-02-16,23:39:11.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3875508308410645
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 90.00 MiB already allocated; 23.56 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 90.00 MiB already allocated; 23.56 MiB free; 90.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

36932 2023-02-16,23:39:11.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.4006078243255615
37393 2023-02-16,23:39:11.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.439996361732483
37757 2023-02-16,23:39:11.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3976306915283203
38487 2023-02-16,23:39:11.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 349/15000, loss = 1.3793576955795288
37533 2023-02-16,23:39:11.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3559596538543701
38362 2023-02-16,23:39:11.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4034192562103271
37862 2023-02-16,23:39:11.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3956475257873535
37035 2023-02-16,23:39:11.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.3836606740951538
37152 2023-02-16,23:39:11.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.406330943107605
38255 2023-02-16,23:39:11.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4240204095840454
36814 2023-02-16,23:39:11.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 876/15000, loss = 1.3340924978256226
37983 2023-02-16,23:39:11.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3760182857513428
38108 2023-02-16,23:39:11.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4021378755569458
37268 2023-02-16,23:39:11.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.383442997932434
38487 2023-02-16,23:39:11.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 350/15000, loss = 1.3312809467315674
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 100, in <module>
    trainer.train_model()
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 104, in train_model
    loss, jvp = fc.jvp(f, (self.params,), (v_params,))
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 788, in jvp
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/vmap.py", line 35, in fn
    return f(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 837, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 332, in functional_get_loss
    y = model(params,buffers, x)[0]
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/make_functional.py", line 282, in forward
    return self.stateless_model(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 779, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/context.py", line 108, in wrapper_func
    results = f(self, *args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 597, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 365, in forward
    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 306, in forward
    output_attentions=output_attentions,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 215, in forward
    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/lora.py", line 251, in forward
    return F.linear(x, T(self.weight), bias=self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

36932 2023-02-16,23:39:11.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.3522869348526
37393 2023-02-16,23:39:11.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4082080125808716
37757 2023-02-16,23:39:11.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4142553806304932
37533 2023-02-16,23:39:11.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4248210191726685
38362 2023-02-16,23:39:11.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3370085954666138
37862 2023-02-16,23:39:11.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3666126728057861
37035 2023-02-16,23:39:11.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.4368458986282349
37152 2023-02-16,23:39:11.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3887206315994263
38255 2023-02-16,23:39:11.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3877463340759277
36814 2023-02-16,23:39:11.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 877/15000, loss = 1.4238529205322266
37983 2023-02-16,23:39:11.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3798043727874756
38108 2023-02-16,23:39:11.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621746301651
37268 2023-02-16,23:39:12.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.413241982460022
38487 2023-02-16,23:39:12.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 351/15000, loss = 1.3861973285675049
36932 2023-02-16,23:39:12.090 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.4210652112960815
37393 2023-02-16,23:39:12.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4336950778961182
37757 2023-02-16,23:39:12.098 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4068095684051514
37533 2023-02-16,23:39:12.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.383200764656067
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)40433 2023-02-16,23:39:12.123 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
38362 2023-02-16,23:39:12.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3741463422775269
37862 2023-02-16,23:39:12.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.35417640209198
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 110.04 MiB already allocated; 23.56 MiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

37035 2023-02-16,23:39:12.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.3434829711914062
40433 2023-02-16,23:39:12.175 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
37152 2023-02-16,23:39:12.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.409150242805481
38255 2023-02-16,23:39:12.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3841184377670288
36814 2023-02-16,23:39:12.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 878/15000, loss = 1.3712108135223389
38108 2023-02-16,23:39:12.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.396109938621521
37983 2023-02-16,23:39:12.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.376630187034607
37268 2023-02-16,23:39:12.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4110515117645264
38487 2023-02-16,23:39:12.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 352/15000, loss = 1.4066485166549683
36932 2023-02-16,23:39:12.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3947618007659912
37393 2023-02-16,23:39:12.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4380091428756714
37757 2023-02-16,23:39:12.438 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4034513235092163
37533 2023-02-16,23:39:12.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3960700035095215
38362 2023-02-16,23:39:12.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3833417892456055
37862 2023-02-16,23:39:12.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.390296220779419
37035 2023-02-16,23:39:12.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.3770114183425903
36814 2023-02-16,23:39:12.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 879/15000, loss = 1.3561913967132568
37152 2023-02-16,23:39:12.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3861725330352783
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 97, in <module>
    trainer = ForwardTextClassificationTrainer(model_args, device, model, train_dl, test_dl)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 41, in __init__
    self.model.to(self.device)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

38255 2023-02-16,23:39:12.570 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4235304594039917
37983 2023-02-16,23:39:12.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4135957956314087
38108 2023-02-16,23:39:12.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3788565397262573
37268 2023-02-16,23:39:12.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4099061489105225
36932 2023-02-16,23:39:12.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.335782766342163
37393 2023-02-16,23:39:12.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3724644184112549
38487 2023-02-16,23:39:12.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 353/15000, loss = 1.3899972438812256
37757 2023-02-16,23:39:12.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3977457284927368
38362 2023-02-16,23:39:12.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3731099367141724
37533 2023-02-16,23:39:12.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3953893184661865
37862 2023-02-16,23:39:12.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3313772678375244
37035 2023-02-16,23:39:12.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.4231458902359009
36814 2023-02-16,23:39:12.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 880/15000, loss = 1.3823233842849731
37152 2023-02-16,23:39:12.829 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3292620182037354
38255 2023-02-16,23:39:12.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4107307195663452
37983 2023-02-16,23:39:12.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3809726238250732
38108 2023-02-16,23:39:12.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3944437503814697
37268 2023-02-16,23:39:13.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4195493459701538
38487 2023-02-16,23:39:13.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 354/15000, loss = 1.3849406242370605
36932 2023-02-16,23:39:13.033 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.3674548864364624
37393 2023-02-16,23:39:13.039 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4038605690002441
37757 2023-02-16,23:39:13.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4437988996505737
38362 2023-02-16,23:39:13.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825099468231201
37862 2023-02-16,23:39:13.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3721140623092651
37533 2023-02-16,23:39:13.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.386672854423523
36814 2023-02-16,23:39:13.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 881/15000, loss = 1.392220377922058
37035 2023-02-16,23:39:13.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.377968668937683
37152 2023-02-16,23:39:13.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4090728759765625
37983 2023-02-16,23:39:13.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4489637613296509
38255 2023-02-16,23:39:13.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4083302021026611
38108 2023-02-16,23:39:13.301 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4193025827407837
38487 2023-02-16,23:39:13.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 355/15000, loss = 1.3635790348052979
36932 2023-02-16,23:39:13.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3877205848693848
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37393 2023-02-16,23:39:13.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4212052822113037
37268 2023-02-16,23:39:13.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4203228950500488
37757 2023-02-16,23:39:13.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3985226154327393
38362 2023-02-16,23:39:13.425 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4124404191970825
37533 2023-02-16,23:39:13.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4064284563064575
37862 2023-02-16,23:39:13.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.363290548324585
36814 2023-02-16,23:39:13.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 882/15000, loss = 1.3873803615570068
37152 2023-02-16,23:39:13.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3847523927688599
37035 2023-02-16,23:39:13.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.435179591178894
37983 2023-02-16,23:39:13.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4465030431747437
38255 2023-02-16,23:39:13.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4024429321289062
38108 2023-02-16,23:39:13.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.401460886001587
38487 2023-02-16,23:39:13.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 356/15000, loss = 1.3562684059143066
36932 2023-02-16,23:39:13.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.357780933380127
37268 2023-02-16,23:39:13.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4002647399902344
37393 2023-02-16,23:39:13.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4181498289108276
37757 2023-02-16,23:39:13.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3508014678955078
36814 2023-02-16,23:39:13.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 883/15000, loss = 1.3856174945831299
38362 2023-02-16,23:39:13.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3561749458312988
37533 2023-02-16,23:39:13.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4057631492614746
37862 2023-02-16,23:39:13.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3717145919799805
37035 2023-02-16,23:39:13.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3920334577560425
37152 2023-02-16,23:39:13.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3777230978012085
37983 2023-02-16,23:39:13.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3449475765228271
38255 2023-02-16,23:39:13.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4188129901885986
38108 2023-02-16,23:39:13.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.391930341720581
38487 2023-02-16,23:39:13.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 357/15000, loss = 1.3649520874023438
36932 2023-02-16,23:39:13.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.38860023021698
36814 2023-02-16,23:39:14.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 884/15000, loss = 1.3807835578918457
37268 2023-02-16,23:39:14.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4044804573059082
37393 2023-02-16,23:39:14.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4353010654449463
37757 2023-02-16,23:39:14.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3494606018066406
37533 2023-02-16,23:39:14.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4137382507324219
38362 2023-02-16,23:39:14.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.454383373260498
37035 2023-02-16,23:39:14.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.3631256818771362
37862 2023-02-16,23:39:14.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.446886658668518
37152 2023-02-16,23:39:14.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.415816307067871
37983 2023-02-16,23:39:14.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4068684577941895
36814 2023-02-16,23:39:14.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 885/15000, loss = 1.3781074285507202
36932 2023-02-16,23:39:14.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.340366244316101
38108 2023-02-16,23:39:14.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3990193605422974
38255 2023-02-16,23:39:14.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3663474321365356
37268 2023-02-16,23:39:14.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.366348147392273
38487 2023-02-16,23:39:14.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 358/15000, loss = 1.3600856065750122
37393 2023-02-16,23:39:14.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4011154174804688
37757 2023-02-16,23:39:14.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4207111597061157
37035 2023-02-16,23:39:14.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4054306745529175
37533 2023-02-16,23:39:14.440 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.412521243095398
37862 2023-02-16,23:39:14.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3992398977279663
37152 2023-02-16,23:39:14.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3666555881500244
38362 2023-02-16,23:39:14.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3826731443405151
36814 2023-02-16,23:39:14.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 886/15000, loss = 1.4294719696044922
37983 2023-02-16,23:39:14.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4710417985916138
36932 2023-02-16,23:39:14.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.3951387405395508
38108 2023-02-16,23:39:14.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3858413696289062
38255 2023-02-16,23:39:14.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4100210666656494
37268 2023-02-16,23:39:14.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4177627563476562
37393 2023-02-16,23:39:14.731 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4201494455337524
38487 2023-02-16,23:39:14.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 359/15000, loss = 1.3701504468917847
37757 2023-02-16,23:39:14.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.432236909866333
37533 2023-02-16,23:39:14.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4235961437225342
36814 2023-02-16,23:39:14.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 887/15000, loss = 1.3793730735778809
37035 2023-02-16,23:39:14.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.395844578742981
37862 2023-02-16,23:39:14.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.420390009880066
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)37152 2023-02-16,23:39:14.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3946609497070312
40554 2023-02-16,23:39:14.822 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
38362 2023-02-16,23:39:14.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.407165765762329
37983 2023-02-16,23:39:14.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.387628436088562
40554 2023-02-16,23:39:14.920 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36932 2023-02-16,23:39:14.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.390273094177246
38108 2023-02-16,23:39:15.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3712348937988281
38255 2023-02-16,23:39:15.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4083633422851562
37268 2023-02-16,23:39:15.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4350392818450928
37393 2023-02-16,23:39:15.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3963005542755127
38487 2023-02-16,23:39:15.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 360/15000, loss = 1.3871904611587524
37533 2023-02-16,23:39:15.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3799844980239868
37757 2023-02-16,23:39:15.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3882161378860474
36814 2023-02-16,23:39:15.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 888/15000, loss = 1.3643070459365845
37035 2023-02-16,23:39:15.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.422022819519043
37152 2023-02-16,23:39:15.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.378226637840271
37862 2023-02-16,23:39:15.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3782116174697876
38362 2023-02-16,23:39:15.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3953481912612915
37983 2023-02-16,23:39:15.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3815782070159912
36932 2023-02-16,23:39:15.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.3575751781463623
37268 2023-02-16,23:39:15.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4185932874679565
38108 2023-02-16,23:39:15.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4070098400115967
38255 2023-02-16,23:39:15.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4158644676208496
37393 2023-02-16,23:39:15.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4551962614059448
36814 2023-02-16,23:39:15.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 889/15000, loss = 1.3810850381851196
37533 2023-02-16,23:39:15.439 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4086458683013916
37757 2023-02-16,23:39:15.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4322291612625122
38487 2023-02-16,23:39:15.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 361/15000, loss = 1.4587775468826294
37035 2023-02-16,23:39:15.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.406768798828125
37152 2023-02-16,23:39:15.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3847681283950806
37862 2023-02-16,23:39:15.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3665622472763062
38362 2023-02-16,23:39:15.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4256030321121216
37983 2023-02-16,23:39:15.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4014791250228882
36932 2023-02-16,23:39:15.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.3726475238800049
37268 2023-02-16,23:39:15.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.410268783569336
38108 2023-02-16,23:39:15.712 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.39401113986969
38255 2023-02-16,23:39:15.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.392065167427063
37393 2023-02-16,23:39:15.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3536826372146606
36814 2023-02-16,23:39:15.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 890/15000, loss = 1.3800312280654907
37533 2023-02-16,23:39:15.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3782914876937866
37757 2023-02-16,23:39:15.782 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.357602596282959
38487 2023-02-16,23:39:15.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 362/15000, loss = 1.3986730575561523
37035 2023-02-16,23:39:15.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.3777000904083252
37152 2023-02-16,23:39:15.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3527449369430542
37862 2023-02-16,23:39:15.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3888459205627441
38362 2023-02-16,23:39:15.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.4003220796585083
37983 2023-02-16,23:39:15.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3741182088851929
36932 2023-02-16,23:39:15.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.3954929113388062
37268 2023-02-16,23:39:16.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3784013986587524
38108 2023-02-16,23:39:16.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3738657236099243
38255 2023-02-16,23:39:16.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3322566747665405
37393 2023-02-16,23:39:16.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4174388647079468
36814 2023-02-16,23:39:16.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 891/15000, loss = 1.3722341060638428
37533 2023-02-16,23:39:16.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3749192953109741
37757 2023-02-16,23:39:16.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3848073482513428
38487 2023-02-16,23:39:16.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 363/15000, loss = 1.3972679376602173
37035 2023-02-16,23:39:16.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3519182205200195
37152 2023-02-16,23:39:16.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4251306056976318
37862 2023-02-16,23:39:16.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3624428510665894
38362 2023-02-16,23:39:16.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3967576026916504
37983 2023-02-16,23:39:16.279 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3937554359436035
36932 2023-02-16,23:39:16.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.436414361000061
37268 2023-02-16,23:39:16.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3880927562713623
38108 2023-02-16,23:39:16.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3550492525100708
38255 2023-02-16,23:39:16.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3900110721588135
37393 2023-02-16,23:39:16.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3781808614730835
37533 2023-02-16,23:39:16.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3822075128555298
37757 2023-02-16,23:39:16.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3789591789245605
38487 2023-02-16,23:39:16.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 364/15000, loss = 1.3410470485687256
36814 2023-02-16,23:39:16.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 892/15000, loss = 1.408812165260315
37035 2023-02-16,23:39:16.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4099359512329102
37152 2023-02-16,23:39:16.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.405050277709961
37862 2023-02-16,23:39:16.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3725441694259644
38362 2023-02-16,23:39:16.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4240878820419312
37983 2023-02-16,23:39:16.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4187953472137451
36932 2023-02-16,23:39:16.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.430151343345642
37268 2023-02-16,23:39:16.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3797571659088135
38108 2023-02-16,23:39:16.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3844690322875977
38255 2023-02-16,23:39:16.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3907146453857422
37393 2023-02-16,23:39:16.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3170318603515625
37533 2023-02-16,23:39:16.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3257564306259155
37757 2023-02-16,23:39:16.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3781726360321045
38487 2023-02-16,23:39:16.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 365/15000, loss = 1.4160370826721191
36814 2023-02-16,23:39:16.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 893/15000, loss = 1.3826375007629395
37035 2023-02-16,23:39:16.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.4343372583389282
37152 2023-02-16,23:39:16.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3648664951324463
37862 2023-02-16,23:39:16.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3555840253829956
38362 2023-02-16,23:39:16.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.38770592212677
37983 2023-02-16,23:39:16.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4159677028656006
36932 2023-02-16,23:39:16.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 805/15000, loss = 1.3878235816955566
37268 2023-02-16,23:39:17.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3711867332458496
38255 2023-02-16,23:39:17.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.426029920578003
37393 2023-02-16,23:39:17.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3645702600479126
38108 2023-02-16,23:39:17.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.439462423324585
37757 2023-02-16,23:39:17.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.392974853515625
38487 2023-02-16,23:39:17.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 366/15000, loss = 1.383858323097229
36814 2023-02-16,23:39:17.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 894/15000, loss = 1.3563575744628906
37035 2023-02-16,23:39:17.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4296190738677979
37533 2023-02-16,23:39:17.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.37821626663208
37152 2023-02-16,23:39:17.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4190783500671387
37862 2023-02-16,23:39:17.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3728864192962646
38362 2023-02-16,23:39:17.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3840678930282593
37983 2023-02-16,23:39:17.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3807505369186401
36932 2023-02-16,23:39:17.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 806/15000, loss = 1.3818562030792236
37268 2023-02-16,23:39:17.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.419985294342041
38255 2023-02-16,23:39:17.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4031749963760376
37393 2023-02-16,23:39:17.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4368321895599365
38108 2023-02-16,23:39:17.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3719156980514526
36814 2023-02-16,23:39:17.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 895/15000, loss = 1.3867470026016235
37035 2023-02-16,23:39:17.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.4073773622512817
37533 2023-02-16,23:39:17.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3485037088394165
37757 2023-02-16,23:39:17.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.394249439239502
38487 2023-02-16,23:39:17.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 367/15000, loss = 1.4127953052520752
37152 2023-02-16,23:39:17.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3913320302963257
37862 2023-02-16,23:39:17.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3620257377624512
38362 2023-02-16,23:39:17.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4234809875488281
37983 2023-02-16,23:39:17.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4085192680358887
36932 2023-02-16,23:39:17.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 807/15000, loss = 1.3708627223968506
37268 2023-02-16,23:39:17.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3960132598876953
38255 2023-02-16,23:39:17.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4097046852111816
37393 2023-02-16,23:39:17.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3956180810928345
38108 2023-02-16,23:39:17.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.394086480140686
36814 2023-02-16,23:39:17.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 896/15000, loss = 1.3705471754074097
37035 2023-02-16,23:39:17.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.451944351196289
37533 2023-02-16,23:39:17.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4009021520614624
37757 2023-02-16,23:39:17.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3665788173675537
38487 2023-02-16,23:39:17.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 368/15000, loss = 1.3542250394821167
37152 2023-02-16,23:39:17.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.478988528251648
37862 2023-02-16,23:39:17.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4124627113342285
38362 2023-02-16,23:39:17.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.410956859588623
37983 2023-02-16,23:39:17.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4651025533676147
36932 2023-02-16,23:39:18.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 808/15000, loss = 1.4171000719070435
37268 2023-02-16,23:39:18.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.403630018234253
38255 2023-02-16,23:39:18.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.3901219367980957
37393 2023-02-16,23:39:18.106 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3434171676635742
38108 2023-02-16,23:39:18.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3245816230773926
DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)40677 2023-02-16,23:39:18.121 - {initializer.py (66)} - create_model(): DistilBertForSequenceClassification(
  (shared_parameters): ModuleDict()
  (distilbert): DistilBertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict()
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (k_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (v_lin): Linear(
              in_features=768, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
            (prefix_tuning): PrefixTuningShim(
              (prefix_gates): ModuleDict()
              (pool): PrefixTuningPool(
                (prefix_tunings): ModuleDict()
              )
            )
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (lin2): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (attention_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
          (output_adapters): AdapterLayer(
            (adapters): ModuleDict()
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
40677 2023-02-16,23:39:18.175 - {base_data_manager.py (306)} - _load_data_loader_from_cache():  Loading features from cached file cache_dir/distilbert_distilbert-base-uncased_cached_64_ClassificationModel_agnews_uniform_-1
36814 2023-02-16,23:39:18.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 897/15000, loss = 1.3924096822738647
37035 2023-02-16,23:39:18.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4108543395996094
37533 2023-02-16,23:39:18.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4180290699005127
37757 2023-02-16,23:39:18.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4105803966522217
38487 2023-02-16,23:39:18.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 369/15000, loss = 1.3794903755187988
37152 2023-02-16,23:39:18.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3672168254852295
37862 2023-02-16,23:39:18.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4225224256515503
38362 2023-02-16,23:39:18.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4083296060562134
37983 2023-02-16,23:39:18.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.438153624534607
36932 2023-02-16,23:39:18.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 809/15000, loss = 1.4130847454071045
37268 2023-02-16,23:39:18.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3662406206130981
38255 2023-02-16,23:39:18.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.38265860080719
37393 2023-02-16,23:39:18.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.393362283706665
38108 2023-02-16,23:39:18.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.37276029586792
36814 2023-02-16,23:39:18.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 898/15000, loss = 1.4190940856933594
37035 2023-02-16,23:39:18.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.3584225177764893
37533 2023-02-16,23:39:18.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.410586953163147
37757 2023-02-16,23:39:18.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3857340812683105
38487 2023-02-16,23:39:18.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 370/15000, loss = 1.3481041193008423
37152 2023-02-16,23:39:18.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3791531324386597
37862 2023-02-16,23:39:18.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4430482387542725
37983 2023-02-16,23:39:18.665 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3955470323562622
38362 2023-02-16,23:39:18.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4025598764419556
36932 2023-02-16,23:39:18.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 810/15000, loss = 1.3887925148010254
37268 2023-02-16,23:39:18.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3704900741577148
38255 2023-02-16,23:39:18.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3498831987380981
37393 2023-02-16,23:39:18.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3891677856445312
38108 2023-02-16,23:39:18.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.417189598083496
36814 2023-02-16,23:39:18.834 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 899/15000, loss = 1.3647711277008057
37035 2023-02-16,23:39:18.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3890836238861084
37533 2023-02-16,23:39:18.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3941335678100586
37757 2023-02-16,23:39:18.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3957452774047852
38487 2023-02-16,23:39:18.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 371/15000, loss = 1.4008725881576538
37152 2023-02-16,23:39:18.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.3723361492156982
37862 2023-02-16,23:39:18.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3696290254592896
37983 2023-02-16,23:39:19.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.366507649421692
38362 2023-02-16,23:39:19.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.4188355207443237
36932 2023-02-16,23:39:19.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 811/15000, loss = 1.2972949743270874
37268 2023-02-16,23:39:19.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.380484938621521
38255 2023-02-16,23:39:19.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3816909790039062
37393 2023-02-16,23:39:19.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.382535457611084
38108 2023-02-16,23:39:19.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3449112176895142
36814 2023-02-16,23:39:19.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 900/15000, loss = 1.3811135292053223
37035 2023-02-16,23:39:19.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.3717347383499146
37533 2023-02-16,23:39:19.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3948781490325928
37757 2023-02-16,23:39:19.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.401122808456421
38487 2023-02-16,23:39:19.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 372/15000, loss = 1.3741092681884766
37152 2023-02-16,23:39:19.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4484559297561646
37862 2023-02-16,23:39:19.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.397577166557312
37983 2023-02-16,23:39:19.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.354007601737976
38362 2023-02-16,23:39:19.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.366359829902649
36932 2023-02-16,23:39:19.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 812/15000, loss = 1.3448466062545776
37268 2023-02-16,23:39:19.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.399341106414795
38255 2023-02-16,23:39:19.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.402290940284729
37393 2023-02-16,23:39:19.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4144222736358643
38108 2023-02-16,23:39:19.482 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3761281967163086
36814 2023-02-16,23:39:19.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 901/15000, loss = 1.3727904558181763
37035 2023-02-16,23:39:19.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.412947177886963
37533 2023-02-16,23:39:19.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3926633596420288
37757 2023-02-16,23:39:19.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4304174184799194
38487 2023-02-16,23:39:19.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 373/15000, loss = 1.4615910053253174
37152 2023-02-16,23:39:19.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3718088865280151
37862 2023-02-16,23:39:19.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4142515659332275
37983 2023-02-16,23:39:19.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3903523683547974
38362 2023-02-16,23:39:19.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4099379777908325
36932 2023-02-16,23:39:19.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 813/15000, loss = 1.3836913108825684
37268 2023-02-16,23:39:19.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4577999114990234
38255 2023-02-16,23:39:19.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621677160263062
37393 2023-02-16,23:39:19.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4104033708572388
38108 2023-02-16,23:39:19.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3797513246536255
36814 2023-02-16,23:39:19.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 902/15000, loss = 1.3789559602737427
37035 2023-02-16,23:39:19.883 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.3704100847244263
37533 2023-02-16,23:39:19.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4456815719604492
37757 2023-02-16,23:39:19.892 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3800071477890015
37152 2023-02-16,23:39:19.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.380763292312622
38487 2023-02-16,23:39:19.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 374/15000, loss = 1.3472578525543213
37862 2023-02-16,23:39:19.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4068830013275146
37983 2023-02-16,23:39:20.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.331228256225586
38362 2023-02-16,23:39:20.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4085298776626587
36932 2023-02-16,23:39:20.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 814/15000, loss = 1.375794529914856
37268 2023-02-16,23:39:20.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4074788093566895
38255 2023-02-16,23:39:20.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.396209716796875
37393 2023-02-16,23:39:20.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4094747304916382
38108 2023-02-16,23:39:20.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3766010999679565
36814 2023-02-16,23:39:20.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 903/15000, loss = 1.3786911964416504
37035 2023-02-16,23:39:20.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.3939497470855713
37533 2023-02-16,23:39:20.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3767668008804321
37757 2023-02-16,23:39:20.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4244728088378906
37152 2023-02-16,23:39:20.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4039164781570435
38487 2023-02-16,23:39:20.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 375/15000, loss = 1.4079031944274902
37862 2023-02-16,23:39:20.283 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4035072326660156
37983 2023-02-16,23:39:20.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3720282316207886
38362 2023-02-16,23:39:20.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4159057140350342
36932 2023-02-16,23:39:20.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 815/15000, loss = 1.4128763675689697
37268 2023-02-16,23:39:20.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4088534116744995
38255 2023-02-16,23:39:20.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3788087368011475
37393 2023-02-16,23:39:20.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4200520515441895
38108 2023-02-16,23:39:20.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4136451482772827
36814 2023-02-16,23:39:20.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 904/15000, loss = 1.4448723793029785
37035 2023-02-16,23:39:20.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.4311656951904297
37533 2023-02-16,23:39:20.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3544608354568481
37757 2023-02-16,23:39:20.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3783711194992065
37152 2023-02-16,23:39:20.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.368605136871338
38487 2023-02-16,23:39:20.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 376/15000, loss = 1.4023067951202393
37862 2023-02-16,23:39:20.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3978195190429688
37983 2023-02-16,23:39:20.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3632596731185913
38362 2023-02-16,23:39:20.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.392143964767456
36932 2023-02-16,23:39:20.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 816/15000, loss = 1.4230366945266724
37268 2023-02-16,23:39:20.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.371726393699646
38255 2023-02-16,23:39:20.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3943514823913574
37393 2023-02-16,23:39:20.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4224997758865356
38108 2023-02-16,23:39:20.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.381202220916748
36814 2023-02-16,23:39:20.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 905/15000, loss = 1.3587512969970703
37035 2023-02-16,23:39:20.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.4002466201782227
37533 2023-02-16,23:39:20.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4002443552017212
37757 2023-02-16,23:39:20.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3980814218521118
37152 2023-02-16,23:39:20.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.3160380125045776
38487 2023-02-16,23:39:20.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 377/15000, loss = 1.391068696975708
37862 2023-02-16,23:39:20.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4438493251800537
37983 2023-02-16,23:39:21.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.371509313583374
38362 2023-02-16,23:39:21.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3322988748550415
36932 2023-02-16,23:39:21.089 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 817/15000, loss = 1.4209654331207275
37268 2023-02-16,23:39:21.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.373923420906067
38255 2023-02-16,23:39:21.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4194639921188354
37393 2023-02-16,23:39:21.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.40138840675354
38108 2023-02-16,23:39:21.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4489753246307373
36814 2023-02-16,23:39:21.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 906/15000, loss = 1.4019708633422852
37035 2023-02-16,23:39:21.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.352419137954712
37533 2023-02-16,23:39:21.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3608336448669434
37757 2023-02-16,23:39:21.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.356095790863037
37152 2023-02-16,23:39:21.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3979686498641968
38487 2023-02-16,23:39:21.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 378/15000, loss = 1.4033793210983276
37862 2023-02-16,23:39:21.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3984276056289673
37983 2023-02-16,23:39:21.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4469084739685059
38362 2023-02-16,23:39:21.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.390123724937439
36932 2023-02-16,23:39:21.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 818/15000, loss = 1.388404130935669
37268 2023-02-16,23:39:21.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4066191911697388
38255 2023-02-16,23:39:21.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4016096591949463
37393 2023-02-16,23:39:21.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.404971718788147
38108 2023-02-16,23:39:21.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4463361501693726
36814 2023-02-16,23:39:21.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 907/15000, loss = 1.406619668006897
37035 2023-02-16,23:39:21.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.4203609228134155
37533 2023-02-16,23:39:21.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3797640800476074
37757 2023-02-16,23:39:21.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.425898551940918
37152 2023-02-16,23:39:21.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3800103664398193
38487 2023-02-16,23:39:21.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 379/15000, loss = 1.3370310068130493
37862 2023-02-16,23:39:21.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3509345054626465
37983 2023-02-16,23:39:21.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3994145393371582
38362 2023-02-16,23:39:21.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3906704187393188
36932 2023-02-16,23:39:21.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 819/15000, loss = 1.418137550354004
37268 2023-02-16,23:39:21.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3988028764724731
38255 2023-02-16,23:39:21.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3918497562408447
37393 2023-02-16,23:39:21.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3671919107437134
38108 2023-02-16,23:39:21.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3450812101364136
36814 2023-02-16,23:39:21.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 908/15000, loss = 1.4246187210083008
37035 2023-02-16,23:39:21.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3943145275115967
37533 2023-02-16,23:39:21.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4412498474121094
37757 2023-02-16,23:39:21.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.383428931236267
37152 2023-02-16,23:39:21.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.440401554107666
38487 2023-02-16,23:39:21.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 380/15000, loss = 1.3741408586502075
37862 2023-02-16,23:39:21.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3495019674301147
37983 2023-02-16,23:39:22.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4203904867172241
38362 2023-02-16,23:39:22.101 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4260752201080322
36932 2023-02-16,23:39:22.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 820/15000, loss = 1.3875046968460083
37268 2023-02-16,23:39:22.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3812034130096436
38255 2023-02-16,23:39:22.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.399100661277771
37393 2023-02-16,23:39:22.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4171149730682373
38108 2023-02-16,23:39:22.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.406862735748291
36814 2023-02-16,23:39:22.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 909/15000, loss = 1.430572509765625
37035 2023-02-16,23:39:22.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.3348345756530762
37533 2023-02-16,23:39:22.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3797680139541626
37757 2023-02-16,23:39:22.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3964813947677612
37152 2023-02-16,23:39:22.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3126953840255737
38487 2023-02-16,23:39:22.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 381/15000, loss = 1.3834104537963867
37862 2023-02-16,23:39:22.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4207843542099
37983 2023-02-16,23:39:22.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.378220558166504
38362 2023-02-16,23:39:22.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4031778573989868
36932 2023-02-16,23:39:22.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 821/15000, loss = 1.3621408939361572
37268 2023-02-16,23:39:22.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4290356636047363
38255 2023-02-16,23:39:22.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3857593536376953
37393 2023-02-16,23:39:22.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.435055136680603
38108 2023-02-16,23:39:22.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.470946192741394
36814 2023-02-16,23:39:22.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 910/15000, loss = 1.4187686443328857
37035 2023-02-16,23:39:22.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.3668334484100342
37533 2023-02-16,23:39:22.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4116555452346802
37757 2023-02-16,23:39:22.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3959401845932007
37152 2023-02-16,23:39:22.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3363176584243774
38487 2023-02-16,23:39:22.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 382/15000, loss = 1.3731037378311157
37862 2023-02-16,23:39:22.660 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4321691989898682
37983 2023-02-16,23:39:22.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3665592670440674
38362 2023-02-16,23:39:22.782 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4097864627838135
36932 2023-02-16,23:39:22.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 822/15000, loss = 1.4138966798782349
37268 2023-02-16,23:39:22.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3898142576217651
38255 2023-02-16,23:39:22.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3712509870529175
37393 2023-02-16,23:39:22.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4183568954467773
38108 2023-02-16,23:39:22.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3875923156738281
36814 2023-02-16,23:39:22.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 911/15000, loss = 1.4090795516967773
37035 2023-02-16,23:39:22.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3875818252563477
37533 2023-02-16,23:39:22.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3898594379425049
37757 2023-02-16,23:39:22.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3868920803070068
37152 2023-02-16,23:39:22.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3692024946212769
38487 2023-02-16,23:39:22.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 383/15000, loss = 1.3825820684432983
37862 2023-02-16,23:39:22.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3882966041564941
37983 2023-02-16,23:39:23.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3888212442398071
38362 2023-02-16,23:39:23.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.390208125114441
36932 2023-02-16,23:39:23.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 823/15000, loss = 1.4321203231811523
37268 2023-02-16,23:39:23.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4104180335998535
38255 2023-02-16,23:39:23.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4070985317230225
37393 2023-02-16,23:39:23.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4098119735717773
38108 2023-02-16,23:39:23.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3815767765045166
36814 2023-02-16,23:39:23.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 912/15000, loss = 1.40851891040802
37035 2023-02-16,23:39:23.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.3586313724517822
37533 2023-02-16,23:39:23.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3869571685791016
37757 2023-02-16,23:39:23.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4073210954666138
37152 2023-02-16,23:39:23.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3385580778121948
38487 2023-02-16,23:39:23.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 384/15000, loss = 1.4124743938446045
37862 2023-02-16,23:39:23.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4323203563690186
37983 2023-02-16,23:39:23.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.362418532371521
38362 2023-02-16,23:39:23.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3825478553771973
36932 2023-02-16,23:39:23.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 824/15000, loss = 1.4093223810195923
37268 2023-02-16,23:39:23.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4297934770584106
38255 2023-02-16,23:39:23.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3939695358276367
37393 2023-02-16,23:39:23.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3796128034591675
38108 2023-02-16,23:39:23.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.401526927947998
36814 2023-02-16,23:39:23.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 913/15000, loss = 1.3671284914016724
37035 2023-02-16,23:39:23.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.388381004333496
37533 2023-02-16,23:39:23.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3801265954971313
37757 2023-02-16,23:39:23.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4060853719711304
37152 2023-02-16,23:39:23.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4052765369415283
38487 2023-02-16,23:39:23.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 385/15000, loss = 1.3562426567077637
37862 2023-02-16,23:39:23.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.357654094696045
37983 2023-02-16,23:39:23.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3724992275238037
38362 2023-02-16,23:39:23.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3499270677566528
36932 2023-02-16,23:39:23.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 825/15000, loss = 1.400232195854187
37268 2023-02-16,23:39:23.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.362941026687622
38255 2023-02-16,23:39:23.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3739144802093506
37393 2023-02-16,23:39:23.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.388911247253418
38108 2023-02-16,23:39:23.905 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3742119073867798
36814 2023-02-16,23:39:23.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 914/15000, loss = 1.401119351387024
37035 2023-02-16,23:39:23.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.3389527797698975
37533 2023-02-16,23:39:23.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3860260248184204
37757 2023-02-16,23:39:23.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4150731563568115
37152 2023-02-16,23:39:23.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4231362342834473
38487 2023-02-16,23:39:24.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 386/15000, loss = 1.4542958736419678
37862 2023-02-16,23:39:24.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3847391605377197
37983 2023-02-16,23:39:24.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.355449914932251
38362 2023-02-16,23:39:24.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3817006349563599
36932 2023-02-16,23:39:24.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 826/15000, loss = 1.4137946367263794
37268 2023-02-16,23:39:24.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3679797649383545
38255 2023-02-16,23:39:24.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3550670146942139
37393 2023-02-16,23:39:24.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3808197975158691
38108 2023-02-16,23:39:24.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3937547206878662
36814 2023-02-16,23:39:24.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 915/15000, loss = 1.3794636726379395
37035 2023-02-16,23:39:24.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.395293951034546
37533 2023-02-16,23:39:24.314 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4390835762023926
37757 2023-02-16,23:39:24.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4134976863861084
37152 2023-02-16,23:39:24.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.4144822359085083
38487 2023-02-16,23:39:24.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 387/15000, loss = 1.3826522827148438
37862 2023-02-16,23:39:24.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.379030704498291
37983 2023-02-16,23:39:24.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3727961778640747
38362 2023-02-16,23:39:24.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.402221441268921
36932 2023-02-16,23:39:24.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 827/15000, loss = 1.3995108604431152
37268 2023-02-16,23:39:24.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4056358337402344
38255 2023-02-16,23:39:24.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3843709230422974
37393 2023-02-16,23:39:24.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3709173202514648
38108 2023-02-16,23:39:24.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4187239408493042
36814 2023-02-16,23:39:24.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 916/15000, loss = 1.4574401378631592
37533 2023-02-16,23:39:24.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3809763193130493
37757 2023-02-16,23:39:24.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4247443675994873
37152 2023-02-16,23:39:24.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3901947736740112
38487 2023-02-16,23:39:24.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 388/15000, loss = 1.4072067737579346
37035 2023-02-16,23:39:24.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.3892629146575928
37862 2023-02-16,23:39:24.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.378261685371399
37983 2023-02-16,23:39:24.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3621156215667725
38362 2023-02-16,23:39:24.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3621940612792969
36932 2023-02-16,23:39:24.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 828/15000, loss = 1.4066216945648193
37268 2023-02-16,23:39:24.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.324276089668274
38255 2023-02-16,23:39:24.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4394068717956543
37393 2023-02-16,23:39:24.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.422568440437317
38108 2023-02-16,23:39:24.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4158649444580078
36814 2023-02-16,23:39:24.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 917/15000, loss = 1.3721940517425537
37533 2023-02-16,23:39:24.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3943157196044922
37757 2023-02-16,23:39:24.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3798370361328125
37152 2023-02-16,23:39:25.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3588440418243408
38487 2023-02-16,23:39:25.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 389/15000, loss = 1.3953114748001099
37035 2023-02-16,23:39:25.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.3565665483474731
37862 2023-02-16,23:39:25.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3930381536483765
37983 2023-02-16,23:39:25.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4126464128494263
38362 2023-02-16,23:39:25.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.3962284326553345
36932 2023-02-16,23:39:25.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 829/15000, loss = 1.3963801860809326
37268 2023-02-16,23:39:25.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3797403573989868
38255 2023-02-16,23:39:25.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3718228340148926
37393 2023-02-16,23:39:25.257 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3965644836425781
38108 2023-02-16,23:39:25.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.38071608543396
36814 2023-02-16,23:39:25.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 918/15000, loss = 1.3426706790924072
37533 2023-02-16,23:39:25.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3748990297317505
37757 2023-02-16,23:39:25.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4087494611740112
37152 2023-02-16,23:39:25.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4070916175842285
38487 2023-02-16,23:39:25.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 390/15000, loss = 1.4256173372268677
37035 2023-02-16,23:39:25.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.3708059787750244
37862 2023-02-16,23:39:25.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3941798210144043
37983 2023-02-16,23:39:25.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4226267337799072
38362 2023-02-16,23:39:25.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3788317441940308
36932 2023-02-16,23:39:25.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 830/15000, loss = 1.4169914722442627
37268 2023-02-16,23:39:25.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3664357662200928
38255 2023-02-16,23:39:25.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3940253257751465
37393 2023-02-16,23:39:25.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4006786346435547
38108 2023-02-16,23:39:25.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.408506989479065
36814 2023-02-16,23:39:25.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 919/15000, loss = 1.379411220550537
37533 2023-02-16,23:39:25.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3743821382522583
37757 2023-02-16,23:39:25.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.377812147140503
37152 2023-02-16,23:39:25.705 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.4000498056411743
38487 2023-02-16,23:39:25.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 391/15000, loss = 1.4003159999847412
37035 2023-02-16,23:39:25.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.3940366506576538
37862 2023-02-16,23:39:25.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3666020631790161
37983 2023-02-16,23:39:25.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4431153535842896
38362 2023-02-16,23:39:25.847 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.394413709640503
36932 2023-02-16,23:39:25.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 831/15000, loss = 1.367623209953308
37268 2023-02-16,23:39:25.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.41606605052948
38255 2023-02-16,23:39:25.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3245947360992432
37393 2023-02-16,23:39:25.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3676470518112183
38108 2023-02-16,23:39:25.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4648281335830688
36814 2023-02-16,23:39:25.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 920/15000, loss = 1.4560428857803345
37533 2023-02-16,23:39:26.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3880608081817627
37757 2023-02-16,23:39:26.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3749635219573975
38487 2023-02-16,23:39:26.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 392/15000, loss = 1.3967785835266113
37035 2023-02-16,23:39:26.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.4363850355148315
37152 2023-02-16,23:39:26.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3494555950164795
37862 2023-02-16,23:39:26.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4106941223144531
37983 2023-02-16,23:39:26.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3696986436843872
38362 2023-02-16,23:39:26.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4193782806396484
36932 2023-02-16,23:39:26.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 832/15000, loss = 1.4444061517715454
37268 2023-02-16,23:39:26.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4243147373199463
38255 2023-02-16,23:39:26.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3725665807724
37393 2023-02-16,23:39:26.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.371302843093872
38108 2023-02-16,23:39:26.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.438014030456543
36814 2023-02-16,23:39:26.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 921/15000, loss = 1.37959623336792
37533 2023-02-16,23:39:26.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3909106254577637
37757 2023-02-16,23:39:26.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3815243244171143
38487 2023-02-16,23:39:26.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 393/15000, loss = 1.4240014553070068
37035 2023-02-16,23:39:26.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.4275832176208496
37152 2023-02-16,23:39:26.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3744544982910156
37862 2023-02-16,23:39:26.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.385871410369873
37983 2023-02-16,23:39:26.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.39752197265625
38362 2023-02-16,23:39:26.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4015668630599976
36932 2023-02-16,23:39:26.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 833/15000, loss = 1.35831880569458
37268 2023-02-16,23:39:26.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3295834064483643
38255 2023-02-16,23:39:26.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4171937704086304
38108 2023-02-16,23:39:26.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3955954313278198
37393 2023-02-16,23:39:26.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.3805899620056152
36814 2023-02-16,23:39:26.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 922/15000, loss = 1.3466472625732422
37533 2023-02-16,23:39:26.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3835456371307373
37757 2023-02-16,23:39:26.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3256193399429321
38487 2023-02-16,23:39:26.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 394/15000, loss = 1.3877925872802734
37035 2023-02-16,23:39:26.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 805/15000, loss = 1.3871067762374878
37152 2023-02-16,23:39:26.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.372739553451538
37862 2023-02-16,23:39:26.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.395893931388855
37983 2023-02-16,23:39:26.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4141712188720703
38362 2023-02-16,23:39:26.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.391903281211853
36932 2023-02-16,23:39:26.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 834/15000, loss = 1.372876524925232
37268 2023-02-16,23:39:26.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4472309350967407
38255 2023-02-16,23:39:26.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3448145389556885
38108 2023-02-16,23:39:26.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3665555715560913
37393 2023-02-16,23:39:26.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3990850448608398
36814 2023-02-16,23:39:27.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 923/15000, loss = 1.3903402090072632
37533 2023-02-16,23:39:27.040 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3631510734558105
37757 2023-02-16,23:39:27.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3787299394607544
38487 2023-02-16,23:39:27.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 395/15000, loss = 1.3841075897216797
37035 2023-02-16,23:39:27.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 806/15000, loss = 1.3810749053955078
37152 2023-02-16,23:39:27.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.379091739654541
37862 2023-02-16,23:39:27.093 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4012466669082642
37983 2023-02-16,23:39:27.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4069043397903442
38362 2023-02-16,23:39:27.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3990697860717773
36932 2023-02-16,23:39:27.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 835/15000, loss = 1.3611342906951904
37268 2023-02-16,23:39:27.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3767863512039185
38255 2023-02-16,23:39:27.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.376065969467163
38108 2023-02-16,23:39:27.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3540388345718384
37393 2023-02-16,23:39:27.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.459261178970337
36814 2023-02-16,23:39:27.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 924/15000, loss = 1.410233974456787
37533 2023-02-16,23:39:27.382 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3674018383026123
37757 2023-02-16,23:39:27.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3482625484466553
38487 2023-02-16,23:39:27.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 396/15000, loss = 1.4235095977783203
37035 2023-02-16,23:39:27.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 807/15000, loss = 1.369541883468628
37152 2023-02-16,23:39:27.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.399726152420044
37862 2023-02-16,23:39:27.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4305224418640137
37983 2023-02-16,23:39:27.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4036381244659424
38362 2023-02-16,23:39:27.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.3858009576797485
36932 2023-02-16,23:39:27.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 836/15000, loss = 1.4324219226837158
37268 2023-02-16,23:39:27.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.400675892829895
38255 2023-02-16,23:39:27.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3798942565917969
38108 2023-02-16,23:39:27.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.390373706817627
36814 2023-02-16,23:39:27.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 925/15000, loss = 1.3679982423782349
37393 2023-02-16,23:39:27.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4082481861114502
37533 2023-02-16,23:39:27.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.351340413093567
37757 2023-02-16,23:39:27.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4017865657806396
38487 2023-02-16,23:39:27.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 397/15000, loss = 1.4108610153198242
37035 2023-02-16,23:39:27.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 808/15000, loss = 1.4163151979446411
37862 2023-02-16,23:39:27.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3799183368682861
37152 2023-02-16,23:39:27.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4567371606826782
37983 2023-02-16,23:39:27.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.39772629737854
38362 2023-02-16,23:39:27.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.371291160583496
36932 2023-02-16,23:39:27.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 837/15000, loss = 1.344095230102539
37268 2023-02-16,23:39:27.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4358129501342773
38255 2023-02-16,23:39:27.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3766403198242188
38108 2023-02-16,23:39:27.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.331345796585083
36814 2023-02-16,23:39:28.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 926/15000, loss = 1.3898361921310425
37393 2023-02-16,23:39:28.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4096410274505615
37533 2023-02-16,23:39:28.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.411949634552002
37757 2023-02-16,23:39:28.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4202189445495605
38487 2023-02-16,23:39:28.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 398/15000, loss = 1.4082666635513306
37035 2023-02-16,23:39:28.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 809/15000, loss = 1.4134061336517334
37152 2023-02-16,23:39:28.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.3417426347732544
37862 2023-02-16,23:39:28.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4247136116027832
37983 2023-02-16,23:39:28.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4439774751663208
38362 2023-02-16,23:39:28.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4070525169372559
36932 2023-02-16,23:39:28.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 838/15000, loss = 1.3627357482910156
37268 2023-02-16,23:39:28.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4210848808288574
38255 2023-02-16,23:39:28.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.413637638092041
38108 2023-02-16,23:39:28.341 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3720340728759766
36814 2023-02-16,23:39:28.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 927/15000, loss = 1.3902605772018433
37393 2023-02-16,23:39:28.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3721444606781006
37533 2023-02-16,23:39:28.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.381817102432251
37757 2023-02-16,23:39:28.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4114991426467896
37035 2023-02-16,23:39:28.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 810/15000, loss = 1.3890442848205566
37862 2023-02-16,23:39:28.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3782954216003418
37152 2023-02-16,23:39:28.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3838962316513062
38487 2023-02-16,23:39:28.500 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 399/15000, loss = 1.4025139808654785
37983 2023-02-16,23:39:28.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3985401391983032
38362 2023-02-16,23:39:28.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3939998149871826
36932 2023-02-16,23:39:28.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 839/15000, loss = 1.4078741073608398
37268 2023-02-16,23:39:28.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4021859169006348
38255 2023-02-16,23:39:28.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.380988359451294
38108 2023-02-16,23:39:28.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3633322715759277
36814 2023-02-16,23:39:28.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 928/15000, loss = 1.4040411710739136
37393 2023-02-16,23:39:28.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.373094916343689
37533 2023-02-16,23:39:28.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4360038042068481
37757 2023-02-16,23:39:28.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3951900005340576
37035 2023-02-16,23:39:28.795 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 811/15000, loss = 1.2949658632278442
37152 2023-02-16,23:39:28.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3966418504714966
37862 2023-02-16,23:39:28.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.398097276687622
38487 2023-02-16,23:39:28.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 400/15000, loss = 1.418810248374939
37983 2023-02-16,23:39:28.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.350842833518982
38362 2023-02-16,23:39:28.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.373974323272705
36932 2023-02-16,23:39:28.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 840/15000, loss = 1.4112052917480469
37268 2023-02-16,23:39:28.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4473602771759033
38255 2023-02-16,23:39:28.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4488904476165771
38108 2023-02-16,23:39:29.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3715242147445679
36814 2023-02-16,23:39:29.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 929/15000, loss = 1.3213406801223755
37757 2023-02-16,23:39:29.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3947162628173828
37393 2023-02-16,23:39:29.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4066121578216553
37533 2023-02-16,23:39:29.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4322888851165771
37035 2023-02-16,23:39:29.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 812/15000, loss = 1.3423950672149658
37152 2023-02-16,23:39:29.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4048641920089722
37862 2023-02-16,23:39:29.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3561441898345947
38487 2023-02-16,23:39:29.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 401/15000, loss = 1.3663991689682007
37983 2023-02-16,23:39:29.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3495442867279053
38362 2023-02-16,23:39:29.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3550307750701904
36932 2023-02-16,23:39:29.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 841/15000, loss = 1.390946865081787
37268 2023-02-16,23:39:29.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4032409191131592
38255 2023-02-16,23:39:29.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4464483261108398
38108 2023-02-16,23:39:29.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4467456340789795
36814 2023-02-16,23:39:29.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 930/15000, loss = 1.3563730716705322
37757 2023-02-16,23:39:29.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3925342559814453
37393 2023-02-16,23:39:29.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3977820873260498
37533 2023-02-16,23:39:29.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4292163848876953
37035 2023-02-16,23:39:29.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 813/15000, loss = 1.3832926750183105
37152 2023-02-16,23:39:29.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.379945993423462
37862 2023-02-16,23:39:29.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4259663820266724
38487 2023-02-16,23:39:29.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 402/15000, loss = 1.4100487232208252
37983 2023-02-16,23:39:29.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4209072589874268
38362 2023-02-16,23:39:29.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3844313621520996
36932 2023-02-16,23:39:29.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 842/15000, loss = 1.3825535774230957
37268 2023-02-16,23:39:29.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3673095703125
38255 2023-02-16,23:39:29.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3449534177780151
38108 2023-02-16,23:39:29.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3993868827819824
36814 2023-02-16,23:39:29.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 931/15000, loss = 1.4325180053710938
37757 2023-02-16,23:39:29.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4456281661987305
37393 2023-02-16,23:39:29.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3812041282653809
37533 2023-02-16,23:39:29.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3665920495986938
37035 2023-02-16,23:39:29.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 814/15000, loss = 1.3753925561904907
37152 2023-02-16,23:39:29.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.365830898284912
37862 2023-02-16,23:39:29.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3836370706558228
38487 2023-02-16,23:39:29.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 403/15000, loss = 1.4083954095840454
37983 2023-02-16,23:39:29.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4323047399520874
38362 2023-02-16,23:39:29.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4395545721054077
36932 2023-02-16,23:39:29.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 843/15000, loss = 1.350173830986023
37268 2023-02-16,23:39:29.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3715389966964722
38255 2023-02-16,23:39:30.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4069021940231323
38108 2023-02-16,23:39:30.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4205377101898193
36814 2023-02-16,23:39:30.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 932/15000, loss = 1.365029215812683
37757 2023-02-16,23:39:30.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3759291172027588
37393 2023-02-16,23:39:30.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4293127059936523
37533 2023-02-16,23:39:30.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3776575326919556
37035 2023-02-16,23:39:30.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 815/15000, loss = 1.412885069847107
37152 2023-02-16,23:39:30.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4229501485824585
37862 2023-02-16,23:39:30.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3966379165649414
38487 2023-02-16,23:39:30.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 404/15000, loss = 1.4159038066864014
37983 2023-02-16,23:39:30.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.388271450996399
38362 2023-02-16,23:39:30.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3718483448028564
36932 2023-02-16,23:39:30.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 844/15000, loss = 1.3777812719345093
37268 2023-02-16,23:39:30.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.416508674621582
38255 2023-02-16,23:39:30.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.471043348312378
38108 2023-02-16,23:39:30.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3783420324325562
36814 2023-02-16,23:39:30.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 933/15000, loss = 1.3840724229812622
37757 2023-02-16,23:39:30.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3539201021194458
37393 2023-02-16,23:39:30.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3910930156707764
37533 2023-02-16,23:39:30.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.419247031211853
37035 2023-02-16,23:39:30.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 816/15000, loss = 1.42195725440979
37862 2023-02-16,23:39:30.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3960002660751343
37152 2023-02-16,23:39:30.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4166840314865112
38487 2023-02-16,23:39:30.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 405/15000, loss = 1.3920987844467163
37983 2023-02-16,23:39:30.584 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4323573112487793
38362 2023-02-16,23:39:30.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3941090106964111
36932 2023-02-16,23:39:30.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 845/15000, loss = 1.3636140823364258
37268 2023-02-16,23:39:30.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3863465785980225
38255 2023-02-16,23:39:30.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3876020908355713
38108 2023-02-16,23:39:30.782 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.366618037223816
36814 2023-02-16,23:39:30.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 934/15000, loss = 1.4332822561264038
37757 2023-02-16,23:39:30.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4011573791503906
37393 2023-02-16,23:39:30.829 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4111024141311646
37533 2023-02-16,23:39:30.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3478164672851562
37035 2023-02-16,23:39:30.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 817/15000, loss = 1.4184998273849487
37862 2023-02-16,23:39:30.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3870441913604736
37152 2023-02-16,23:39:30.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.406733512878418
38487 2023-02-16,23:39:30.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 406/15000, loss = 1.3324309587478638
37983 2023-02-16,23:39:30.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3575090169906616
38362 2023-02-16,23:39:30.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3245881795883179
36932 2023-02-16,23:39:30.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 846/15000, loss = 1.4299887418746948
37268 2023-02-16,23:39:31.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4191194772720337
38255 2023-02-16,23:39:31.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.381585955619812
38108 2023-02-16,23:39:31.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3888673782348633
36814 2023-02-16,23:39:31.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 935/15000, loss = 1.4075660705566406
37757 2023-02-16,23:39:31.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3604135513305664
37393 2023-02-16,23:39:31.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4314379692077637
37533 2023-02-16,23:39:31.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3324999809265137
37035 2023-02-16,23:39:31.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 818/15000, loss = 1.3859479427337646
37862 2023-02-16,23:39:31.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4074721336364746
37152 2023-02-16,23:39:31.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3703292608261108
38487 2023-02-16,23:39:31.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 407/15000, loss = 1.3900963068008423
37983 2023-02-16,23:39:31.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3846848011016846
38362 2023-02-16,23:39:31.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3727482557296753
36932 2023-02-16,23:39:31.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 847/15000, loss = 1.4215720891952515
37268 2023-02-16,23:39:31.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4082460403442383
38255 2023-02-16,23:39:31.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4015549421310425
38108 2023-02-16,23:39:31.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3622286319732666
36814 2023-02-16,23:39:31.476 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 936/15000, loss = 1.384501338005066
37757 2023-02-16,23:39:31.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3796358108520508
37533 2023-02-16,23:39:31.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.415367841720581
37035 2023-02-16,23:39:31.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 819/15000, loss = 1.4177370071411133
37393 2023-02-16,23:39:31.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3637923002243042
37862 2023-02-16,23:39:31.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4060090780258179
38487 2023-02-16,23:39:31.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 408/15000, loss = 1.3908244371414185
37152 2023-02-16,23:39:31.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3283995389938354
37983 2023-02-16,23:39:31.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3790171146392822
38362 2023-02-16,23:39:31.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4172669649124146
36932 2023-02-16,23:39:31.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 848/15000, loss = 1.4090182781219482
37268 2023-02-16,23:39:31.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4063912630081177
38255 2023-02-16,23:39:31.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3742146492004395
38108 2023-02-16,23:39:31.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3725080490112305
36814 2023-02-16,23:39:31.817 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 937/15000, loss = 1.3814010620117188
37757 2023-02-16,23:39:31.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4418457746505737
37533 2023-02-16,23:39:31.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4012482166290283
37035 2023-02-16,23:39:31.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 820/15000, loss = 1.3858884572982788
37393 2023-02-16,23:39:31.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3679020404815674
37862 2023-02-16,23:39:31.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.415040373802185
38487 2023-02-16,23:39:31.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 409/15000, loss = 1.4260541200637817
37152 2023-02-16,23:39:31.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.3638253211975098
37983 2023-02-16,23:39:31.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3780853748321533
38362 2023-02-16,23:39:31.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.344882607460022
36932 2023-02-16,23:39:31.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 849/15000, loss = 1.4025784730911255
37268 2023-02-16,23:39:32.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3894867897033691
38255 2023-02-16,23:39:32.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3937736749649048
36814 2023-02-16,23:39:32.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 938/15000, loss = 1.4403330087661743
37757 2023-02-16,23:39:32.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3801507949829102
38108 2023-02-16,23:39:32.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3554269075393677
37533 2023-02-16,23:39:32.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3719067573547363
37035 2023-02-16,23:39:32.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 821/15000, loss = 1.3595606088638306
37393 2023-02-16,23:39:32.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4080637693405151
37862 2023-02-16,23:39:32.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.413649559020996
38487 2023-02-16,23:39:32.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 410/15000, loss = 1.4031238555908203
37152 2023-02-16,23:39:32.283 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.3762370347976685
37983 2023-02-16,23:39:32.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3930513858795166
38362 2023-02-16,23:39:32.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.376203179359436
36932 2023-02-16,23:39:32.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 850/15000, loss = 1.3929154872894287
37268 2023-02-16,23:39:32.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4103246927261353
38255 2023-02-16,23:39:32.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4187877178192139
36814 2023-02-16,23:39:32.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 939/15000, loss = 1.3855202198028564
37757 2023-02-16,23:39:32.515 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4122440814971924
38108 2023-02-16,23:39:32.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372862696647644
40293 2023-02-16,23:39:32.528 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
40293 2023-02-16,23:39:32.529 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
40293 2023-02-16,23:39:32.529 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2']
40293 2023-02-16,23:39:32.531 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 21266692}
37533 2023-02-16,23:39:32.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4172399044036865
37035 2023-02-16,23:39:32.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 822/15000, loss = 1.4148626327514648
37393 2023-02-16,23:39:32.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3260014057159424
37862 2023-02-16,23:39:32.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.424906849861145
38487 2023-02-16,23:39:32.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 411/15000, loss = 1.4096988439559937
37152 2023-02-16,23:39:32.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.396398663520813
37983 2023-02-16,23:39:32.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3940644264221191
38362 2023-02-16,23:39:32.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3798484802246094
36932 2023-02-16,23:39:32.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 851/15000, loss = 1.4516198635101318
37268 2023-02-16,23:39:32.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3863816261291504
38255 2023-02-16,23:39:32.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4159413576126099
36814 2023-02-16,23:39:32.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 940/15000, loss = 1.399902582168579
37757 2023-02-16,23:39:32.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3891547918319702
38108 2023-02-16,23:39:32.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3621519804000854
37533 2023-02-16,23:39:32.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.432454228401184
37035 2023-02-16,23:39:32.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 823/15000, loss = 1.4313249588012695
37393 2023-02-16,23:39:32.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.379400610923767
37862 2023-02-16,23:39:32.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3797345161437988
38487 2023-02-16,23:39:32.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 412/15000, loss = 1.390222191810608
37152 2023-02-16,23:39:32.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.362783670425415
37983 2023-02-16,23:39:32.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.366640329360962
38362 2023-02-16,23:39:33.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3765692710876465
36932 2023-02-16,23:39:33.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 852/15000, loss = 1.3954342603683472
37268 2023-02-16,23:39:33.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3314497470855713
38255 2023-02-16,23:39:33.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3807485103607178
36814 2023-02-16,23:39:33.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 941/15000, loss = 1.3991416692733765
38108 2023-02-16,23:39:33.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.412532091140747
37757 2023-02-16,23:39:33.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3877995014190674
37035 2023-02-16,23:39:33.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 824/15000, loss = 1.4081181287765503
37393 2023-02-16,23:39:33.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3664182424545288
37533 2023-02-16,23:39:33.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.38099205493927
37862 2023-02-16,23:39:33.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4087544679641724
38487 2023-02-16,23:39:33.281 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 413/15000, loss = 1.3826873302459717
37152 2023-02-16,23:39:33.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3590118885040283
37983 2023-02-16,23:39:33.323 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4108229875564575
38362 2023-02-16,23:39:33.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4136953353881836
36932 2023-02-16,23:39:33.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 853/15000, loss = 1.380761742591858
37268 2023-02-16,23:39:33.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4093073606491089
38255 2023-02-16,23:39:33.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.4085570573806763
36814 2023-02-16,23:39:33.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 942/15000, loss = 1.3923249244689941
38108 2023-02-16,23:39:33.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.422667384147644
37757 2023-02-16,23:39:33.572 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3808269500732422
37035 2023-02-16,23:39:33.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 825/15000, loss = 1.3990025520324707
37393 2023-02-16,23:39:33.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4161983728408813
37533 2023-02-16,23:39:33.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.360888957977295
37862 2023-02-16,23:39:33.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.377785563468933
38487 2023-02-16,23:39:33.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 414/15000, loss = 1.3499170541763306
37152 2023-02-16,23:39:33.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.3943641185760498
37983 2023-02-16,23:39:33.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3858284950256348
38362 2023-02-16,23:39:33.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3811352252960205
36932 2023-02-16,23:39:33.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 854/15000, loss = 1.359756350517273
37268 2023-02-16,23:39:33.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3832708597183228
38255 2023-02-16,23:39:33.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.464918851852417
36814 2023-02-16,23:39:33.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 943/15000, loss = 1.479529619216919
38108 2023-02-16,23:39:33.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4429354667663574
37757 2023-02-16,23:39:33.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.386360764503479
37035 2023-02-16,23:39:33.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 826/15000, loss = 1.4118802547454834
37393 2023-02-16,23:39:33.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4262405633926392
37533 2023-02-16,23:39:33.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4229307174682617
37862 2023-02-16,23:39:33.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3749382495880127
38487 2023-02-16,23:39:33.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 415/15000, loss = 1.3817569017410278
37152 2023-02-16,23:39:34.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3621106147766113
37983 2023-02-16,23:39:34.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3958675861358643
38362 2023-02-16,23:39:34.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4490160942077637
36932 2023-02-16,23:39:34.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 855/15000, loss = 1.3748271465301514
37268 2023-02-16,23:39:34.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3769077062606812
38255 2023-02-16,23:39:34.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4380340576171875
36814 2023-02-16,23:39:34.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 944/15000, loss = 1.3548988103866577
38108 2023-02-16,23:39:34.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.369734525680542
37757 2023-02-16,23:39:34.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4401341676712036
37035 2023-02-16,23:39:34.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 827/15000, loss = 1.3968913555145264
37393 2023-02-16,23:39:34.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3307626247406006
37533 2023-02-16,23:39:34.283 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.392167091369629
37862 2023-02-16,23:39:34.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.381392002105713
38487 2023-02-16,23:39:34.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 416/15000, loss = 1.4023065567016602
37152 2023-02-16,23:39:34.342 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.3605073690414429
37983 2023-02-16,23:39:34.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4010465145111084
38362 2023-02-16,23:39:34.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.446446180343628
36932 2023-02-16,23:39:34.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 856/15000, loss = 1.4276188611984253
37268 2023-02-16,23:39:34.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4138537645339966
38255 2023-02-16,23:39:34.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3955551385879517
36814 2023-02-16,23:39:34.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 945/15000, loss = 1.3631768226623535
38108 2023-02-16,23:39:34.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.397438645362854
37035 2023-02-16,23:39:34.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 828/15000, loss = 1.4066256284713745
37393 2023-02-16,23:39:34.625 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4476420879364014
37533 2023-02-16,23:39:34.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3872151374816895
37757 2023-02-16,23:39:34.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3813128471374512
37862 2023-02-16,23:39:34.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3254809379577637
38487 2023-02-16,23:39:34.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 417/15000, loss = 1.3622586727142334
37152 2023-02-16,23:39:34.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.389709711074829
37983 2023-02-16,23:39:34.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4304277896881104
38362 2023-02-16,23:39:34.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.345080018043518
36932 2023-02-16,23:39:34.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 857/15000, loss = 1.390507459640503
37268 2023-02-16,23:39:34.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.366410255432129
38255 2023-02-16,23:39:34.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3665764331817627
40433 2023-02-16,23:39:34.811 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
40433 2023-02-16,23:39:34.816 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
40433 2023-02-16,23:39:34.816 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3']
40433 2023-02-16,23:39:34.819 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 14178820}
36814 2023-02-16,23:39:34.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 946/15000, loss = 1.3672130107879639
38108 2023-02-16,23:39:34.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4141582250595093
37035 2023-02-16,23:39:34.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 829/15000, loss = 1.3961659669876099
37393 2023-02-16,23:39:34.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3768664598464966
37533 2023-02-16,23:39:34.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3225756883621216
37757 2023-02-16,23:39:34.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3951950073242188
37862 2023-02-16,23:39:34.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3789191246032715
38487 2023-02-16,23:39:35.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 418/15000, loss = 1.396268606185913
37152 2023-02-16,23:39:35.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3744401931762695
37983 2023-02-16,23:39:35.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.379899263381958
38362 2023-02-16,23:39:35.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4069780111312866
36932 2023-02-16,23:39:35.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 858/15000, loss = 1.4126389026641846
37268 2023-02-16,23:39:35.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3934837579727173
38255 2023-02-16,23:39:35.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3541492223739624
36814 2023-02-16,23:39:35.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 947/15000, loss = 1.3471500873565674
38108 2023-02-16,23:39:35.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4069017171859741
37035 2023-02-16,23:39:35.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 830/15000, loss = 1.4145698547363281
37393 2023-02-16,23:39:35.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4011520147323608
37533 2023-02-16,23:39:35.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4093726873397827
37862 2023-02-16,23:39:35.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3482109308242798
37757 2023-02-16,23:39:35.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.374957799911499
38487 2023-02-16,23:39:35.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 419/15000, loss = 1.3787983655929565
37152 2023-02-16,23:39:35.380 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4049350023269653
37983 2023-02-16,23:39:35.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.424841046333313
38362 2023-02-16,23:39:35.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.4710248708724976
36932 2023-02-16,23:39:35.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 859/15000, loss = 1.4106303453445435
37268 2023-02-16,23:39:35.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3814959526062012
38255 2023-02-16,23:39:35.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3904885053634644
36814 2023-02-16,23:39:35.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 948/15000, loss = 1.4082987308502197
38108 2023-02-16,23:39:35.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4036918878555298
37035 2023-02-16,23:39:35.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 831/15000, loss = 1.3663958311080933
37393 2023-02-16,23:39:35.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4343225955963135
37533 2023-02-16,23:39:35.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3775460720062256
37862 2023-02-16,23:39:35.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4019001722335815
37757 2023-02-16,23:39:35.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3739144802093506
38487 2023-02-16,23:39:35.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 420/15000, loss = 1.3943970203399658
37152 2023-02-16,23:39:35.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.403860092163086
37983 2023-02-16,23:39:35.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.378246784210205
38362 2023-02-16,23:39:35.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3876367807388306
36932 2023-02-16,23:39:35.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 860/15000, loss = 1.4075508117675781
37268 2023-02-16,23:39:35.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.387163758277893
38255 2023-02-16,23:39:35.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.331363558769226
36814 2023-02-16,23:39:35.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 949/15000, loss = 1.3759294748306274
38108 2023-02-16,23:39:35.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.397615671157837
37035 2023-02-16,23:39:36.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 832/15000, loss = 1.4450173377990723
37393 2023-02-16,23:39:36.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4208897352218628
37533 2023-02-16,23:39:36.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4127670526504517
37862 2023-02-16,23:39:36.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4203613996505737
37757 2023-02-16,23:39:36.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3882668018341064
38487 2023-02-16,23:39:36.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 421/15000, loss = 1.4194605350494385
37152 2023-02-16,23:39:36.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.4073816537857056
37983 2023-02-16,23:39:36.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.398158311843872
38362 2023-02-16,23:39:36.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3817088603973389
36932 2023-02-16,23:39:36.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 861/15000, loss = 1.403366208076477
37268 2023-02-16,23:39:36.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.353751540184021
38255 2023-02-16,23:39:36.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.372145175933838
36814 2023-02-16,23:39:36.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 950/15000, loss = 1.3536268472671509
38108 2023-02-16,23:39:36.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4439375400543213
37035 2023-02-16,23:39:36.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 833/15000, loss = 1.3572568893432617
37393 2023-02-16,23:39:36.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4038978815078735
37533 2023-02-16,23:39:36.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3838077783584595
37862 2023-02-16,23:39:36.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.411581039428711
37757 2023-02-16,23:39:36.380 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.39097261428833
38487 2023-02-16,23:39:36.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 422/15000, loss = 1.4017000198364258
37152 2023-02-16,23:39:36.415 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.350868821144104
37983 2023-02-16,23:39:36.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3560101985931396
38362 2023-02-16,23:39:36.457 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4016025066375732
36932 2023-02-16,23:39:36.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 862/15000, loss = 1.3813742399215698
37268 2023-02-16,23:39:36.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4255574941635132
38255 2023-02-16,23:39:36.534 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3633732795715332
36814 2023-02-16,23:39:36.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 951/15000, loss = 1.4216015338897705
38108 2023-02-16,23:39:36.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3985244035720825
37035 2023-02-16,23:39:36.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 834/15000, loss = 1.371015191078186
37393 2023-02-16,23:39:36.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4456384181976318
37533 2023-02-16,23:39:36.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3610390424728394
37862 2023-02-16,23:39:36.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3951517343521118
37757 2023-02-16,23:39:36.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3833087682724
38487 2023-02-16,23:39:36.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 423/15000, loss = 1.3919402360916138
40554 2023-02-16,23:39:36.733 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
40554 2023-02-16,23:39:36.735 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
40554 2023-02-16,23:39:36.735 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3', '4']
40554 2023-02-16,23:39:36.738 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 7090948}
37152 2023-02-16,23:39:36.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4023158550262451
37983 2023-02-16,23:39:36.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4260995388031006
38362 2023-02-16,23:39:36.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3742527961730957
36932 2023-02-16,23:39:36.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 863/15000, loss = 1.4125667810440063
37268 2023-02-16,23:39:36.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4059380292892456
38255 2023-02-16,23:39:36.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3715256452560425
36814 2023-02-16,23:39:36.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 952/15000, loss = 1.4029110670089722
38108 2023-02-16,23:39:36.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3509446382522583
37035 2023-02-16,23:39:37.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 835/15000, loss = 1.3606387376785278
37393 2023-02-16,23:39:37.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.403862714767456
37533 2023-02-16,23:39:37.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4263123273849487
37862 2023-02-16,23:39:37.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3947727680206299
37757 2023-02-16,23:39:37.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3630919456481934
38487 2023-02-16,23:39:37.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 424/15000, loss = 1.3991408348083496
37152 2023-02-16,23:39:37.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.3599025011062622
37983 2023-02-16,23:39:37.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3836052417755127
38362 2023-02-16,23:39:37.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3938500881195068
36932 2023-02-16,23:39:37.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 864/15000, loss = 1.4084339141845703
37268 2023-02-16,23:39:37.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3637943267822266
38255 2023-02-16,23:39:37.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4468369483947754
36814 2023-02-16,23:39:37.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 953/15000, loss = 1.3824377059936523
38108 2023-02-16,23:39:37.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3495829105377197
37035 2023-02-16,23:39:37.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 836/15000, loss = 1.430380940437317
37393 2023-02-16,23:39:37.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3673996925354004
37533 2023-02-16,23:39:37.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.420722246170044
37862 2023-02-16,23:39:37.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3925230503082275
37757 2023-02-16,23:39:37.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677260875701904
38487 2023-02-16,23:39:37.420 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 425/15000, loss = 1.385847568511963
37152 2023-02-16,23:39:37.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.418544054031372
37983 2023-02-16,23:39:37.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3965319395065308
38362 2023-02-16,23:39:37.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.4188010692596436
36932 2023-02-16,23:39:37.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 865/15000, loss = 1.3893612623214722
37268 2023-02-16,23:39:37.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4191675186157227
38255 2023-02-16,23:39:37.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3993149995803833
36814 2023-02-16,23:39:37.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 954/15000, loss = 1.4092161655426025
38108 2023-02-16,23:39:37.683 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4207730293273926
37035 2023-02-16,23:39:37.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 837/15000, loss = 1.3424150943756104
37393 2023-02-16,23:39:37.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.373457670211792
37533 2023-02-16,23:39:37.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4105783700942993
37862 2023-02-16,23:39:37.731 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4456080198287964
37757 2023-02-16,23:39:37.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3506202697753906
38487 2023-02-16,23:39:37.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 426/15000, loss = 1.3713139295578003
37152 2023-02-16,23:39:37.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.3632758855819702
37983 2023-02-16,23:39:37.828 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3959503173828125
38362 2023-02-16,23:39:37.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4160006046295166
36932 2023-02-16,23:39:37.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 866/15000, loss = 1.3667784929275513
37268 2023-02-16,23:39:37.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3896939754486084
38255 2023-02-16,23:39:37.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.420423984527588
36814 2023-02-16,23:39:38.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 955/15000, loss = 1.3809314966201782
38108 2023-02-16,23:39:38.026 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4323911666870117
37035 2023-02-16,23:39:38.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 838/15000, loss = 1.3607834577560425
37393 2023-02-16,23:39:38.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.416685938835144
37533 2023-02-16,23:39:38.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4519463777542114
37862 2023-02-16,23:39:38.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3760321140289307
37757 2023-02-16,23:39:38.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4130843877792358
38487 2023-02-16,23:39:38.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 427/15000, loss = 1.4071345329284668
37152 2023-02-16,23:39:38.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.3782730102539062
37983 2023-02-16,23:39:38.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3870004415512085
38362 2023-02-16,23:39:38.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.380728006362915
36932 2023-02-16,23:39:38.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 867/15000, loss = 1.3600842952728271
37268 2023-02-16,23:39:38.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4790916442871094
38255 2023-02-16,23:39:38.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3781425952911377
36814 2023-02-16,23:39:38.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 956/15000, loss = 1.4022771120071411
38108 2023-02-16,23:39:38.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3883213996887207
37035 2023-02-16,23:39:38.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 839/15000, loss = 1.4075210094451904
37393 2023-02-16,23:39:38.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3865962028503418
37533 2023-02-16,23:39:38.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4041240215301514
37862 2023-02-16,23:39:38.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.353942632675171
37757 2023-02-16,23:39:38.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3815536499023438
38487 2023-02-16,23:39:38.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 428/15000, loss = 1.3940777778625488
37152 2023-02-16,23:39:38.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.3846862316131592
37983 2023-02-16,23:39:38.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4073965549468994
38362 2023-02-16,23:39:38.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.40854811668396
36932 2023-02-16,23:39:38.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 868/15000, loss = 1.4335116147994995
37268 2023-02-16,23:39:38.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3675761222839355
38255 2023-02-16,23:39:38.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3666205406188965
36814 2023-02-16,23:39:38.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 957/15000, loss = 1.4317750930786133
38108 2023-02-16,23:39:38.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.432142734527588
37035 2023-02-16,23:39:38.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 840/15000, loss = 1.411338448524475
37393 2023-02-16,23:39:38.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4205586910247803
37533 2023-02-16,23:39:38.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3915414810180664
37862 2023-02-16,23:39:38.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4013172388076782
37757 2023-02-16,23:39:38.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4367986917495728
38487 2023-02-16,23:39:38.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 429/15000, loss = 1.3740042448043823
37152 2023-02-16,23:39:38.828 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.4400898218154907
37983 2023-02-16,23:39:38.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4061064720153809
38362 2023-02-16,23:39:38.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.4649896621704102
36932 2023-02-16,23:39:38.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 869/15000, loss = 1.3417123556137085
37268 2023-02-16,23:39:38.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3791285753250122
38255 2023-02-16,23:39:38.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.388826608657837
36814 2023-02-16,23:39:39.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 958/15000, loss = 1.3626296520233154
38108 2023-02-16,23:39:39.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3575509786605835
37035 2023-02-16,23:39:39.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 841/15000, loss = 1.3902387619018555
37393 2023-02-16,23:39:39.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4095726013183594
37533 2023-02-16,23:39:39.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.402900218963623
37862 2023-02-16,23:39:39.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.360515832901001
37757 2023-02-16,23:39:39.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4331648349761963
38487 2023-02-16,23:39:39.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 430/15000, loss = 1.3551576137542725
37152 2023-02-16,23:39:39.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.345603346824646
37983 2023-02-16,23:39:39.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4150514602661133
38362 2023-02-16,23:39:39.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4380497932434082
36932 2023-02-16,23:39:39.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 870/15000, loss = 1.350663661956787
37268 2023-02-16,23:39:39.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.3723233938217163
38255 2023-02-16,23:39:39.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3623301982879639
36814 2023-02-16,23:39:39.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 959/15000, loss = 1.4129211902618408
38108 2023-02-16,23:39:39.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3846832513809204
37035 2023-02-16,23:39:39.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 842/15000, loss = 1.3818094730377197
37393 2023-02-16,23:39:39.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4070957899093628
37533 2023-02-16,23:39:39.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4066417217254639
37862 2023-02-16,23:39:39.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3795667886734009
37757 2023-02-16,23:39:39.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4310654401779175
38487 2023-02-16,23:39:39.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 431/15000, loss = 1.3844558000564575
37152 2023-02-16,23:39:39.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.3755524158477783
37983 2023-02-16,23:39:39.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4136816263198853
38362 2023-02-16,23:39:39.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3956546783447266
36932 2023-02-16,23:39:39.570 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 871/15000, loss = 1.3651726245880127
37268 2023-02-16,23:39:39.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4492541551589966
38255 2023-02-16,23:39:39.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3724794387817383
36814 2023-02-16,23:39:39.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 960/15000, loss = 1.3794838190078735
38108 2023-02-16,23:39:39.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3790470361709595
37035 2023-02-16,23:39:39.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 843/15000, loss = 1.3484148979187012
37393 2023-02-16,23:39:39.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3897864818572998
37533 2023-02-16,23:39:39.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4026912450790405
37862 2023-02-16,23:39:39.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.441937804222107
37757 2023-02-16,23:39:39.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3672518730163574
38487 2023-02-16,23:39:39.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 432/15000, loss = 1.4393219947814941
37152 2023-02-16,23:39:39.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.4267308712005615
37983 2023-02-16,23:39:39.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.424920678138733
38362 2023-02-16,23:39:39.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3666819334030151
36932 2023-02-16,23:39:39.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 872/15000, loss = 1.4013158082962036
37268 2023-02-16,23:39:39.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.373158574104309
38255 2023-02-16,23:39:39.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3555974960327148
36814 2023-02-16,23:39:40.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 961/15000, loss = 1.377685546875
38108 2023-02-16,23:39:40.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3781464099884033
37035 2023-02-16,23:39:40.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 844/15000, loss = 1.3770478963851929
37393 2023-02-16,23:39:40.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4103481769561768
37533 2023-02-16,23:39:40.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.3996180295944214
37862 2023-02-16,23:39:40.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3801075220108032
37757 2023-02-16,23:39:40.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3779711723327637
38487 2023-02-16,23:39:40.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 433/15000, loss = 1.3718891143798828
37152 2023-02-16,23:39:40.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.3796088695526123
37983 2023-02-16,23:39:40.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3796533346176147
38362 2023-02-16,23:39:40.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.354156732559204
36932 2023-02-16,23:39:40.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 873/15000, loss = 1.3813116550445557
37268 2023-02-16,23:39:40.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3797484636306763
38255 2023-02-16,23:39:40.327 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3728208541870117
36814 2023-02-16,23:39:40.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 962/15000, loss = 1.388292670249939
38108 2023-02-16,23:39:40.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3931093215942383
37393 2023-02-16,23:39:40.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3868385553359985
37533 2023-02-16,23:39:40.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3982961177825928
37862 2023-02-16,23:39:40.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4123027324676514
37035 2023-02-16,23:39:40.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 845/15000, loss = 1.3625720739364624
37757 2023-02-16,23:39:40.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4200372695922852
38487 2023-02-16,23:39:40.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 434/15000, loss = 1.3940725326538086
37152 2023-02-16,23:39:40.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.4372601509094238
40677 2023-02-16,23:39:40.571 - {tc_transformer_trainer.py (64)} - train_model(): train_model self.device: cuda:5
40677 2023-02-16,23:39:40.574 - {tc_transformer_trainer.py (232)} - build_optimizer(): warmup steps = 45000
40677 2023-02-16,23:39:40.574 - {tc_transformer_trainer.py (247)} - freeze_model_parameters(): freeze layers: ['e', '0', '1', '2', '3', '4', '5']
40677 2023-02-16,23:39:40.576 - {tc_transformer_trainer.py (266)} - freeze_model_parameters(): {'Total': 66365956, 'Trainable': 3076}
37983 2023-02-16,23:39:40.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4087227582931519
38362 2023-02-16,23:39:40.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3904359340667725
36932 2023-02-16,23:39:40.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 874/15000, loss = 1.413283109664917
37268 2023-02-16,23:39:40.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4032862186431885
38255 2023-02-16,23:39:40.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3621262311935425
36814 2023-02-16,23:39:40.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 963/15000, loss = 1.369676113128662
38108 2023-02-16,23:39:40.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3939217329025269
37393 2023-02-16,23:39:40.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3318730592727661
37533 2023-02-16,23:39:40.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4330698251724243
37862 2023-02-16,23:39:40.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3891255855560303
37035 2023-02-16,23:39:40.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 846/15000, loss = 1.4304038286209106
37757 2023-02-16,23:39:40.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3474498987197876
38487 2023-02-16,23:39:40.874 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 435/15000, loss = 1.3247313499450684
37152 2023-02-16,23:39:40.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3955060243606567
37983 2023-02-16,23:39:40.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3777368068695068
38362 2023-02-16,23:39:40.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3314043283462524
36932 2023-02-16,23:39:40.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 875/15000, loss = 1.4130222797393799
37268 2023-02-16,23:39:40.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3687528371810913
38255 2023-02-16,23:39:41.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.41257905960083
36814 2023-02-16,23:39:41.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 964/15000, loss = 1.3893386125564575
38108 2023-02-16,23:39:41.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3666393756866455
37393 2023-02-16,23:39:41.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.410140037536621
37862 2023-02-16,23:39:41.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3878217935562134
37035 2023-02-16,23:39:41.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 847/15000, loss = 1.4211809635162354
37757 2023-02-16,23:39:41.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3324912786483765
38487 2023-02-16,23:39:41.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 436/15000, loss = 1.3727225065231323
37533 2023-02-16,23:39:41.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.43998122215271
37152 2023-02-16,23:39:41.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.3619860410690308
37983 2023-02-16,23:39:41.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3746806383132935
38362 2023-02-16,23:39:41.290 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3720632791519165
36932 2023-02-16,23:39:41.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 876/15000, loss = 1.3338942527770996
37268 2023-02-16,23:39:41.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.316079020500183
38255 2023-02-16,23:39:41.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.422457218170166
36814 2023-02-16,23:39:41.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 965/15000, loss = 1.3630738258361816
38108 2023-02-16,23:39:41.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.410860538482666
37393 2023-02-16,23:39:41.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3846204280853271
37862 2023-02-16,23:39:41.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3809032440185547
37035 2023-02-16,23:39:41.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 848/15000, loss = 1.4099141359329224
37757 2023-02-16,23:39:41.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4164625406265259
38487 2023-02-16,23:39:41.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 437/15000, loss = 1.4171714782714844
37533 2023-02-16,23:39:41.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4090737104415894
37152 2023-02-16,23:39:41.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.407573938369751
37983 2023-02-16,23:39:41.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3814119100570679
38362 2023-02-16,23:39:41.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3633315563201904
36932 2023-02-16,23:39:41.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 877/15000, loss = 1.4265680313110352
37268 2023-02-16,23:39:41.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.399092197418213
38255 2023-02-16,23:39:41.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4431767463684082
36814 2023-02-16,23:39:41.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 966/15000, loss = 1.3357716798782349
38108 2023-02-16,23:39:41.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3860000371932983
37393 2023-02-16,23:39:41.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3782843351364136
37862 2023-02-16,23:39:41.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3864109516143799
37035 2023-02-16,23:39:41.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 849/15000, loss = 1.4021518230438232
38487 2023-02-16,23:39:41.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 438/15000, loss = 1.3449561595916748
37757 2023-02-16,23:39:41.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4008748531341553
37533 2023-02-16,23:39:41.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.433542013168335
37152 2023-02-16,23:39:41.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.3953421115875244
37983 2023-02-16,23:39:41.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3253111839294434
38362 2023-02-16,23:39:41.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.3715808391571045
36932 2023-02-16,23:39:41.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 878/15000, loss = 1.3712900876998901
37268 2023-02-16,23:39:42.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3798282146453857
38255 2023-02-16,23:39:42.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3696513175964355
36814 2023-02-16,23:39:42.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 967/15000, loss = 1.4247117042541504
38108 2023-02-16,23:39:42.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3959550857543945
37393 2023-02-16,23:39:42.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4161772727966309
37862 2023-02-16,23:39:42.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4402703046798706
37035 2023-02-16,23:39:42.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 850/15000, loss = 1.3928313255310059
37757 2023-02-16,23:39:42.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.372672438621521
38487 2023-02-16,23:39:42.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 439/15000, loss = 1.3761905431747437
37533 2023-02-16,23:39:42.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4375343322753906
37152 2023-02-16,23:39:42.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.4254939556121826
37983 2023-02-16,23:39:42.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.378861904144287
38362 2023-02-16,23:39:42.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.446884036064148
36932 2023-02-16,23:39:42.328 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 879/15000, loss = 1.356381893157959
37268 2023-02-16,23:39:42.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4389647245407104
38255 2023-02-16,23:39:42.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3975467681884766
36814 2023-02-16,23:39:42.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 968/15000, loss = 1.3558145761489868
38108 2023-02-16,23:39:42.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4009358882904053
37393 2023-02-16,23:39:42.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3675192594528198
37862 2023-02-16,23:39:42.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3814061880111694
37035 2023-02-16,23:39:42.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 851/15000, loss = 1.4508250951766968
37757 2023-02-16,23:39:42.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4172041416168213
38487 2023-02-16,23:39:42.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 440/15000, loss = 1.3799445629119873
37533 2023-02-16,23:39:42.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.373761534690857
37152 2023-02-16,23:39:42.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.4070451259613037
37983 2023-02-16,23:39:42.661 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.348067045211792
38362 2023-02-16,23:39:42.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3994059562683105
36932 2023-02-16,23:39:42.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 880/15000, loss = 1.3856701850891113
37268 2023-02-16,23:39:42.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3131442070007324
38255 2023-02-16,23:39:42.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4142450094223022
36814 2023-02-16,23:39:42.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 969/15000, loss = 1.4032922983169556
38108 2023-02-16,23:39:42.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.430436372756958
37393 2023-02-16,23:39:42.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3943357467651367
37862 2023-02-16,23:39:42.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3954315185546875
37035 2023-02-16,23:39:42.925 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 852/15000, loss = 1.3954391479492188
37757 2023-02-16,23:39:42.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.434384822845459
38487 2023-02-16,23:39:42.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 441/15000, loss = 1.3766701221466064
37533 2023-02-16,23:39:42.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.404934048652649
37152 2023-02-16,23:39:42.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.377636432647705
37983 2023-02-16,23:39:43.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.401870608329773
38362 2023-02-16,23:39:43.011 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4205706119537354
36932 2023-02-16,23:39:43.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 881/15000, loss = 1.3898682594299316
37268 2023-02-16,23:39:43.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3370311260223389
38255 2023-02-16,23:39:43.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4069255590438843
36814 2023-02-16,23:39:43.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 970/15000, loss = 1.3823564052581787
38108 2023-02-16,23:39:43.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.379867672920227
37393 2023-02-16,23:39:43.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3806648254394531
37862 2023-02-16,23:39:43.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3749691247940063
37035 2023-02-16,23:39:43.267 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 853/15000, loss = 1.3784946203231812
37757 2023-02-16,23:39:43.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.381598949432373
38487 2023-02-16,23:39:43.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 442/15000, loss = 1.4135990142822266
37533 2023-02-16,23:39:43.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.420413613319397
37152 2023-02-16,23:39:43.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3526384830474854
37983 2023-02-16,23:39:43.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4203276634216309
38362 2023-02-16,23:39:43.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3782837390899658
36932 2023-02-16,23:39:43.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 882/15000, loss = 1.38808274269104
37268 2023-02-16,23:39:43.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.369520902633667
38255 2023-02-16,23:39:43.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4035505056381226
36814 2023-02-16,23:39:43.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 971/15000, loss = 1.3354690074920654
38108 2023-02-16,23:39:43.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4249098300933838
37393 2023-02-16,23:39:43.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3864132165908813
37035 2023-02-16,23:39:43.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 854/15000, loss = 1.359338402748108
37757 2023-02-16,23:39:43.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3613613843917847
37862 2023-02-16,23:39:43.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3739906549453735
38487 2023-02-16,23:39:43.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 443/15000, loss = 1.3810676336288452
37533 2023-02-16,23:39:43.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4177558422088623
37152 2023-02-16,23:39:43.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4101465940475464
37983 2023-02-16,23:39:43.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4115175008773804
38362 2023-02-16,23:39:43.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3666138648986816
36932 2023-02-16,23:39:43.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 883/15000, loss = 1.386017918586731
37268 2023-02-16,23:39:43.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3395581245422363
38255 2023-02-16,23:39:43.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.397792935371399
36814 2023-02-16,23:39:43.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 972/15000, loss = 1.3896578550338745
38108 2023-02-16,23:39:43.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.378190517425537
37393 2023-02-16,23:39:43.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3543299436569214
37035 2023-02-16,23:39:43.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 855/15000, loss = 1.3746620416641235
37757 2023-02-16,23:39:43.959 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4242713451385498
37862 2023-02-16,23:39:43.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3882008790969849
38487 2023-02-16,23:39:43.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 444/15000, loss = 1.4487924575805664
37533 2023-02-16,23:39:43.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4344782829284668
37152 2023-02-16,23:39:43.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.4341490268707275
37983 2023-02-16,23:39:44.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3951529264450073
38362 2023-02-16,23:39:44.034 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.3889405727386475
36932 2023-02-16,23:39:44.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 884/15000, loss = 1.3820549249649048
37268 2023-02-16,23:39:44.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.403230905532837
38255 2023-02-16,23:39:44.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4439737796783447
36814 2023-02-16,23:39:44.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 973/15000, loss = 1.3468515872955322
38108 2023-02-16,23:39:44.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3980793952941895
37393 2023-02-16,23:39:44.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4265297651290894
37035 2023-02-16,23:39:44.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 856/15000, loss = 1.4260921478271484
37757 2023-02-16,23:39:44.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3920209407806396
37862 2023-02-16,23:39:44.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.390993356704712
38487 2023-02-16,23:39:44.313 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 445/15000, loss = 1.4463934898376465
37533 2023-02-16,23:39:44.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.400822639465332
37152 2023-02-16,23:39:44.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4319363832473755
37983 2023-02-16,23:39:44.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3946434259414673
38362 2023-02-16,23:39:44.377 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3623849153518677
36932 2023-02-16,23:39:44.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 885/15000, loss = 1.37894606590271
37268 2023-02-16,23:39:44.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4208322763442993
38255 2023-02-16,23:39:44.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.398577094078064
36814 2023-02-16,23:39:44.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 974/15000, loss = 1.4133485555648804
38108 2023-02-16,23:39:44.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.356276512145996
37393 2023-02-16,23:39:44.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.406531572341919
37035 2023-02-16,23:39:44.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 857/15000, loss = 1.3893426656723022
37757 2023-02-16,23:39:44.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.387243390083313
37862 2023-02-16,23:39:44.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3834422826766968
38487 2023-02-16,23:39:44.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 446/15000, loss = 1.3450541496276855
37533 2023-02-16,23:39:44.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.419743537902832
37152 2023-02-16,23:39:44.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.4081202745437622
37983 2023-02-16,23:39:44.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.392528772354126
38362 2023-02-16,23:39:44.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.372571349143982
36932 2023-02-16,23:39:44.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 886/15000, loss = 1.4324514865875244
37268 2023-02-16,23:39:44.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.4153950214385986
38255 2023-02-16,23:39:44.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.350913405418396
36814 2023-02-16,23:39:44.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 975/15000, loss = 1.3817720413208008
38108 2023-02-16,23:39:44.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.426157832145691
37393 2023-02-16,23:39:44.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3659569025039673
37035 2023-02-16,23:39:44.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 858/15000, loss = 1.4129159450531006
37757 2023-02-16,23:39:44.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3216806650161743
37862 2023-02-16,23:39:44.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3629566431045532
38487 2023-02-16,23:39:44.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 447/15000, loss = 1.4069526195526123
37533 2023-02-16,23:39:45.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3954408168792725
37152 2023-02-16,23:39:45.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4544928073883057
37983 2023-02-16,23:39:45.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4456826448440552
38362 2023-02-16,23:39:45.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3555368185043335
36932 2023-02-16,23:39:45.070 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 887/15000, loss = 1.3772881031036377
37268 2023-02-16,23:39:45.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3919029235839844
38255 2023-02-16,23:39:45.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.349581241607666
36814 2023-02-16,23:39:45.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 976/15000, loss = 1.3855808973312378
38108 2023-02-16,23:39:45.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3837742805480957
37393 2023-02-16,23:39:45.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4191054105758667
37035 2023-02-16,23:39:45.322 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 859/15000, loss = 1.4103434085845947
37757 2023-02-16,23:39:45.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.410168170928955
37862 2023-02-16,23:39:45.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677611351013184
38487 2023-02-16,23:39:45.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 448/15000, loss = 1.470967411994934
37533 2023-02-16,23:39:45.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.455162763595581
37152 2023-02-16,23:39:45.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4124958515167236
37983 2023-02-16,23:39:45.405 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3758971691131592
38362 2023-02-16,23:39:45.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.372870922088623
36932 2023-02-16,23:39:45.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 888/15000, loss = 1.3656913042068481
37268 2023-02-16,23:39:45.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3585824966430664
38255 2023-02-16,23:39:45.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4208488464355469
36814 2023-02-16,23:39:45.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 977/15000, loss = 1.4135932922363281
38108 2023-02-16,23:39:45.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3965052366256714
37393 2023-02-16,23:39:45.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3915249109268188
37035 2023-02-16,23:39:45.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 860/15000, loss = 1.406280279159546
37757 2023-02-16,23:39:45.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.377178430557251
37862 2023-02-16,23:39:45.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3505678176879883
38487 2023-02-16,23:39:45.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 449/15000, loss = 1.3877328634262085
37533 2023-02-16,23:39:45.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3542025089263916
37152 2023-02-16,23:39:45.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.3608301877975464
37983 2023-02-16,23:39:45.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3539284467697144
38362 2023-02-16,23:39:45.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.3621373176574707
36932 2023-02-16,23:39:45.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 889/15000, loss = 1.3807364702224731
37268 2023-02-16,23:39:45.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.405149221420288
38255 2023-02-16,23:39:45.829 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4323060512542725
36814 2023-02-16,23:39:45.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 978/15000, loss = 1.337036371231079
38108 2023-02-16,23:39:45.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3960403203964233
37393 2023-02-16,23:39:45.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4780484437942505
37035 2023-02-16,23:39:46.015 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 861/15000, loss = 1.4023237228393555
37757 2023-02-16,23:39:46.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.413494348526001
37862 2023-02-16,23:39:46.026 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4131782054901123
38487 2023-02-16,23:39:46.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 450/15000, loss = 1.3816370964050293
37533 2023-02-16,23:39:46.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4178307056427002
37152 2023-02-16,23:39:46.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3893723487854004
37983 2023-02-16,23:39:46.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4012565612792969
38362 2023-02-16,23:39:46.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4126181602478027
36932 2023-02-16,23:39:46.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 890/15000, loss = 1.3818053007125854
37268 2023-02-16,23:39:46.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.398462176322937
38255 2023-02-16,23:39:46.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3882951736450195
36814 2023-02-16,23:39:46.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 979/15000, loss = 1.385512113571167
38108 2023-02-16,23:39:46.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3869723081588745
37393 2023-02-16,23:39:46.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3675025701522827
37035 2023-02-16,23:39:46.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 862/15000, loss = 1.380232334136963
37757 2023-02-16,23:39:46.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3836309909820557
37862 2023-02-16,23:39:46.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3814775943756104
38487 2023-02-16,23:39:46.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 451/15000, loss = 1.4016239643096924
37533 2023-02-16,23:39:46.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.380130648612976
37152 2023-02-16,23:39:46.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.3744981288909912
37983 2023-02-16,23:39:46.445 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3605427742004395
38362 2023-02-16,23:39:46.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.422695517539978
36932 2023-02-16,23:39:46.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 891/15000, loss = 1.3698936700820923
37268 2023-02-16,23:39:46.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.350193738937378
38255 2023-02-16,23:39:46.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4323396682739258
36814 2023-02-16,23:39:46.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 980/15000, loss = 1.3736790418624878
38108 2023-02-16,23:39:46.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4075225591659546
37393 2023-02-16,23:39:46.686 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3792152404785156
37035 2023-02-16,23:39:46.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 863/15000, loss = 1.4125375747680664
37757 2023-02-16,23:39:46.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3610574007034302
37862 2023-02-16,23:39:46.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4369795322418213
38487 2023-02-16,23:39:46.728 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 452/15000, loss = 1.3743294477462769
37533 2023-02-16,23:39:46.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3183274269104004
37152 2023-02-16,23:39:46.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.415175199508667
37983 2023-02-16,23:39:46.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3795274496078491
38362 2023-02-16,23:39:46.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4430663585662842
36932 2023-02-16,23:39:46.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 892/15000, loss = 1.410075068473816
37268 2023-02-16,23:39:46.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3731708526611328
38255 2023-02-16,23:39:46.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.357567548751831
36814 2023-02-16,23:39:46.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 981/15000, loss = 1.4354641437530518
38108 2023-02-16,23:39:46.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4059556722640991
37393 2023-02-16,23:39:47.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.374237298965454
37035 2023-02-16,23:39:47.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 864/15000, loss = 1.4080564975738525
37757 2023-02-16,23:39:47.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4272582530975342
37862 2023-02-16,23:39:47.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4333020448684692
38487 2023-02-16,23:39:47.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 453/15000, loss = 1.3938746452331543
37533 2023-02-16,23:39:47.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3648600578308105
37152 2023-02-16,23:39:47.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.3702479600906372
37983 2023-02-16,23:39:47.137 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4418776035308838
38362 2023-02-16,23:39:47.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.3696951866149902
36932 2023-02-16,23:39:47.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 893/15000, loss = 1.3817373514175415
37268 2023-02-16,23:39:47.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.371931552886963
38255 2023-02-16,23:39:47.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3847514390945435
36814 2023-02-16,23:39:47.312 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 982/15000, loss = 1.4275684356689453
38108 2023-02-16,23:39:47.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4148738384246826
37393 2023-02-16,23:39:47.379 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4493707418441772
37035 2023-02-16,23:39:47.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 865/15000, loss = 1.3903722763061523
37757 2023-02-16,23:39:47.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.42277991771698
37862 2023-02-16,23:39:47.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4311858415603638
38487 2023-02-16,23:39:47.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 454/15000, loss = 1.418843150138855
37533 2023-02-16,23:39:47.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4368295669555664
37152 2023-02-16,23:39:47.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.3957934379577637
37983 2023-02-16,23:39:47.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3800944089889526
38362 2023-02-16,23:39:47.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.3975744247436523
36932 2023-02-16,23:39:47.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 894/15000, loss = 1.357720136642456
37268 2023-02-16,23:39:47.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3817625045776367
38255 2023-02-16,23:39:47.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3790333271026611
36814 2023-02-16,23:39:47.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 983/15000, loss = 1.3476755619049072
38108 2023-02-16,23:39:47.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4135336875915527
37393 2023-02-16,23:39:47.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3731861114501953
37035 2023-02-16,23:39:47.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 866/15000, loss = 1.3643659353256226
37757 2023-02-16,23:39:47.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.411156415939331
37862 2023-02-16,23:39:47.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3672964572906494
38487 2023-02-16,23:39:47.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 455/15000, loss = 1.4159538745880127
37533 2023-02-16,23:39:47.779 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.395289421081543
37152 2023-02-16,23:39:47.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.4316904544830322
37983 2023-02-16,23:39:47.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4122923612594604
38362 2023-02-16,23:39:47.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4142853021621704
36932 2023-02-16,23:39:47.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 895/15000, loss = 1.3879570960998535
37268 2023-02-16,23:39:47.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.3993408679962158
38255 2023-02-16,23:39:47.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3781752586364746
36814 2023-02-16,23:39:48.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 984/15000, loss = 1.415246844291687
38108 2023-02-16,23:39:48.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4248640537261963
37393 2023-02-16,23:39:48.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3818645477294922
37035 2023-02-16,23:39:48.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 867/15000, loss = 1.3591289520263672
37757 2023-02-16,23:39:48.102 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.453588843345642
37862 2023-02-16,23:39:48.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3781062364578247
38487 2023-02-16,23:39:48.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 456/15000, loss = 1.3808207511901855
37533 2023-02-16,23:39:48.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.344130516052246
37152 2023-02-16,23:39:48.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.400061011314392
37983 2023-02-16,23:39:48.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.389093041419983
38362 2023-02-16,23:39:48.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.4069815874099731
36932 2023-02-16,23:39:48.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 896/15000, loss = 1.3699870109558105
37268 2023-02-16,23:39:48.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.459106206893921
38255 2023-02-16,23:39:48.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3930861949920654
36814 2023-02-16,23:39:48.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 985/15000, loss = 1.4040040969848633
38108 2023-02-16,23:39:48.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3795840740203857
37393 2023-02-16,23:39:48.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4040896892547607
37035 2023-02-16,23:39:48.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 868/15000, loss = 1.4326339960098267
37757 2023-02-16,23:39:48.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4048069715499878
37862 2023-02-16,23:39:48.451 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4201009273529053
38487 2023-02-16,23:39:48.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 457/15000, loss = 1.408583164215088
37533 2023-02-16,23:39:48.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.393548846244812
37152 2023-02-16,23:39:48.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.3510046005249023
37983 2023-02-16,23:39:48.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.387837529182434
38362 2023-02-16,23:39:48.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4036601781845093
36932 2023-02-16,23:39:48.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 897/15000, loss = 1.3944511413574219
37268 2023-02-16,23:39:48.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.3435132503509521
38255 2023-02-16,23:39:48.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3940562009811401
36814 2023-02-16,23:39:48.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 986/15000, loss = 1.4279067516326904
38108 2023-02-16,23:39:48.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4087117910385132
37393 2023-02-16,23:39:48.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3696434497833252
37035 2023-02-16,23:39:48.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 869/15000, loss = 1.3421281576156616
37757 2023-02-16,23:39:48.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3923615217208862
37862 2023-02-16,23:39:48.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.347348690032959
37533 2023-02-16,23:39:48.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3886486291885376
38487 2023-02-16,23:39:48.829 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 458/15000, loss = 1.464874505996704
37152 2023-02-16,23:39:48.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.4207566976547241
37983 2023-02-16,23:39:48.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3807704448699951
38362 2023-02-16,23:39:48.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3977595567703247
36932 2023-02-16,23:39:48.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 898/15000, loss = 1.4214015007019043
37268 2023-02-16,23:39:48.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3858489990234375
38255 2023-02-16,23:39:48.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3666446208953857
36814 2023-02-16,23:39:49.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 987/15000, loss = 1.356663465499878
38108 2023-02-16,23:39:49.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3776483535766602
37393 2023-02-16,23:39:49.109 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.316853404045105
37035 2023-02-16,23:39:49.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 870/15000, loss = 1.3519889116287231
37757 2023-02-16,23:39:49.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4025771617889404
37862 2023-02-16,23:39:49.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.332522988319397
37533 2023-02-16,23:39:49.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3841348886489868
38487 2023-02-16,23:39:49.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 459/15000, loss = 1.4380223751068115
37152 2023-02-16,23:39:49.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3954403400421143
37983 2023-02-16,23:39:49.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3864020109176636
38362 2023-02-16,23:39:49.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4440168142318726
36932 2023-02-16,23:39:49.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 899/15000, loss = 1.3644657135009766
37268 2023-02-16,23:39:49.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3981645107269287
38255 2023-02-16,23:39:49.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4107449054718018
36814 2023-02-16,23:39:49.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 988/15000, loss = 1.3196284770965576
38108 2023-02-16,23:39:49.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3746470212936401
37393 2023-02-16,23:39:49.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3988991975784302
37035 2023-02-16,23:39:49.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 871/15000, loss = 1.3631393909454346
37757 2023-02-16,23:39:49.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4070780277252197
37862 2023-02-16,23:39:49.487 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.416555404663086
37533 2023-02-16,23:39:49.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4135351181030273
38487 2023-02-16,23:39:49.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 460/15000, loss = 1.3956575393676758
37152 2023-02-16,23:39:49.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.334477424621582
37983 2023-02-16,23:39:49.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4403307437896729
38362 2023-02-16,23:39:49.567 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.398613452911377
36932 2023-02-16,23:39:49.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 900/15000, loss = 1.3809497356414795
37268 2023-02-16,23:39:49.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4045674800872803
38255 2023-02-16,23:39:49.637 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.385860800743103
36814 2023-02-16,23:39:49.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 989/15000, loss = 1.3910787105560303
38108 2023-02-16,23:39:49.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3813157081604004
37393 2023-02-16,23:39:49.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3804917335510254
37035 2023-02-16,23:39:49.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 872/15000, loss = 1.399809718132019
37757 2023-02-16,23:39:49.831 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.40402352809906
37862 2023-02-16,23:39:49.832 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4007987976074219
37533 2023-02-16,23:39:49.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4110873937606812
38487 2023-02-16,23:39:49.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 461/15000, loss = 1.3666918277740479
37152 2023-02-16,23:39:49.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.3675954341888428
37983 2023-02-16,23:39:49.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3814831972122192
38362 2023-02-16,23:39:49.912 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.351011872291565
36932 2023-02-16,23:39:49.917 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 901/15000, loss = 1.3753491640090942
37268 2023-02-16,23:39:49.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.382739543914795
38255 2023-02-16,23:39:49.982 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3958510160446167
36814 2023-02-16,23:39:50.077 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 990/15000, loss = 1.3855116367340088
38108 2023-02-16,23:39:50.098 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3253848552703857
37393 2023-02-16,23:39:50.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4400355815887451
37035 2023-02-16,23:39:50.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 873/15000, loss = 1.3799446821212769
37757 2023-02-16,23:39:50.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.400789499282837
37862 2023-02-16,23:39:50.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.372836709022522
37533 2023-02-16,23:39:50.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4099317789077759
38487 2023-02-16,23:39:50.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 462/15000, loss = 1.3543100357055664
37152 2023-02-16,23:39:50.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3880358934402466
37983 2023-02-16,23:39:50.249 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3953927755355835
38362 2023-02-16,23:39:50.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.349629282951355
36932 2023-02-16,23:39:50.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 902/15000, loss = 1.3801360130310059
37268 2023-02-16,23:39:50.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3670880794525146
38255 2023-02-16,23:39:50.324 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4011015892028809
36814 2023-02-16,23:39:50.420 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 991/15000, loss = 1.3855419158935547
38108 2023-02-16,23:39:50.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3789470195770264
37393 2023-02-16,23:39:50.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3135195970535278
37035 2023-02-16,23:39:50.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 874/15000, loss = 1.4120171070098877
37757 2023-02-16,23:39:50.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3981940746307373
37862 2023-02-16,23:39:50.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4172886610031128
37533 2023-02-16,23:39:50.541 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4197639226913452
38487 2023-02-16,23:39:50.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 463/15000, loss = 1.3906508684158325
37152 2023-02-16,23:39:50.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.3574025630950928
37983 2023-02-16,23:39:50.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3751332759857178
38362 2023-02-16,23:39:50.598 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4208179712295532
36932 2023-02-16,23:39:50.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 903/15000, loss = 1.3795768022537231
37268 2023-02-16,23:39:50.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4242684841156006
38255 2023-02-16,23:39:50.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4303982257843018
36814 2023-02-16,23:39:50.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 992/15000, loss = 1.3810920715332031
38108 2023-02-16,23:39:50.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3481413125991821
37393 2023-02-16,23:39:50.831 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3374539613723755
37035 2023-02-16,23:39:50.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 875/15000, loss = 1.4119747877120972
37862 2023-02-16,23:39:50.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4345225095748901
37757 2023-02-16,23:39:50.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4335559606552124
37533 2023-02-16,23:39:50.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4208571910858154
38487 2023-02-16,23:39:50.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 464/15000, loss = 1.3314733505249023
37983 2023-02-16,23:39:50.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3737713098526
38362 2023-02-16,23:39:50.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4323843717575073
36932 2023-02-16,23:39:50.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 904/15000, loss = 1.4465512037277222
37152 2023-02-16,23:39:50.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.3903864622116089
37268 2023-02-16,23:39:50.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4185711145401
38255 2023-02-16,23:39:51.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.379917860031128
36814 2023-02-16,23:39:51.109 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 993/15000, loss = 1.3726732730865479
38108 2023-02-16,23:39:51.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4017958641052246
37393 2023-02-16,23:39:51.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3694260120391846
37035 2023-02-16,23:39:51.199 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 876/15000, loss = 1.333357572555542
37862 2023-02-16,23:39:51.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3816384077072144
37533 2023-02-16,23:39:51.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4013079404830933
38487 2023-02-16,23:39:51.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 465/15000, loss = 1.3722078800201416
37757 2023-02-16,23:39:51.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4413964748382568
37983 2023-02-16,23:39:51.288 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3882683515548706
38362 2023-02-16,23:39:51.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3883742094039917
36932 2023-02-16,23:39:51.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 905/15000, loss = 1.3611500263214111
37152 2023-02-16,23:39:51.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.3402339220046997
37268 2023-02-16,23:39:51.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.407362699508667
38255 2023-02-16,23:39:51.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4247453212738037
36814 2023-02-16,23:39:51.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 994/15000, loss = 1.3423718214035034
38108 2023-02-16,23:39:51.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4203627109527588
37393 2023-02-16,23:39:51.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3405354022979736
37035 2023-02-16,23:39:51.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 877/15000, loss = 1.424769401550293
37862 2023-02-16,23:39:51.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3613921403884888
37533 2023-02-16,23:39:51.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4039443731307983
38487 2023-02-16,23:39:51.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 466/15000, loss = 1.3634148836135864
37757 2023-02-16,23:39:51.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.409356713294983
37983 2023-02-16,23:39:51.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3909480571746826
38362 2023-02-16,23:39:51.638 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4322798252105713
36932 2023-02-16,23:39:51.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 906/15000, loss = 1.4006316661834717
37152 2023-02-16,23:39:51.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.395592212677002
37268 2023-02-16,23:39:51.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3715699911117554
38255 2023-02-16,23:39:51.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.378296136856079
36814 2023-02-16,23:39:51.798 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 995/15000, loss = 1.3523443937301636
38108 2023-02-16,23:39:51.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4114289283752441
37393 2023-02-16,23:39:51.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.405673861503601
37035 2023-02-16,23:39:51.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 878/15000, loss = 1.3714733123779297
37862 2023-02-16,23:39:51.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4241942167282104
37533 2023-02-16,23:39:51.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3682961463928223
38487 2023-02-16,23:39:51.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 467/15000, loss = 1.371612787246704
37757 2023-02-16,23:39:51.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4361293315887451
37983 2023-02-16,23:39:51.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3834165334701538
38362 2023-02-16,23:39:51.982 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3575801849365234
36932 2023-02-16,23:39:51.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 907/15000, loss = 1.407912254333496
37268 2023-02-16,23:39:52.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3271992206573486
37152 2023-02-16,23:39:52.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.3907736539840698
38255 2023-02-16,23:39:52.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.398134469985962
36814 2023-02-16,23:39:52.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 996/15000, loss = 1.391402006149292
38108 2023-02-16,23:39:52.165 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3950694799423218
37393 2023-02-16,23:39:52.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4226735830307007
37035 2023-02-16,23:39:52.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 879/15000, loss = 1.354729413986206
37862 2023-02-16,23:39:52.272 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3920613527297974
37533 2023-02-16,23:39:52.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4180498123168945
38487 2023-02-16,23:39:52.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 468/15000, loss = 1.4467920064926147
37757 2023-02-16,23:39:52.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4394347667694092
37983 2023-02-16,23:39:52.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3628921508789062
38362 2023-02-16,23:39:52.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3847670555114746
36932 2023-02-16,23:39:52.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 908/15000, loss = 1.4278366565704346
37268 2023-02-16,23:39:52.414 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.36422598361969
37152 2023-02-16,23:39:52.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.3554235696792603
38255 2023-02-16,23:39:52.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3560643196105957
36814 2023-02-16,23:39:52.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 997/15000, loss = 1.3707505464553833
38108 2023-02-16,23:39:52.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.394670844078064
37393 2023-02-16,23:39:52.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.4170042276382446
37035 2023-02-16,23:39:52.668 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 880/15000, loss = 1.3845597505569458
37533 2023-02-16,23:39:52.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4348012208938599
37862 2023-02-16,23:39:52.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3873275518417358
38487 2023-02-16,23:39:52.715 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 469/15000, loss = 1.3993504047393799
37757 2023-02-16,23:39:52.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.373698115348816
37983 2023-02-16,23:39:52.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677433729171753
38362 2023-02-16,23:39:52.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3790618181228638
36932 2023-02-16,23:39:52.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 909/15000, loss = 1.4310277700424194
37268 2023-02-16,23:39:52.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.374775767326355
38255 2023-02-16,23:39:52.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4260421991348267
37152 2023-02-16,23:39:52.848 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.3709261417388916
36814 2023-02-16,23:39:52.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 998/15000, loss = 1.3724582195281982
38108 2023-02-16,23:39:52.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.392382025718689
37393 2023-02-16,23:39:52.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3906583786010742
37035 2023-02-16,23:39:53.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 881/15000, loss = 1.3893898725509644
37533 2023-02-16,23:39:53.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4185587167739868
37862 2023-02-16,23:39:53.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.321512222290039
38487 2023-02-16,23:39:53.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 470/15000, loss = 1.4204471111297607
37757 2023-02-16,23:39:53.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4055043458938599
37983 2023-02-16,23:39:53.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3505175113677979
38362 2023-02-16,23:39:53.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3782424926757812
36932 2023-02-16,23:39:53.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 910/15000, loss = 1.4195220470428467
37268 2023-02-16,23:39:53.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.3955727815628052
38255 2023-02-16,23:39:53.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.383649468421936
37152 2023-02-16,23:39:53.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.393541693687439
38108 2023-02-16,23:39:53.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4454504251480103
37393 2023-02-16,23:39:53.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3593491315841675
36814 2023-02-16,23:39:53.388 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 999/15000, loss = 1.413501501083374
37533 2023-02-16,23:39:53.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.410766839981079
37035 2023-02-16,23:39:53.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 882/15000, loss = 1.3875555992126465
37862 2023-02-16,23:39:53.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4101043939590454
38487 2023-02-16,23:39:53.500 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 471/15000, loss = 1.3781487941741943
38362 2023-02-16,23:39:53.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3931779861450195
37757 2023-02-16,23:39:53.566 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4223357439041138
37983 2023-02-16,23:39:53.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.413196325302124
36932 2023-02-16,23:39:53.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 911/15000, loss = 1.4094421863555908
37268 2023-02-16,23:39:53.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3606319427490234
38255 2023-02-16,23:39:53.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.396506428718567
37152 2023-02-16,23:39:53.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.4395091533660889
38108 2023-02-16,23:39:53.700 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3760355710983276
37393 2023-02-16,23:39:53.749 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4068306684494019
36814 2023-02-16,23:39:53.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1000/15000, loss = 1.4249787330627441
37533 2023-02-16,23:39:53.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.378896951675415
37862 2023-02-16,23:39:53.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3772882223129272
38487 2023-02-16,23:39:53.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 472/15000, loss = 1.3667566776275635
37035 2023-02-16,23:39:53.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 883/15000, loss = 1.3845411539077759
38362 2023-02-16,23:39:53.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3940682411193848
37757 2023-02-16,23:39:53.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.419643759727478
37983 2023-02-16,23:39:53.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.381388545036316
36932 2023-02-16,23:39:53.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 912/15000, loss = 1.4064626693725586
37268 2023-02-16,23:39:54.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3603917360305786
38255 2023-02-16,23:39:54.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3959463834762573
37152 2023-02-16,23:39:54.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.429112434387207
38108 2023-02-16,23:39:54.111 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3540632724761963
37393 2023-02-16,23:39:54.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.4003177881240845
36814 2023-02-16,23:39:54.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1001/15000, loss = 1.3775497674942017
37533 2023-02-16,23:39:54.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3887426853179932
38487 2023-02-16,23:39:54.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 473/15000, loss = 1.388925313949585
37862 2023-02-16,23:39:54.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4133915901184082
37035 2023-02-16,23:39:54.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 884/15000, loss = 1.380857229232788
38362 2023-02-16,23:39:54.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3666532039642334
37757 2023-02-16,23:39:54.382 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4362362623214722
37983 2023-02-16,23:39:54.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4370416402816772
36932 2023-02-16,23:39:54.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 913/15000, loss = 1.367823839187622
37268 2023-02-16,23:39:54.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.3951367139816284
38255 2023-02-16,23:39:54.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3870254755020142
37152 2023-02-16,23:39:54.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 805/15000, loss = 1.3878040313720703
38108 2023-02-16,23:39:54.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.401320219039917
37393 2023-02-16,23:39:54.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.350378155708313
36814 2023-02-16,23:39:54.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1002/15000, loss = 1.3345531225204468
37533 2023-02-16,23:39:54.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.380662441253662
38487 2023-02-16,23:39:54.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 474/15000, loss = 1.3624248504638672
37862 2023-02-16,23:39:54.695 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3837049007415771
37035 2023-02-16,23:39:54.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 885/15000, loss = 1.3784531354904175
38362 2023-02-16,23:39:54.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.410844087600708
37757 2023-02-16,23:39:54.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.401413917541504
37983 2023-02-16,23:39:54.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.433258295059204
36932 2023-02-16,23:39:54.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 914/15000, loss = 1.4025177955627441
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
40293 2023-02-16,23:39:54.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37268 2023-02-16,23:39:54.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3611063957214355
38255 2023-02-16,23:39:54.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4073830842971802
37152 2023-02-16,23:39:54.883 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 806/15000, loss = 1.38265860080719
38108 2023-02-16,23:39:54.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3605092763900757
37393 2023-02-16,23:39:54.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3739910125732422
36814 2023-02-16,23:39:54.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1003/15000, loss = 1.3943555355072021
38487 2023-02-16,23:39:55.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 475/15000, loss = 1.3725636005401611
37533 2023-02-16,23:39:55.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3722110986709595
37035 2023-02-16,23:39:55.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 886/15000, loss = 1.432269811630249
37862 2023-02-16,23:39:55.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.361108660697937
38362 2023-02-16,23:39:55.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3859920501708984
37983 2023-02-16,23:39:55.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4314069747924805
37757 2023-02-16,23:39:55.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4214273691177368
36932 2023-02-16,23:39:55.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 915/15000, loss = 1.379648208618164
40293 2023-02-16,23:39:55.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37268 2023-02-16,23:39:55.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.360621452331543
38255 2023-02-16,23:39:55.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4062052965164185
37152 2023-02-16,23:39:55.317 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 807/15000, loss = 1.371117353439331
38108 2023-02-16,23:39:55.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.379426121711731
37393 2023-02-16,23:39:55.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.372363567352295
36814 2023-02-16,23:39:55.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1004/15000, loss = 1.4172133207321167
38487 2023-02-16,23:39:55.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 476/15000, loss = 1.3556733131408691
37533 2023-02-16,23:39:55.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.4217005968093872
38362 2023-02-16,23:39:55.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3960373401641846
37035 2023-02-16,23:39:55.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 887/15000, loss = 1.3777220249176025
37862 2023-02-16,23:39:55.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4273204803466797
37983 2023-02-16,23:39:55.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3671592473983765
37757 2023-02-16,23:39:55.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3963590860366821
36932 2023-02-16,23:39:55.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 916/15000, loss = 1.4591264724731445
40293 2023-02-16,23:39:55.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37268 2023-02-16,23:39:55.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.389836072921753
38255 2023-02-16,23:39:55.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4150441884994507
37152 2023-02-16,23:39:55.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 808/15000, loss = 1.418307900428772
38108 2023-02-16,23:39:55.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4419453144073486
37393 2023-02-16,23:39:55.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3802422285079956
38487 2023-02-16,23:39:55.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 477/15000, loss = 1.3728820085525513
36814 2023-02-16,23:39:55.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1005/15000, loss = 1.4087224006652832
37533 2023-02-16,23:39:55.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.396527886390686
38362 2023-02-16,23:39:55.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4010987281799316
37862 2023-02-16,23:39:55.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4227309226989746
37035 2023-02-16,23:39:55.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 888/15000, loss = 1.3641903400421143
37983 2023-02-16,23:39:56.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.378169298171997
37757 2023-02-16,23:39:56.115 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4569262266159058
36932 2023-02-16,23:39:56.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 917/15000, loss = 1.3728638887405396
40293 2023-02-16,23:39:56.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37268 2023-02-16,23:39:56.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3759472370147705
38255 2023-02-16,23:39:56.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4136453866958618
37152 2023-02-16,23:39:56.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 809/15000, loss = 1.4170899391174316
38108 2023-02-16,23:39:56.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3801531791687012
37393 2023-02-16,23:39:56.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.4006539583206177
37533 2023-02-16,23:39:56.280 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4032009840011597
38487 2023-02-16,23:39:56.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 478/15000, loss = 1.362212061882019
36814 2023-02-16,23:39:56.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1006/15000, loss = 1.3985832929611206
38362 2023-02-16,23:39:56.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4304883480072021
37862 2023-02-16,23:39:56.396 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4110357761383057
37035 2023-02-16,23:39:56.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 889/15000, loss = 1.3796708583831787
37983 2023-02-16,23:39:56.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.420067310333252
37757 2023-02-16,23:39:56.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3535014390945435
36932 2023-02-16,23:39:56.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 918/15000, loss = 1.3408974409103394
40293 2023-02-16,23:39:56.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
37268 2023-02-16,23:39:56.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4045355319976807
38255 2023-02-16,23:39:56.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4249076843261719
37152 2023-02-16,23:39:56.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 810/15000, loss = 1.3900774717330933
38108 2023-02-16,23:39:56.663 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4120597839355469
37393 2023-02-16,23:39:56.708 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4570128917694092
37533 2023-02-16,23:39:56.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3667597770690918
38487 2023-02-16,23:39:56.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 479/15000, loss = 1.4126343727111816
36814 2023-02-16,23:39:56.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1007/15000, loss = 1.4241958856582642
38362 2023-02-16,23:39:56.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3798401355743408
37035 2023-02-16,23:39:56.812 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 890/15000, loss = 1.3787658214569092
37862 2023-02-16,23:39:56.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4537808895111084
37983 2023-02-16,23:39:56.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.347152590751648
37757 2023-02-16,23:39:56.978 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4202027320861816
36932 2023-02-16,23:39:56.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 919/15000, loss = 1.378741979598999
40293 2023-02-16,23:39:57.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37268 2023-02-16,23:39:57.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.404843807220459
38255 2023-02-16,23:39:57.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3798202276229858
37152 2023-02-16,23:39:57.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 811/15000, loss = 1.2952287197113037
38108 2023-02-16,23:39:57.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.389038324356079
37393 2023-02-16,23:39:57.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.3433958292007446
37533 2023-02-16,23:39:57.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.372231364250183
38487 2023-02-16,23:39:57.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 480/15000, loss = 1.4224659204483032
36814 2023-02-16,23:39:57.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1008/15000, loss = 1.3693218231201172
38362 2023-02-16,23:39:57.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4248919486999512
37035 2023-02-16,23:39:57.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 891/15000, loss = 1.3692314624786377
37862 2023-02-16,23:39:57.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4048818349838257
37983 2023-02-16,23:39:57.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3324532508850098
37757 2023-02-16,23:39:57.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3788944482803345
36932 2023-02-16,23:39:57.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 920/15000, loss = 1.4600610733032227
40293 2023-02-16,23:39:57.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37268 2023-02-16,23:39:57.472 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.405963659286499
38255 2023-02-16,23:39:57.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4087774753570557
37152 2023-02-16,23:39:57.498 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 812/15000, loss = 1.3406190872192383
37533 2023-02-16,23:39:57.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.3813284635543823
38108 2023-02-16,23:39:57.538 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3878815174102783
37393 2023-02-16,23:39:57.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3844573497772217
38487 2023-02-16,23:39:57.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 481/15000, loss = 1.4432001113891602
36814 2023-02-16,23:39:57.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1009/15000, loss = 1.362198829650879
38362 2023-02-16,23:39:57.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3783046007156372
37862 2023-02-16,23:39:57.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3923803567886353
37983 2023-02-16,23:39:57.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4165680408477783
37035 2023-02-16,23:39:57.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 892/15000, loss = 1.4094665050506592
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
40433 2023-02-16,23:39:57.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37757 2023-02-16,23:39:57.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.31742525100708
36932 2023-02-16,23:39:57.896 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 921/15000, loss = 1.3789604902267456
40293 2023-02-16,23:39:57.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
37268 2023-02-16,23:39:57.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.3495274782180786
38255 2023-02-16,23:39:57.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3777105808258057
37152 2023-02-16,23:39:57.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 813/15000, loss = 1.3870173692703247
37533 2023-02-16,23:39:57.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3996434211730957
38108 2023-02-16,23:39:57.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3807109594345093
37393 2023-02-16,23:39:58.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3975886106491089
38487 2023-02-16,23:39:58.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 482/15000, loss = 1.369707465171814
36814 2023-02-16,23:39:58.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1010/15000, loss = 1.4146682024002075
38362 2023-02-16,23:39:58.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3981155157089233
37862 2023-02-16,23:39:58.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.402477741241455
37983 2023-02-16,23:39:58.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4007339477539062
37035 2023-02-16,23:39:58.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 893/15000, loss = 1.3817949295043945
40433 2023-02-16,23:39:58.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1/15000, loss = 1.3773303031921387
37757 2023-02-16,23:39:58.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3646942377090454
36932 2023-02-16,23:39:58.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 922/15000, loss = 1.3505237102508545
40293 2023-02-16,23:39:58.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37268 2023-02-16,23:39:58.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4030156135559082
38255 2023-02-16,23:39:58.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3747000694274902
37152 2023-02-16,23:39:58.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 814/15000, loss = 1.3750741481781006
37533 2023-02-16,23:39:58.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4579132795333862
38108 2023-02-16,23:39:58.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.38654363155365
38487 2023-02-16,23:39:58.454 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 483/15000, loss = 1.397605538368225
37393 2023-02-16,23:39:58.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4058760404586792
36814 2023-02-16,23:39:58.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1011/15000, loss = 1.4030466079711914
38362 2023-02-16,23:39:58.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3561697006225586
37862 2023-02-16,23:39:58.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4070709943771362
37983 2023-02-16,23:39:58.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3728387355804443
37035 2023-02-16,23:39:58.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 894/15000, loss = 1.3561285734176636
40433 2023-02-16,23:39:58.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 2/15000, loss = 1.3797693252563477
37757 2023-02-16,23:39:58.712 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4391207695007324
36932 2023-02-16,23:39:58.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 923/15000, loss = 1.3903800249099731
37268 2023-02-16,23:39:58.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.357502818107605
40293 2023-02-16,23:39:58.831 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
37152 2023-02-16,23:39:58.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 815/15000, loss = 1.415783166885376
38255 2023-02-16,23:39:58.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3814902305603027
37533 2023-02-16,23:39:58.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4085800647735596
37393 2023-02-16,23:39:58.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.3817355632781982
38108 2023-02-16,23:39:58.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4403096437454224
38487 2023-02-16,23:39:58.909 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 484/15000, loss = 1.4142130613327026
36814 2023-02-16,23:39:58.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1012/15000, loss = 1.4082109928131104
38362 2023-02-16,23:39:58.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4262111186981201
37862 2023-02-16,23:39:59.054 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4041986465454102
37983 2023-02-16,23:39:59.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.417327880859375
37035 2023-02-16,23:39:59.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 895/15000, loss = 1.3871712684631348
40433 2023-02-16,23:39:59.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 3/15000, loss = 1.416022777557373
37757 2023-02-16,23:39:59.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3970916271209717
36932 2023-02-16,23:39:59.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 924/15000, loss = 1.411070466041565
37268 2023-02-16,23:39:59.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.4190952777862549
40293 2023-02-16,23:39:59.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
37152 2023-02-16,23:39:59.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 816/15000, loss = 1.4226086139678955
37533 2023-02-16,23:39:59.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4089975357055664
38255 2023-02-16,23:39:59.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3253380060195923
37393 2023-02-16,23:39:59.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.367060899734497
38108 2023-02-16,23:39:59.360 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3816276788711548
38487 2023-02-16,23:39:59.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 485/15000, loss = 1.407005786895752
38362 2023-02-16,23:39:59.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.383711576461792
36814 2023-02-16,23:39:59.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1013/15000, loss = 1.3663052320480347
37862 2023-02-16,23:39:59.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4008373022079468
37983 2023-02-16,23:39:59.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.434481143951416
40433 2023-02-16,23:39:59.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 4/15000, loss = 1.3726826906204224
37035 2023-02-16,23:39:59.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 896/15000, loss = 1.3693084716796875
37757 2023-02-16,23:39:59.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.343677282333374
37268 2023-02-16,23:39:59.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.3634337186813354
36932 2023-02-16,23:39:59.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 925/15000, loss = 1.3685044050216675
37533 2023-02-16,23:39:59.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.373207449913025
40293 2023-02-16,23:39:59.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37152 2023-02-16,23:39:59.774 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 817/15000, loss = 1.420049786567688
38255 2023-02-16,23:39:59.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3788039684295654
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
40554 2023-02-16,23:39:59.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 0/15000, loss = 1.4099364280700684
37393 2023-02-16,23:39:59.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4237470626831055
38108 2023-02-16,23:39:59.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3955717086791992
38487 2023-02-16,23:39:59.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 486/15000, loss = 1.4035855531692505
38362 2023-02-16,23:39:59.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3966261148452759
36814 2023-02-16,23:39:59.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1014/15000, loss = 1.3664755821228027
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 100, in <module>
    trainer.train_model()
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 92, in train_model
    v_params = tuple([torch.randn_like(p) if p.requires_grad == True else torch.zeros_like(p) for p in self.params])
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 92, in <listcomp>
    v_params = tuple([torch.randn_like(p) if p.requires_grad == True else torch.zeros_like(p) for p in self.params])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 5; 44.56 GiB total capacity; 1.21 GiB already allocated; 24.56 MiB free; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

37862 2023-02-16,23:39:59.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.398185133934021
37983 2023-02-16,23:39:59.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3815722465515137
40433 2023-02-16,23:40:00.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 5/15000, loss = 1.425908088684082
37035 2023-02-16,23:40:00.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 897/15000, loss = 1.3925211429595947
37757 2023-02-16,23:40:00.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3941419124603271
37268 2023-02-16,23:40:00.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.3790563344955444
36932 2023-02-16,23:40:00.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 926/15000, loss = 1.3909587860107422
37533 2023-02-16,23:40:00.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.374558448791504
40293 2023-02-16,23:40:00.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37152 2023-02-16,23:40:00.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 818/15000, loss = 1.3877148628234863
38255 2023-02-16,23:40:00.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3481452465057373
37393 2023-02-16,23:40:00.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4172697067260742
38108 2023-02-16,23:40:00.301 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3751301765441895
38487 2023-02-16,23:40:00.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 487/15000, loss = 1.3978798389434814
38362 2023-02-16,23:40:00.332 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3960967063903809
36814 2023-02-16,23:40:00.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1015/15000, loss = 1.4041268825531006
/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
Traceback (most recent call last):
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/data/wyz/FedNLP/experiments/centralized/transformer_exps/main_tc.py", line 100, in <module>
    trainer.train_model()
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 104, in train_model
    loss, jvp = fc.jvp(f, (self.params,), (v_params,))
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 788, in jvp
    return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/vmap.py", line 35, in fn
    return f(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/eager_transforms.py", line 837, in _jvp_with_argnums
    result_duals = func(*duals)
  File "/data/wyz/FedNLP/training/tc_transformer_trainer.py", line 332, in functional_get_loss
    y = model(params,buffers, x)[0]
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/functorch/_src/make_functional.py", line 282, in forward
    return self.stateless_model(*args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 779, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/context.py", line 108, in wrapper_func
    results = f(self, *args, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 597, in forward
    return_dict=return_dict,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 365, in forward
    x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 306, in forward
    output_attentions=output_attentions,
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 215, in forward
    q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/transformers/adapters/lora.py", line 251, in forward
    return F.linear(x, T(self.weight), bias=self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

37862 2023-02-16,23:40:00.448 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.433650255203247
37983 2023-02-16,23:40:00.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3613252639770508
40433 2023-02-16,23:40:00.461 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 6/15000, loss = 1.3965091705322266
37035 2023-02-16,23:40:00.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 898/15000, loss = 1.4202604293823242
37757 2023-02-16,23:40:00.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3892325162887573
37268 2023-02-16,23:40:00.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.3843647241592407
37533 2023-02-16,23:40:00.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4070371389389038
36932 2023-02-16,23:40:00.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 927/15000, loss = 1.3893768787384033
40293 2023-02-16,23:40:00.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37152 2023-02-16,23:40:00.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 819/15000, loss = 1.4201948642730713
38255 2023-02-16,23:40:00.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4018124341964722
37393 2023-02-16,23:40:00.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.4079252481460571
38108 2023-02-16,23:40:00.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.373931884765625
38487 2023-02-16,23:40:00.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 488/15000, loss = 1.4439440965652466
38362 2023-02-16,23:40:00.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3870322704315186
36814 2023-02-16,23:40:00.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1016/15000, loss = 1.3548492193222046
37983 2023-02-16,23:40:00.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4241424798965454
40433 2023-02-16,23:40:00.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 7/15000, loss = 1.3797667026519775
37035 2023-02-16,23:40:00.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 899/15000, loss = 1.363790512084961
37757 2023-02-16,23:40:00.960 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3842930793762207
37862 2023-02-16,23:40:00.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4414206743240356
37268 2023-02-16,23:40:01.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.4390687942504883
37533 2023-02-16,23:40:01.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3993213176727295
36932 2023-02-16,23:40:01.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 928/15000, loss = 1.4043675661087036
40293 2023-02-16,23:40:01.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
37152 2023-02-16,23:40:01.132 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 820/15000, loss = 1.388441801071167
38255 2023-02-16,23:40:01.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.420278787612915
37393 2023-02-16,23:40:01.167 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3724477291107178
38108 2023-02-16,23:40:01.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.388215184211731
38487 2023-02-16,23:40:01.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 489/15000, loss = 1.3986783027648926
38362 2023-02-16,23:40:01.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.4075320959091187
36814 2023-02-16,23:40:01.270 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1017/15000, loss = 1.3929866552352905
37983 2023-02-16,23:40:01.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3920403718948364
40433 2023-02-16,23:40:01.367 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 8/15000, loss = 1.3761073350906372
37035 2023-02-16,23:40:01.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 900/15000, loss = 1.379438877105713
37757 2023-02-16,23:40:01.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4147292375564575
37862 2023-02-16,23:40:01.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4093307256698608
37533 2023-02-16,23:40:01.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3822647333145142
37268 2023-02-16,23:40:01.496 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.3450486660003662
36932 2023-02-16,23:40:01.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 929/15000, loss = 1.321342945098877
40293 2023-02-16,23:40:01.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
37152 2023-02-16,23:40:01.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 821/15000, loss = 1.3570876121520996
38255 2023-02-16,23:40:01.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4114848375320435
37393 2023-02-16,23:40:01.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3278868198394775
38108 2023-02-16,23:40:01.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3911259174346924
38487 2023-02-16,23:40:01.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 490/15000, loss = 1.3509489297866821
38362 2023-02-16,23:40:01.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4061471223831177
36814 2023-02-16,23:40:01.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1018/15000, loss = 1.3596075773239136
37983 2023-02-16,23:40:01.807 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.387373447418213
40433 2023-02-16,23:40:01.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 9/15000, loss = 1.4008947610855103
37035 2023-02-16,23:40:01.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 901/15000, loss = 1.373079538345337
37757 2023-02-16,23:40:01.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.412001371383667
37862 2023-02-16,23:40:01.864 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4361670017242432
37533 2023-02-16,23:40:01.920 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4292224645614624
37268 2023-02-16,23:40:01.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.3778072595596313
36932 2023-02-16,23:40:01.972 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 930/15000, loss = 1.3562802076339722
40293 2023-02-16,23:40:02.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37152 2023-02-16,23:40:02.035 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 822/15000, loss = 1.4167970418930054
38255 2023-02-16,23:40:02.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3951141834259033
37393 2023-02-16,23:40:02.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.3642890453338623
38108 2023-02-16,23:40:02.105 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.383604645729065
38362 2023-02-16,23:40:02.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.4150515794754028
38487 2023-02-16,23:40:02.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 491/15000, loss = 1.3496906757354736
36814 2023-02-16,23:40:02.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1019/15000, loss = 1.4403111934661865
37983 2023-02-16,23:40:02.259 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3213285207748413
40433 2023-02-16,23:40:02.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 10/15000, loss = 1.3999075889587402
37035 2023-02-16,23:40:02.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 902/15000, loss = 1.378448724746704
37757 2023-02-16,23:40:02.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4114370346069336
37862 2023-02-16,23:40:02.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4393360614776611
37533 2023-02-16,23:40:02.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3908478021621704
37268 2023-02-16,23:40:02.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.424365520477295
36932 2023-02-16,23:40:02.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 931/15000, loss = 1.433292031288147
40293 2023-02-16,23:40:02.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
37152 2023-02-16,23:40:02.488 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 823/15000, loss = 1.4327747821807861
38255 2023-02-16,23:40:02.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3947200775146484
37393 2023-02-16,23:40:02.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.3751592636108398
38108 2023-02-16,23:40:02.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3629798889160156
38362 2023-02-16,23:40:02.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4136252403259277
38487 2023-02-16,23:40:02.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 492/15000, loss = 1.4208623170852661
36814 2023-02-16,23:40:02.595 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1020/15000, loss = 1.4254961013793945
37983 2023-02-16,23:40:02.712 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4101277589797974
40433 2023-02-16,23:40:02.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 11/15000, loss = 1.387669563293457
37035 2023-02-16,23:40:02.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 903/15000, loss = 1.3775659799575806
37757 2023-02-16,23:40:02.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.421995759010315
37862 2023-02-16,23:40:02.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.373647928237915
37533 2023-02-16,23:40:02.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4113197326660156
37268 2023-02-16,23:40:02.853 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.3782411813735962
36932 2023-02-16,23:40:02.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 932/15000, loss = 1.3656461238861084
37152 2023-02-16,23:40:02.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 824/15000, loss = 1.4107234477996826
37393 2023-02-16,23:40:02.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.39582097530365
38255 2023-02-16,23:40:02.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3925296068191528
40293 2023-02-16,23:40:02.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
38108 2023-02-16,23:40:03.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3676422834396362
38362 2023-02-16,23:40:03.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4249223470687866
38487 2023-02-16,23:40:03.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 493/15000, loss = 1.4323575496673584
36814 2023-02-16,23:40:03.022 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1021/15000, loss = 1.4061428308486938
37983 2023-02-16,23:40:03.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3771188259124756
37035 2023-02-16,23:40:03.188 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 904/15000, loss = 1.4459030628204346
40433 2023-02-16,23:40:03.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 12/15000, loss = 1.4353150129318237
37757 2023-02-16,23:40:03.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4223294258117676
37862 2023-02-16,23:40:03.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.405360460281372
37533 2023-02-16,23:40:03.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4301570653915405
37268 2023-02-16,23:40:03.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.436476469039917
36932 2023-02-16,23:40:03.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 933/15000, loss = 1.384531855583191
37152 2023-02-16,23:40:03.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 825/15000, loss = 1.3999137878417969
37393 2023-02-16,23:40:03.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3627225160598755
38255 2023-02-16,23:40:03.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4456857442855835
40293 2023-02-16,23:40:03.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
38108 2023-02-16,23:40:03.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3506591320037842
38362 2023-02-16,23:40:03.474 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.379655361175537
38487 2023-02-16,23:40:03.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 494/15000, loss = 1.3883448839187622
36814 2023-02-16,23:40:03.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1022/15000, loss = 1.3856620788574219
37983 2023-02-16,23:40:03.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4134602546691895
37035 2023-02-16,23:40:03.643 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 905/15000, loss = 1.3577773571014404
40433 2023-02-16,23:40:03.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 13/15000, loss = 1.3896468877792358
37757 2023-02-16,23:40:03.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4020097255706787
37862 2023-02-16,23:40:03.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.42236328125
37533 2023-02-16,23:40:03.740 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3646907806396484
37268 2023-02-16,23:40:03.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3938000202178955
36932 2023-02-16,23:40:03.792 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 934/15000, loss = 1.4352083206176758
37152 2023-02-16,23:40:03.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 826/15000, loss = 1.416763186454773
37393 2023-02-16,23:40:03.861 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.35979163646698
38255 2023-02-16,23:40:03.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.375950813293457
40293 2023-02-16,23:40:03.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
38108 2023-02-16,23:40:03.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4132217168807983
38362 2023-02-16,23:40:03.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4087930917739868
38487 2023-02-16,23:40:03.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 495/15000, loss = 1.4322948455810547
36814 2023-02-16,23:40:03.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1023/15000, loss = 1.38861083984375
37035 2023-02-16,23:40:04.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 906/15000, loss = 1.4015105962753296
37983 2023-02-16,23:40:04.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3835874795913696
40433 2023-02-16,23:40:04.125 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 14/15000, loss = 1.3300247192382812
37757 2023-02-16,23:40:04.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4056369066238403
37862 2023-02-16,23:40:04.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4197590351104736
37268 2023-02-16,23:40:04.194 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.3617796897888184
37533 2023-02-16,23:40:04.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3687105178833008
36932 2023-02-16,23:40:04.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 935/15000, loss = 1.407405138015747
37152 2023-02-16,23:40:04.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 827/15000, loss = 1.3959410190582275
37393 2023-02-16,23:40:04.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.394911766052246
38255 2023-02-16,23:40:04.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.35408616065979
40293 2023-02-16,23:40:04.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
38108 2023-02-16,23:40:04.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.381414532661438
38362 2023-02-16,23:40:04.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3777917623519897
38487 2023-02-16,23:40:04.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 496/15000, loss = 1.3577057123184204
36814 2023-02-16,23:40:04.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1024/15000, loss = 1.422666311264038
37035 2023-02-16,23:40:04.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 907/15000, loss = 1.4059070348739624
37983 2023-02-16,23:40:04.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.361035704612732
40433 2023-02-16,23:40:04.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 15/15000, loss = 1.3662636280059814
37757 2023-02-16,23:40:04.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.367405652999878
37862 2023-02-16,23:40:04.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.436310052871704
37268 2023-02-16,23:40:04.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4068942070007324
37533 2023-02-16,23:40:04.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4062227010726929
36932 2023-02-16,23:40:04.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 936/15000, loss = 1.3863143920898438
37152 2023-02-16,23:40:04.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 828/15000, loss = 1.4088690280914307
37393 2023-02-16,23:40:04.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3620535135269165
38255 2023-02-16,23:40:04.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4011573791503906
40293 2023-02-16,23:40:04.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
38108 2023-02-16,23:40:04.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4369710683822632
38362 2023-02-16,23:40:04.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.374781608581543
38487 2023-02-16,23:40:04.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 497/15000, loss = 1.3848953247070312
36814 2023-02-16,23:40:04.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1025/15000, loss = 1.423581600189209
37035 2023-02-16,23:40:04.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 908/15000, loss = 1.4266321659088135
37983 2023-02-16,23:40:04.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4273896217346191
40433 2023-02-16,23:40:05.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 16/15000, loss = 1.422603726387024
37757 2023-02-16,23:40:05.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4190597534179688
37862 2023-02-16,23:40:05.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4014689922332764
37268 2023-02-16,23:40:05.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.3962364196777344
37533 2023-02-16,23:40:05.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3269847631454468
36932 2023-02-16,23:40:05.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 937/15000, loss = 1.3811315298080444
37152 2023-02-16,23:40:05.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 829/15000, loss = 1.3977322578430176
37393 2023-02-16,23:40:05.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.360312819480896
38255 2023-02-16,23:40:05.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3605570793151855
40293 2023-02-16,23:40:05.247 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
38108 2023-02-16,23:40:05.298 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.433165192604065
38362 2023-02-16,23:40:05.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3813297748565674
38487 2023-02-16,23:40:05.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 498/15000, loss = 1.3791024684906006
36814 2023-02-16,23:40:05.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1026/15000, loss = 1.4003429412841797
37035 2023-02-16,23:40:05.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 909/15000, loss = 1.4300991296768188
37983 2023-02-16,23:40:05.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4228202104568481
40433 2023-02-16,23:40:05.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 17/15000, loss = 1.4182014465332031
37862 2023-02-16,23:40:05.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4215534925460815
37757 2023-02-16,23:40:05.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4364688396453857
37268 2023-02-16,23:40:05.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.4234199523925781
37533 2023-02-16,23:40:05.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3806371688842773
36932 2023-02-16,23:40:05.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 938/15000, loss = 1.442360758781433
37393 2023-02-16,23:40:05.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.3899011611938477
37152 2023-02-16,23:40:05.682 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 830/15000, loss = 1.4143906831741333
38255 2023-02-16,23:40:05.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3796238899230957
40293 2023-02-16,23:40:05.702 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
38108 2023-02-16,23:40:05.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4312571287155151
38362 2023-02-16,23:40:05.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.3253819942474365
38487 2023-02-16,23:40:05.756 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 499/15000, loss = 1.3782528638839722
36814 2023-02-16,23:40:05.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1027/15000, loss = 1.3418505191802979
37035 2023-02-16,23:40:05.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 910/15000, loss = 1.4191081523895264
37983 2023-02-16,23:40:05.906 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.411156177520752
40433 2023-02-16,23:40:05.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 18/15000, loss = 1.4143790006637573
37757 2023-02-16,23:40:05.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4205384254455566
37862 2023-02-16,23:40:05.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3963449001312256
37268 2023-02-16,23:40:06.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.407944917678833
37533 2023-02-16,23:40:06.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3674474954605103
36932 2023-02-16,23:40:06.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 939/15000, loss = 1.3860753774642944
37393 2023-02-16,23:40:06.110 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3766002655029297
38255 2023-02-16,23:40:06.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4419485330581665
37152 2023-02-16,23:40:06.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 831/15000, loss = 1.367356777191162
40293 2023-02-16,23:40:06.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
38108 2023-02-16,23:40:06.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3671761751174927
38362 2023-02-16,23:40:06.213 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3789799213409424
38487 2023-02-16,23:40:06.214 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 500/15000, loss = 1.3931868076324463
36814 2023-02-16,23:40:06.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1028/15000, loss = 1.4089640378952026
37035 2023-02-16,23:40:06.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 911/15000, loss = 1.4104254245758057
37983 2023-02-16,23:40:06.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4537785053253174
40433 2023-02-16,23:40:06.408 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 19/15000, loss = 1.3646163940429688
37757 2023-02-16,23:40:06.423 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4117332696914673
37862 2023-02-16,23:40:06.424 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4568761587142944
37268 2023-02-16,23:40:06.475 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.3787453174591064
37533 2023-02-16,23:40:06.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4173246622085571
36932 2023-02-16,23:40:06.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 940/15000, loss = 1.400987148284912
38255 2023-02-16,23:40:06.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3801093101501465
37393 2023-02-16,23:40:06.567 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4053179025650024
37152 2023-02-16,23:40:06.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 832/15000, loss = 1.444697380065918
40293 2023-02-16,23:40:06.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
38108 2023-02-16,23:40:06.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3783560991287231
38362 2023-02-16,23:40:06.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3481974601745605
38487 2023-02-16,23:40:06.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 501/15000, loss = 1.3941391706466675
36814 2023-02-16,23:40:06.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1029/15000, loss = 1.390065312385559
37035 2023-02-16,23:40:06.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 912/15000, loss = 1.4081381559371948
37983 2023-02-16,23:40:06.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4049365520477295
40433 2023-02-16,23:40:06.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 20/15000, loss = 1.4156324863433838
37757 2023-02-16,23:40:06.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3798563480377197
37862 2023-02-16,23:40:06.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3537232875823975
37268 2023-02-16,23:40:06.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3519824743270874
37533 2023-02-16,23:40:06.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4250624179840088
38255 2023-02-16,23:40:06.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.412326455116272
36932 2023-02-16,23:40:06.988 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 941/15000, loss = 1.398817539215088
37393 2023-02-16,23:40:07.023 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.4049841165542603
37152 2023-02-16,23:40:07.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 833/15000, loss = 1.356449007987976
40293 2023-02-16,23:40:07.071 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
38108 2023-02-16,23:40:07.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.420111060142517
38362 2023-02-16,23:40:07.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4018522500991821
38487 2023-02-16,23:40:07.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 502/15000, loss = 1.3667100667953491
36814 2023-02-16,23:40:07.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1030/15000, loss = 1.3660037517547607
37035 2023-02-16,23:40:07.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 913/15000, loss = 1.3651511669158936
37983 2023-02-16,23:40:07.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3923598527908325
40433 2023-02-16,23:40:07.323 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 21/15000, loss = 1.3501733541488647
37757 2023-02-16,23:40:07.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.388935923576355
37862 2023-02-16,23:40:07.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4199529886245728
37268 2023-02-16,23:40:07.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4117579460144043
37533 2023-02-16,23:40:07.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3317606449127197
38255 2023-02-16,23:40:07.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3891016244888306
36932 2023-02-16,23:40:07.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 942/15000, loss = 1.3931982517242432
37393 2023-02-16,23:40:07.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.4073766469955444
37152 2023-02-16,23:40:07.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 834/15000, loss = 1.3706343173980713
40293 2023-02-16,23:40:07.524 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
38108 2023-02-16,23:40:07.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3472164869308472
38362 2023-02-16,23:40:07.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4204156398773193
38487 2023-02-16,23:40:07.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 503/15000, loss = 1.4108030796051025
36814 2023-02-16,23:40:07.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1031/15000, loss = 1.3764156103134155
37035 2023-02-16,23:40:07.717 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 914/15000, loss = 1.403426170349121
37983 2023-02-16,23:40:07.731 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4026830196380615
40433 2023-02-16,23:40:07.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 22/15000, loss = 1.4565531015396118
37757 2023-02-16,23:40:07.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3811346292495728
37862 2023-02-16,23:40:07.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3789008855819702
37268 2023-02-16,23:40:07.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.4353007078170776
37533 2023-02-16,23:40:07.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4460302591323853
38255 2023-02-16,23:40:07.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3878870010375977
36932 2023-02-16,23:40:07.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 943/15000, loss = 1.4818212985992432
37393 2023-02-16,23:40:07.929 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.3518537282943726
37152 2023-02-16,23:40:07.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 835/15000, loss = 1.3621100187301636
40293 2023-02-16,23:40:07.977 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
38108 2023-02-16,23:40:07.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3325797319412231
38362 2023-02-16,23:40:08.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4115172624588013
38487 2023-02-16,23:40:08.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 504/15000, loss = 1.3859202861785889
36814 2023-02-16,23:40:08.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1032/15000, loss = 1.4112846851348877
37983 2023-02-16,23:40:08.151 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4071259498596191
37035 2023-02-16,23:40:08.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 915/15000, loss = 1.37910795211792
40433 2023-02-16,23:40:08.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 23/15000, loss = 1.367962121963501
37757 2023-02-16,23:40:08.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3724662065505981
37862 2023-02-16,23:40:08.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3173905611038208
37268 2023-02-16,23:40:08.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.432183861732483
37533 2023-02-16,23:40:08.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3775627613067627
38255 2023-02-16,23:40:08.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3808197975158691
36932 2023-02-16,23:40:08.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 944/15000, loss = 1.3546178340911865
37393 2023-02-16,23:40:08.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4025931358337402
37152 2023-02-16,23:40:08.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 836/15000, loss = 1.4323618412017822
40293 2023-02-16,23:40:08.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
38108 2023-02-16,23:40:08.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4165806770324707
38362 2023-02-16,23:40:08.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.395140528678894
38487 2023-02-16,23:40:08.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 505/15000, loss = 1.3958882093429565
36814 2023-02-16,23:40:08.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1033/15000, loss = 1.3995676040649414
37983 2023-02-16,23:40:08.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4040440320968628
37035 2023-02-16,23:40:08.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 916/15000, loss = 1.4578604698181152
40433 2023-02-16,23:40:08.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 24/15000, loss = 1.4173414707183838
37757 2023-02-16,23:40:08.696 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.422714352607727
37862 2023-02-16,23:40:08.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3648558855056763
37268 2023-02-16,23:40:08.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.4080731868743896
37533 2023-02-16,23:40:08.753 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4018405675888062
38255 2023-02-16,23:40:08.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.386427879333496
36932 2023-02-16,23:40:08.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 945/15000, loss = 1.3657276630401611
37393 2023-02-16,23:40:08.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.3595820665359497
37152 2023-02-16,23:40:08.865 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 837/15000, loss = 1.3403574228286743
38487 2023-02-16,23:40:08.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 506/15000, loss = 1.4011305570602417
40293 2023-02-16,23:40:08.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
38108 2023-02-16,23:40:08.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.400793433189392
38362 2023-02-16,23:40:08.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.394770860671997
36814 2023-02-16,23:40:08.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1034/15000, loss = 1.3569117784500122
37983 2023-02-16,23:40:09.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4008865356445312
37035 2023-02-16,23:40:09.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 917/15000, loss = 1.3722203969955444
40433 2023-02-16,23:40:09.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 25/15000, loss = 1.3867502212524414
37757 2023-02-16,23:40:09.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.398302435874939
37862 2023-02-16,23:40:09.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4390172958374023
37268 2023-02-16,23:40:09.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4538319110870361
37533 2023-02-16,23:40:09.179 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4347567558288574
38255 2023-02-16,23:40:09.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4403001070022583
36932 2023-02-16,23:40:09.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 946/15000, loss = 1.3678412437438965
37393 2023-02-16,23:40:09.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.4185740947723389
37152 2023-02-16,23:40:09.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 838/15000, loss = 1.3617401123046875
38487 2023-02-16,23:40:09.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 507/15000, loss = 1.4304051399230957
40293 2023-02-16,23:40:09.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
38108 2023-02-16,23:40:09.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3730894327163696
38362 2023-02-16,23:40:09.366 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3924381732940674
36814 2023-02-16,23:40:09.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1035/15000, loss = 1.4060105085372925
37983 2023-02-16,23:40:09.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3981786966323853
37035 2023-02-16,23:40:09.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 918/15000, loss = 1.3413242101669312
40433 2023-02-16,23:40:09.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 26/15000, loss = 1.395373821258545
37533 2023-02-16,23:40:09.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4206891059875488
37757 2023-02-16,23:40:09.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4025315046310425
37862 2023-02-16,23:40:09.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.397095799446106
37268 2023-02-16,23:40:09.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4130882024765015
37393 2023-02-16,23:40:09.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.363951325416565
38255 2023-02-16,23:40:09.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.381441593170166
36932 2023-02-16,23:40:09.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 947/15000, loss = 1.3472498655319214
37152 2023-02-16,23:40:09.772 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 839/15000, loss = 1.4107953310012817
38487 2023-02-16,23:40:09.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 508/15000, loss = 1.3800050020217896
40293 2023-02-16,23:40:09.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
38108 2023-02-16,23:40:09.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4172708988189697
38362 2023-02-16,23:40:09.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4455879926681519
36814 2023-02-16,23:40:09.850 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1036/15000, loss = 1.416931390762329
37983 2023-02-16,23:40:09.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.433716893196106
37035 2023-02-16,23:40:09.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 919/15000, loss = 1.3780959844589233
40433 2023-02-16,23:40:10.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 27/15000, loss = 1.3494969606399536
37533 2023-02-16,23:40:10.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4031891822814941
37757 2023-02-16,23:40:10.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3681178092956543
37862 2023-02-16,23:40:10.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3437376022338867
37268 2023-02-16,23:40:10.085 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.359276533126831
38255 2023-02-16,23:40:10.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3954780101776123
36932 2023-02-16,23:40:10.138 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 948/15000, loss = 1.4101641178131104
37393 2023-02-16,23:40:10.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.3781318664550781
38487 2023-02-16,23:40:10.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 509/15000, loss = 1.4247592687606812
37152 2023-02-16,23:40:10.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 840/15000, loss = 1.4141381978988647
40293 2023-02-16,23:40:10.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
38108 2023-02-16,23:40:10.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.4343841075897217
38362 2023-02-16,23:40:10.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3759459257125854
36814 2023-02-16,23:40:10.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1037/15000, loss = 1.4226914644241333
37983 2023-02-16,23:40:10.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.441534399986267
37035 2023-02-16,23:40:10.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 920/15000, loss = 1.4587315320968628
40433 2023-02-16,23:40:10.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 28/15000, loss = 1.4305448532104492
37533 2023-02-16,23:40:10.515 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4476714134216309
37757 2023-02-16,23:40:10.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3714935779571533
37862 2023-02-16,23:40:10.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3941538333892822
37268 2023-02-16,23:40:10.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3911681175231934
38255 2023-02-16,23:40:10.549 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3750892877578735
37393 2023-02-16,23:40:10.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.3854376077651978
36932 2023-02-16,23:40:10.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 949/15000, loss = 1.3773555755615234
38487 2023-02-16,23:40:10.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 510/15000, loss = 1.3784409761428833
37152 2023-02-16,23:40:10.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 841/15000, loss = 1.391391396522522
38108 2023-02-16,23:40:10.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3816654682159424
40293 2023-02-16,23:40:10.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
38362 2023-02-16,23:40:10.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3540832996368408
36814 2023-02-16,23:40:10.761 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1038/15000, loss = 1.40712308883667
37983 2023-02-16,23:40:10.849 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4092110395431519
37035 2023-02-16,23:40:10.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 921/15000, loss = 1.377859115600586
40433 2023-02-16,23:40:10.897 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 29/15000, loss = 1.4209649562835693
37533 2023-02-16,23:40:10.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4037150144577026
37757 2023-02-16,23:40:10.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.381291151046753
37862 2023-02-16,23:40:10.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3890622854232788
37268 2023-02-16,23:40:11.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.372362732887268
38255 2023-02-16,23:40:11.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.373880386352539
37393 2023-02-16,23:40:11.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.4394121170043945
36932 2023-02-16,23:40:11.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 950/15000, loss = 1.3540115356445312
38487 2023-02-16,23:40:11.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 511/15000, loss = 1.3981784582138062
37152 2023-02-16,23:40:11.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 842/15000, loss = 1.3831688165664673
38108 2023-02-16,23:40:11.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3614857196807861
38362 2023-02-16,23:40:11.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4013657569885254
40293 2023-02-16,23:40:11.158 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
36814 2023-02-16,23:40:11.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1039/15000, loss = 1.3909504413604736
37035 2023-02-16,23:40:11.291 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 922/15000, loss = 1.3483911752700806
37983 2023-02-16,23:40:11.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4361720085144043
40433 2023-02-16,23:40:11.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 30/15000, loss = 1.3655815124511719
37533 2023-02-16,23:40:11.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3687080144882202
37757 2023-02-16,23:40:11.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3993154764175415
37862 2023-02-16,23:40:11.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3842378854751587
37268 2023-02-16,23:40:11.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.4138296842575073
38255 2023-02-16,23:40:11.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3882129192352295
37393 2023-02-16,23:40:11.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.345291256904602
36932 2023-02-16,23:40:11.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 951/15000, loss = 1.4234064817428589
38487 2023-02-16,23:40:11.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 512/15000, loss = 1.3561679124832153
37152 2023-02-16,23:40:11.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 843/15000, loss = 1.3468282222747803
38108 2023-02-16,23:40:11.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4241007566452026
38362 2023-02-16,23:40:11.614 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.3605140447616577
40293 2023-02-16,23:40:11.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
36814 2023-02-16,23:40:11.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1040/15000, loss = 1.4452228546142578
40433 2023-02-16,23:40:11.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 31/15000, loss = 1.4135630130767822
37035 2023-02-16,23:40:11.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 923/15000, loss = 1.3901164531707764
37983 2023-02-16,23:40:11.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4393383264541626
37533 2023-02-16,23:40:11.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3738700151443481
37757 2023-02-16,23:40:11.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4600785970687866
37862 2023-02-16,23:40:11.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4146111011505127
37268 2023-02-16,23:40:11.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.3708641529083252
38255 2023-02-16,23:40:11.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3910439014434814
37393 2023-02-16,23:40:11.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.3776429891586304
36932 2023-02-16,23:40:11.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 952/15000, loss = 1.4050166606903076
38487 2023-02-16,23:40:11.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 513/15000, loss = 1.4261059761047363
37152 2023-02-16,23:40:12.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 844/15000, loss = 1.37617826461792
38108 2023-02-16,23:40:12.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3921180963516235
38362 2023-02-16,23:40:12.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3795409202575684
40293 2023-02-16,23:40:12.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
36814 2023-02-16,23:40:12.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1041/15000, loss = 1.4161369800567627
37983 2023-02-16,23:40:12.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.373566746711731
40433 2023-02-16,23:40:12.192 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 32/15000, loss = 1.3815233707427979
37035 2023-02-16,23:40:12.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 924/15000, loss = 1.4098155498504639
37533 2023-02-16,23:40:12.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4172028303146362
37757 2023-02-16,23:40:12.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4093961715698242
37862 2023-02-16,23:40:12.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4120440483093262
37268 2023-02-16,23:40:12.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.3946495056152344
38255 2023-02-16,23:40:12.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3834996223449707
37393 2023-02-16,23:40:12.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.4263273477554321
36932 2023-02-16,23:40:12.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 953/15000, loss = 1.3826440572738647
38487 2023-02-16,23:40:12.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 514/15000, loss = 1.3836586475372314
37152 2023-02-16,23:40:12.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 845/15000, loss = 1.3597248792648315
38108 2023-02-16,23:40:12.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.387431025505066
38362 2023-02-16,23:40:12.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4419835805892944
40293 2023-02-16,23:40:12.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
36814 2023-02-16,23:40:12.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1042/15000, loss = 1.398760437965393
40433 2023-02-16,23:40:12.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 33/15000, loss = 1.3730026483535767
37983 2023-02-16,23:40:12.630 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4053194522857666
37035 2023-02-16,23:40:12.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 925/15000, loss = 1.3667497634887695
37533 2023-02-16,23:40:12.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.387378454208374
37757 2023-02-16,23:40:12.787 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4097193479537964
37862 2023-02-16,23:40:12.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.411348581314087
37268 2023-02-16,23:40:12.814 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.432986855506897
38255 2023-02-16,23:40:12.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3629493713378906
37393 2023-02-16,23:40:12.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.379128336906433
38487 2023-02-16,23:40:12.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 515/15000, loss = 1.3965606689453125
36932 2023-02-16,23:40:12.858 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 954/15000, loss = 1.4093879461288452
40293 2023-02-16,23:40:12.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37152 2023-02-16,23:40:12.952 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 846/15000, loss = 1.432813286781311
38108 2023-02-16,23:40:12.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3213059902191162
38362 2023-02-16,23:40:12.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3801429271697998
36814 2023-02-16,23:40:13.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1043/15000, loss = 1.3692381381988525
40433 2023-02-16,23:40:13.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 34/15000, loss = 1.354580044746399
37983 2023-02-16,23:40:13.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4222785234451294
37035 2023-02-16,23:40:13.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 926/15000, loss = 1.3885910511016846
37533 2023-02-16,23:40:13.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.419357419013977
37757 2023-02-16,23:40:13.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.372918963432312
37862 2023-02-16,23:40:13.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4220246076583862
37268 2023-02-16,23:40:13.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.39876127243042
37393 2023-02-16,23:40:13.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.437229871749878
38255 2023-02-16,23:40:13.274 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677070140838623
38487 2023-02-16,23:40:13.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 516/15000, loss = 1.3960429430007935
36932 2023-02-16,23:40:13.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 955/15000, loss = 1.3804419040679932
40293 2023-02-16,23:40:13.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
37152 2023-02-16,23:40:13.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 847/15000, loss = 1.4248971939086914
38108 2023-02-16,23:40:13.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4100661277770996
38362 2023-02-16,23:40:13.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.412212610244751
36814 2023-02-16,23:40:13.454 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1044/15000, loss = 1.4092185497283936
40433 2023-02-16,23:40:13.515 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 35/15000, loss = 1.389941930770874
37983 2023-02-16,23:40:13.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.419605016708374
37035 2023-02-16,23:40:13.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 927/15000, loss = 1.3894389867782593
37533 2023-02-16,23:40:13.588 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4095287322998047
37757 2023-02-16,23:40:13.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3746683597564697
37862 2023-02-16,23:40:13.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4223706722259521
37268 2023-02-16,23:40:13.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.3516361713409424
37393 2023-02-16,23:40:13.716 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3962793350219727
38255 2023-02-16,23:40:13.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3506144285202026
38487 2023-02-16,23:40:13.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 517/15000, loss = 1.3870657682418823
36932 2023-02-16,23:40:13.760 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 956/15000, loss = 1.4031848907470703
38362 2023-02-16,23:40:13.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3891469240188599
40293 2023-02-16,23:40:13.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
37152 2023-02-16,23:40:13.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 848/15000, loss = 1.4113997220993042
38108 2023-02-16,23:40:13.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3772038221359253
36814 2023-02-16,23:40:13.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1045/15000, loss = 1.3760696649551392
40433 2023-02-16,23:40:13.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 36/15000, loss = 1.3941622972488403
37983 2023-02-16,23:40:13.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4362603425979614
37035 2023-02-16,23:40:14.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 928/15000, loss = 1.403495192527771
37533 2023-02-16,23:40:14.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4070520401000977
37757 2023-02-16,23:40:14.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.407677173614502
37862 2023-02-16,23:40:14.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4019683599472046
37268 2023-02-16,23:40:14.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.421667456626892
37393 2023-02-16,23:40:14.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.3637664318084717
38255 2023-02-16,23:40:14.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4131877422332764
38487 2023-02-16,23:40:14.205 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 518/15000, loss = 1.407433271408081
36932 2023-02-16,23:40:14.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 957/15000, loss = 1.4330503940582275
38362 2023-02-16,23:40:14.293 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3879367113113403
40293 2023-02-16,23:40:14.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37152 2023-02-16,23:40:14.305 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 849/15000, loss = 1.4042109251022339
38108 2023-02-16,23:40:14.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.413498878479004
36814 2023-02-16,23:40:14.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1046/15000, loss = 1.4433233737945557
40433 2023-02-16,23:40:14.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 37/15000, loss = 1.3867847919464111
37983 2023-02-16,23:40:14.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.401286244392395
37035 2023-02-16,23:40:14.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 929/15000, loss = 1.3188440799713135
37533 2023-02-16,23:40:14.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3900765180587769
37757 2023-02-16,23:40:14.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3996894359588623
37862 2023-02-16,23:40:14.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4058268070220947
37268 2023-02-16,23:40:14.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3956267833709717
37393 2023-02-16,23:40:14.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4084928035736084
38255 2023-02-16,23:40:14.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3813972473144531
38487 2023-02-16,23:40:14.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 519/15000, loss = 1.4062831401824951
36932 2023-02-16,23:40:14.629 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 958/15000, loss = 1.3630294799804688
38362 2023-02-16,23:40:14.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.380840539932251
40293 2023-02-16,23:40:14.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37152 2023-02-16,23:40:14.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 850/15000, loss = 1.3949038982391357
38108 2023-02-16,23:40:14.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3836441040039062
36814 2023-02-16,23:40:14.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1047/15000, loss = 1.3363037109375
40433 2023-02-16,23:40:14.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 38/15000, loss = 1.3693640232086182
37983 2023-02-16,23:40:14.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4214529991149902
37035 2023-02-16,23:40:14.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 930/15000, loss = 1.3558980226516724
37533 2023-02-16,23:40:14.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4119995832443237
37757 2023-02-16,23:40:14.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.382541537284851
37862 2023-02-16,23:40:14.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3673421144485474
37268 2023-02-16,23:40:15.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.3344212770462036
37393 2023-02-16,23:40:15.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.396111011505127
38255 2023-02-16,23:40:15.009 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.436957836151123
38487 2023-02-16,23:40:15.037 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 520/15000, loss = 1.415045142173767
36932 2023-02-16,23:40:15.045 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 959/15000, loss = 1.4158408641815186
38362 2023-02-16,23:40:15.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3865115642547607
40293 2023-02-16,23:40:15.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37152 2023-02-16,23:40:15.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 851/15000, loss = 1.4516488313674927
38108 2023-02-16,23:40:15.154 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3610533475875854
36814 2023-02-16,23:40:15.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1048/15000, loss = 1.4469095468521118
40433 2023-02-16,23:40:15.219 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 39/15000, loss = 1.3522926568984985
37983 2023-02-16,23:40:15.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3962665796279907
37035 2023-02-16,23:40:15.253 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 931/15000, loss = 1.431352972984314
37533 2023-02-16,23:40:15.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.387410044670105
37757 2023-02-16,23:40:15.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4308356046676636
37862 2023-02-16,23:40:15.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4189972877502441
37268 2023-02-16,23:40:15.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.368046760559082
37393 2023-02-16,23:40:15.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.4249906539916992
38255 2023-02-16,23:40:15.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4332804679870605
38487 2023-02-16,23:40:15.456 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 521/15000, loss = 1.4136770963668823
36932 2023-02-16,23:40:15.464 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 960/15000, loss = 1.3776968717575073
38362 2023-02-16,23:40:15.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4403480291366577
40293 2023-02-16,23:40:15.547 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37152 2023-02-16,23:40:15.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 852/15000, loss = 1.3962953090667725
38108 2023-02-16,23:40:15.571 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4272500276565552
36814 2023-02-16,23:40:15.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1049/15000, loss = 1.3726252317428589
40433 2023-02-16,23:40:15.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 40/15000, loss = 1.3686505556106567
37983 2023-02-16,23:40:15.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4569743871688843
37035 2023-02-16,23:40:15.669 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 932/15000, loss = 1.362568736076355
37533 2023-02-16,23:40:15.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3320937156677246
37862 2023-02-16,23:40:15.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4365495443344116
37268 2023-02-16,23:40:15.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3878097534179688
37393 2023-02-16,23:40:15.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.4075943231582642
37757 2023-02-16,23:40:15.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3918708562850952
38255 2023-02-16,23:40:15.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4312825202941895
38487 2023-02-16,23:40:15.875 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 522/15000, loss = 1.4248627424240112
36932 2023-02-16,23:40:15.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 961/15000, loss = 1.3782025575637817
38362 2023-02-16,23:40:15.961 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.3815597295761108
40293 2023-02-16,23:40:15.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37152 2023-02-16,23:40:15.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 853/15000, loss = 1.3776764869689941
38108 2023-02-16,23:40:15.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.422684907913208
36814 2023-02-16,23:40:16.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1050/15000, loss = 1.3537912368774414
40433 2023-02-16,23:40:16.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 41/15000, loss = 1.4084277153015137
37983 2023-02-16,23:40:16.072 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.353591799736023
37035 2023-02-16,23:40:16.087 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 933/15000, loss = 1.3841793537139893
37533 2023-02-16,23:40:16.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4106115102767944
37862 2023-02-16,23:40:16.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4203873872756958
37268 2023-02-16,23:40:16.254 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.3577179908752441
37393 2023-02-16,23:40:16.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.37909734249115
37757 2023-02-16,23:40:16.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4120758771896362
38255 2023-02-16,23:40:16.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3671730756759644
38487 2023-02-16,23:40:16.292 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 523/15000, loss = 1.3798258304595947
36932 2023-02-16,23:40:16.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 962/15000, loss = 1.3900136947631836
38362 2023-02-16,23:40:16.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.3955632448196411
40293 2023-02-16,23:40:16.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37152 2023-02-16,23:40:16.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 854/15000, loss = 1.3581821918487549
38108 2023-02-16,23:40:16.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4110193252563477
36814 2023-02-16,23:40:16.442 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1051/15000, loss = 1.4109423160552979
40433 2023-02-16,23:40:16.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 42/15000, loss = 1.3699455261230469
37983 2023-02-16,23:40:16.490 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4200403690338135
37035 2023-02-16,23:40:16.504 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 934/15000, loss = 1.4324300289154053
37533 2023-02-16,23:40:16.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3844597339630127
37862 2023-02-16,23:40:16.645 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4116864204406738
37268 2023-02-16,23:40:16.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.3889633417129517
37393 2023-02-16,23:40:16.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3525437116622925
37757 2023-02-16,23:40:16.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.43281888961792
38255 2023-02-16,23:40:16.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3782098293304443
38487 2023-02-16,23:40:16.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 524/15000, loss = 1.4088904857635498
36932 2023-02-16,23:40:16.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 963/15000, loss = 1.371354579925537
38362 2023-02-16,23:40:16.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.375063180923462
40293 2023-02-16,23:40:16.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37152 2023-02-16,23:40:16.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 855/15000, loss = 1.3764508962631226
38108 2023-02-16,23:40:16.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4537767171859741
36814 2023-02-16,23:40:16.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1052/15000, loss = 1.3925907611846924
40433 2023-02-16,23:40:16.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 43/15000, loss = 1.4136492013931274
37983 2023-02-16,23:40:16.908 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.378900170326233
37035 2023-02-16,23:40:16.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 935/15000, loss = 1.4076822996139526
37533 2023-02-16,23:40:16.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3788450956344604
37862 2023-02-16,23:40:17.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3800015449523926
37268 2023-02-16,23:40:17.090 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.3408976793289185
37393 2023-02-16,23:40:17.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4106618165969849
37757 2023-02-16,23:40:17.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3644750118255615
38255 2023-02-16,23:40:17.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4200291633605957
38487 2023-02-16,23:40:17.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 525/15000, loss = 1.3778512477874756
36932 2023-02-16,23:40:17.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 964/15000, loss = 1.391613245010376
38362 2023-02-16,23:40:17.215 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.373893141746521
40293 2023-02-16,23:40:17.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
37152 2023-02-16,23:40:17.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 856/15000, loss = 1.426987648010254
38108 2023-02-16,23:40:17.242 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4048638343811035
36814 2023-02-16,23:40:17.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1053/15000, loss = 1.3752306699752808
40433 2023-02-16,23:40:17.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 44/15000, loss = 1.370940923690796
37983 2023-02-16,23:40:17.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3173511028289795
37035 2023-02-16,23:40:17.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 936/15000, loss = 1.383531928062439
37533 2023-02-16,23:40:17.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4143552780151367
37862 2023-02-16,23:40:17.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.389026403427124
37268 2023-02-16,23:40:17.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.3958373069763184
37393 2023-02-16,23:40:17.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.4344513416290283
37757 2023-02-16,23:40:17.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3689632415771484
38255 2023-02-16,23:40:17.517 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3472429513931274
38487 2023-02-16,23:40:17.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 526/15000, loss = 1.3748102188110352
36932 2023-02-16,23:40:17.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 965/15000, loss = 1.3643020391464233
38362 2023-02-16,23:40:17.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3883063793182373
40293 2023-02-16,23:40:17.634 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37152 2023-02-16,23:40:17.644 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 857/15000, loss = 1.3906667232513428
38108 2023-02-16,23:40:17.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3924462795257568
36814 2023-02-16,23:40:17.694 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1054/15000, loss = 1.4011032581329346
40433 2023-02-16,23:40:17.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 45/15000, loss = 1.4302952289581299
37983 2023-02-16,23:40:17.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3649910688400269
37035 2023-02-16,23:40:17.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 937/15000, loss = 1.3810584545135498
37533 2023-02-16,23:40:17.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3675158023834229
37862 2023-02-16,23:40:17.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3811458349227905
37393 2023-02-16,23:40:17.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4315128326416016
37757 2023-02-16,23:40:17.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4081413745880127
38255 2023-02-16,23:40:17.936 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3324651718139648
38487 2023-02-16,23:40:17.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 527/15000, loss = 1.3815696239471436
36932 2023-02-16,23:40:17.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 966/15000, loss = 1.337296962738037
37268 2023-02-16,23:40:17.980 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.3898299932479858
38362 2023-02-16,23:40:18.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.391114354133606
40293 2023-02-16,23:40:18.052 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37152 2023-02-16,23:40:18.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 858/15000, loss = 1.4143365621566772
38108 2023-02-16,23:40:18.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4026150703430176
36814 2023-02-16,23:40:18.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1055/15000, loss = 1.3695913553237915
40433 2023-02-16,23:40:18.141 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 46/15000, loss = 1.4235832691192627
37983 2023-02-16,23:40:18.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4388892650604248
37035 2023-02-16,23:40:18.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 938/15000, loss = 1.4417668581008911
37533 2023-02-16,23:40:18.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.394620656967163
37862 2023-02-16,23:40:18.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3723859786987305
37393 2023-02-16,23:40:18.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.409372091293335
37757 2023-02-16,23:40:18.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3261287212371826
38255 2023-02-16,23:40:18.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4165852069854736
38487 2023-02-16,23:40:18.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 528/15000, loss = 1.325406789779663
36932 2023-02-16,23:40:18.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 967/15000, loss = 1.4256926774978638
37268 2023-02-16,23:40:18.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.3563152551651
38362 2023-02-16,23:40:18.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.3835736513137817
40293 2023-02-16,23:40:18.470 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
37152 2023-02-16,23:40:18.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 859/15000, loss = 1.411328673362732
38108 2023-02-16,23:40:18.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4071823358535767
36814 2023-02-16,23:40:18.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1056/15000, loss = 1.3868553638458252
40433 2023-02-16,23:40:18.559 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 47/15000, loss = 1.3742704391479492
37983 2023-02-16,23:40:18.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3971015214920044
37035 2023-02-16,23:40:18.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 939/15000, loss = 1.3843258619308472
37533 2023-02-16,23:40:18.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3814940452575684
37862 2023-02-16,23:40:18.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.422645926475525
37393 2023-02-16,23:40:18.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4546177387237549
37757 2023-02-16,23:40:18.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3804869651794434
38255 2023-02-16,23:40:18.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.400856375694275
38487 2023-02-16,23:40:18.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 529/15000, loss = 1.3788833618164062
36932 2023-02-16,23:40:18.810 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 968/15000, loss = 1.3555943965911865
37268 2023-02-16,23:40:18.815 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.371382236480713
38362 2023-02-16,23:40:18.885 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.363020896911621
40293 2023-02-16,23:40:18.888 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
37152 2023-02-16,23:40:18.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 860/15000, loss = 1.4073373079299927
38108 2023-02-16,23:40:18.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4041002988815308
36814 2023-02-16,23:40:18.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1057/15000, loss = 1.416300654411316
40433 2023-02-16,23:40:18.978 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 48/15000, loss = 1.425612449645996
37983 2023-02-16,23:40:18.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3435925245285034
37035 2023-02-16,23:40:19.012 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 940/15000, loss = 1.3998135328292847
37533 2023-02-16,23:40:19.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3869035243988037
37862 2023-02-16,23:40:19.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3983020782470703
37393 2023-02-16,23:40:19.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4132239818572998
37757 2023-02-16,23:40:19.185 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3676567077636719
38255 2023-02-16,23:40:19.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3728933334350586
38487 2023-02-16,23:40:19.222 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 530/15000, loss = 1.3483192920684814
36932 2023-02-16,23:40:19.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 969/15000, loss = 1.4064909219741821
37268 2023-02-16,23:40:19.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.3944134712219238
38362 2023-02-16,23:40:19.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677178621292114
40293 2023-02-16,23:40:19.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
37152 2023-02-16,23:40:19.318 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 861/15000, loss = 1.4022293090820312
38108 2023-02-16,23:40:19.333 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4007865190505981
36814 2023-02-16,23:40:19.369 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1058/15000, loss = 1.3985726833343506
40433 2023-02-16,23:40:19.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 49/15000, loss = 1.3494389057159424
37983 2023-02-16,23:40:19.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3940682411193848
37035 2023-02-16,23:40:19.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 941/15000, loss = 1.3975309133529663
37533 2023-02-16,23:40:19.440 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3546746969223022
37862 2023-02-16,23:40:19.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4023998975753784
37393 2023-02-16,23:40:19.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.3610947132110596
37757 2023-02-16,23:40:19.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4180006980895996
38255 2023-02-16,23:40:19.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4173469543457031
38487 2023-02-16,23:40:19.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 531/15000, loss = 1.4018614292144775
36932 2023-02-16,23:40:19.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 970/15000, loss = 1.3819154500961304
37268 2023-02-16,23:40:19.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.4369672536849976
38362 2023-02-16,23:40:19.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3506619930267334
40293 2023-02-16,23:40:19.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
37152 2023-02-16,23:40:19.737 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 862/15000, loss = 1.3818845748901367
38108 2023-02-16,23:40:19.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3982454538345337
36814 2023-02-16,23:40:19.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1059/15000, loss = 1.339219570159912
40433 2023-02-16,23:40:19.817 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 50/15000, loss = 1.3879282474517822
37983 2023-02-16,23:40:19.837 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3890879154205322
37035 2023-02-16,23:40:19.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 942/15000, loss = 1.3943508863449097
37533 2023-02-16,23:40:19.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4261411428451538
37862 2023-02-16,23:40:19.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3681671619415283
37393 2023-02-16,23:40:20.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3908644914627075
37757 2023-02-16,23:40:20.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.426734447479248
38255 2023-02-16,23:40:20.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.434401035308838
38487 2023-02-16,23:40:20.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 532/15000, loss = 1.4202280044555664
36932 2023-02-16,23:40:20.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 971/15000, loss = 1.3363856077194214
37268 2023-02-16,23:40:20.074 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.4293915033340454
38362 2023-02-16,23:40:20.144 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4132111072540283
40293 2023-02-16,23:40:20.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
37152 2023-02-16,23:40:20.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 863/15000, loss = 1.4140089750289917
38108 2023-02-16,23:40:20.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4336060285568237
36814 2023-02-16,23:40:20.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 1060/15000, loss = 1.4037431478500366
40433 2023-02-16,23:40:20.236 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 51/15000, loss = 1.3497686386108398
37983 2023-02-16,23:40:20.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3842815160751343
37035 2023-02-16,23:40:20.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 943/15000, loss = 1.4804034233093262
37533 2023-02-16,23:40:20.278 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4063652753829956
38255 2023-02-16,23:40:20.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3815820217132568
38487 2023-02-16,23:40:20.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 533/15000, loss = 1.4115056991577148
38362 2023-02-16,23:40:20.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3815078735351562
40293 2023-02-16,23:40:20.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
38108 2023-02-16,23:40:20.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4414021968841553
40433 2023-02-16,23:40:20.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 52/15000, loss = 1.3417086601257324
37983 2023-02-16,23:40:20.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4145996570587158
37533 2023-02-16,23:40:20.623 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3653573989868164
38255 2023-02-16,23:40:20.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3614187240600586
38487 2023-02-16,23:40:20.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 534/15000, loss = 1.3951290845870972
38362 2023-02-16,23:40:20.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4370819330215454
40293 2023-02-16,23:40:20.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
38108 2023-02-16,23:40:20.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4094215631484985
37533 2023-02-16,23:40:20.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4185566902160645
37983 2023-02-16,23:40:20.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4121259450912476
40433 2023-02-16,23:40:20.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 53/15000, loss = 1.3645882606506348
38255 2023-02-16,23:40:20.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4241480827331543
38487 2023-02-16,23:40:20.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 535/15000, loss = 1.3948450088500977
40293 2023-02-16,23:40:20.940 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
38362 2023-02-16,23:40:20.953 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4332771301269531
Terminated
37533 2023-02-16,23:40:20.985 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.391801357269287
37983 2023-02-16,23:40:20.993 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.411308765411377
38108 2023-02-16,23:40:21.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4362480640411377
40433 2023-02-16,23:40:21.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 54/15000, loss = 1.4163726568222046
38255 2023-02-16,23:40:21.119 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3921618461608887
38487 2023-02-16,23:40:21.122 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 536/15000, loss = 1.3925597667694092
40293 2023-02-16,23:40:21.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
38362 2023-02-16,23:40:21.181 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.431331992149353
37533 2023-02-16,23:40:21.187 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4781489372253418
37983 2023-02-16,23:40:21.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.421906590461731
38108 2023-02-16,23:40:21.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4393805265426636
40433 2023-02-16,23:40:21.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 55/15000, loss = 1.3534841537475586
38487 2023-02-16,23:40:21.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 537/15000, loss = 1.4457495212554932
38255 2023-02-16,23:40:21.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3872671127319336
40293 2023-02-16,23:40:21.372 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
38362 2023-02-16,23:40:21.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.3673237562179565
37533 2023-02-16,23:40:21.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3688311576843262
37983 2023-02-16,23:40:21.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4222921133041382
38108 2023-02-16,23:40:21.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3735125064849854
40433 2023-02-16,23:40:21.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 56/15000, loss = 1.433931827545166
Terminated
38487 2023-02-16,23:40:21.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 538/15000, loss = 1.3760900497436523
38255 2023-02-16,23:40:21.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.3214828968048096
40293 2023-02-16,23:40:21.573 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
38362 2023-02-16,23:40:21.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3783504962921143
37533 2023-02-16,23:40:21.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.380359172821045
37983 2023-02-16,23:40:21.619 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4019591808319092
38108 2023-02-16,23:40:21.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.40553617477417
40433 2023-02-16,23:40:21.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 57/15000, loss = 1.3476957082748413
38487 2023-02-16,23:40:21.718 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 539/15000, loss = 1.3542064428329468
38255 2023-02-16,23:40:21.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4101474285125732
40293 2023-02-16,23:40:21.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
38362 2023-02-16,23:40:21.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.420177936553955
37533 2023-02-16,23:40:21.799 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.374097466468811
37983 2023-02-16,23:40:21.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4056732654571533
38108 2023-02-16,23:40:21.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.42246675491333
40433 2023-02-16,23:40:21.863 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 58/15000, loss = 1.447641372680664
38487 2023-02-16,23:40:21.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 540/15000, loss = 1.4012418985366821
38255 2023-02-16,23:40:21.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.37727689743042
40293 2023-02-16,23:40:21.986 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
38362 2023-02-16,23:40:21.998 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3472663164138794
37533 2023-02-16,23:40:22.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4499146938323975
Terminated
Terminated
Terminated
Terminated
Terminated
Terminated
37983 2023-02-16,23:40:22.031 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3673229217529297
38108 2023-02-16,23:40:22.032 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4197195768356323
40433 2023-02-16,23:40:22.068 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 59/15000, loss = 1.3742070198059082
38487 2023-02-16,23:40:22.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 541/15000, loss = 1.36061692237854
38255 2023-02-16,23:40:22.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4134650230407715
40293 2023-02-16,23:40:22.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37533 2023-02-16,23:40:22.195 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3742544651031494
38362 2023-02-16,23:40:22.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3325355052947998
37983 2023-02-16,23:40:22.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4190458059310913
38108 2023-02-16,23:40:22.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.436297059059143
40433 2023-02-16,23:40:22.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 60/15000, loss = 1.4275633096694946
38487 2023-02-16,23:40:22.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 542/15000, loss = 1.3797131776809692
38255 2023-02-16,23:40:22.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3835923671722412
40293 2023-02-16,23:40:22.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37533 2023-02-16,23:40:22.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3805984258651733
38362 2023-02-16,23:40:22.406 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4166327714920044
38108 2023-02-16,23:40:22.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.401592493057251
37983 2023-02-16,23:40:22.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4364831447601318
40433 2023-02-16,23:40:22.485 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 61/15000, loss = 1.4167351722717285
38255 2023-02-16,23:40:22.525 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3610918521881104
38487 2023-02-16,23:40:22.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 543/15000, loss = 1.4419790506362915
40293 2023-02-16,23:40:22.557 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37533 2023-02-16,23:40:22.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4040318727493286
38362 2023-02-16,23:40:22.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4007582664489746
38108 2023-02-16,23:40:22.626 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.421576976776123
37983 2023-02-16,23:40:22.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4203414916992188
40433 2023-02-16,23:40:22.679 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 62/15000, loss = 1.414634108543396
38255 2023-02-16,23:40:22.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4273873567581177
38487 2023-02-16,23:40:22.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 544/15000, loss = 1.3801801204681396
40293 2023-02-16,23:40:22.764 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
37533 2023-02-16,23:40:22.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.370600938796997
38362 2023-02-16,23:40:22.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.373004674911499
38108 2023-02-16,23:40:22.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3966403007507324
37983 2023-02-16,23:40:22.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4116123914718628
40433 2023-02-16,23:40:22.870 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 63/15000, loss = 1.4004226922988892
38255 2023-02-16,23:40:22.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4227159023284912
38487 2023-02-16,23:40:22.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 545/15000, loss = 1.4123165607452393
40293 2023-02-16,23:40:22.973 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
37533 2023-02-16,23:40:22.978 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.3180856704711914
38362 2023-02-16,23:40:23.010 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.417372226715088
38108 2023-02-16,23:40:23.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4569659233093262
40433 2023-02-16,23:40:23.060 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 64/15000, loss = 1.4088605642318726
37983 2023-02-16,23:40:23.063 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.379737138748169
38255 2023-02-16,23:40:23.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4111535549163818
38487 2023-02-16,23:40:23.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 546/15000, loss = 1.3893035650253296
40293 2023-02-16,23:40:23.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
37533 2023-02-16,23:40:23.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.4000277519226074
38362 2023-02-16,23:40:23.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.434468150138855
38108 2023-02-16,23:40:23.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3536134958267212
40433 2023-02-16,23:40:23.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 65/15000, loss = 1.3956220149993896
37983 2023-02-16,23:40:23.266 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3889981508255005
38255 2023-02-16,23:40:23.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4537979364395142
38487 2023-02-16,23:40:23.354 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 547/15000, loss = 1.3879033327102661
40293 2023-02-16,23:40:23.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
37533 2023-02-16,23:40:23.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3810539245605469
38362 2023-02-16,23:40:23.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.3817055225372314
38108 2023-02-16,23:40:23.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4202122688293457
40433 2023-02-16,23:40:23.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 66/15000, loss = 1.359217643737793
37983 2023-02-16,23:40:23.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3811231851577759
38255 2023-02-16,23:40:23.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.404998779296875
38487 2023-02-16,23:40:23.555 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 548/15000, loss = 1.3808865547180176
37533 2023-02-16,23:40:23.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4379940032958984
40293 2023-02-16,23:40:23.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
38108 2023-02-16,23:40:23.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3787143230438232
38362 2023-02-16,23:40:23.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3615469932556152
40433 2023-02-16,23:40:23.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 67/15000, loss = 1.4102730751037598
37983 2023-02-16,23:40:23.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.37236487865448
38255 2023-02-16,23:40:23.758 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3924016952514648
38487 2023-02-16,23:40:23.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 549/15000, loss = 1.3864964246749878
37533 2023-02-16,23:40:23.769 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.3156400918960571
40293 2023-02-16,23:40:23.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
38108 2023-02-16,23:40:23.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3174247741699219
38362 2023-02-16,23:40:23.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.424228310585022
40433 2023-02-16,23:40:23.873 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 68/15000, loss = 1.3859295845031738
37983 2023-02-16,23:40:23.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.422406554222107
38487 2023-02-16,23:40:23.965 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 550/15000, loss = 1.4402906894683838
37533 2023-02-16,23:40:23.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3390220403671265
38255 2023-02-16,23:40:23.976 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4026540517807007
40293 2023-02-16,23:40:23.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
38108 2023-02-16,23:40:24.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3648102283477783
38362 2023-02-16,23:40:24.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3920915126800537
40433 2023-02-16,23:40:24.076 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 69/15000, loss = 1.3894084692001343
37983 2023-02-16,23:40:24.079 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3983162641525269
38487 2023-02-16,23:40:24.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 551/15000, loss = 1.381568193435669
37533 2023-02-16,23:40:24.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3713533878326416
40293 2023-02-16,23:40:24.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
38255 2023-02-16,23:40:24.189 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.407160997390747
38108 2023-02-16,23:40:24.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4392361640930176
38362 2023-02-16,23:40:24.225 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.3874021768569946
40433 2023-02-16,23:40:24.276 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 70/15000, loss = 1.381132960319519
37983 2023-02-16,23:40:24.280 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4026490449905396
38487 2023-02-16,23:40:24.368 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 552/15000, loss = 1.395464301109314
40293 2023-02-16,23:40:24.370 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37533 2023-02-16,23:40:24.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3417978286743164
38255 2023-02-16,23:40:24.391 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4039852619171143
38108 2023-02-16,23:40:24.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3971258401870728
38362 2023-02-16,23:40:24.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.321425437927246
37983 2023-02-16,23:40:24.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.368053674697876
40433 2023-02-16,23:40:24.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 71/15000, loss = 1.380545735359192
38487 2023-02-16,23:40:24.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 553/15000, loss = 1.3751698732376099
40293 2023-02-16,23:40:24.576 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37533 2023-02-16,23:40:24.579 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4049789905548096
38255 2023-02-16,23:40:24.596 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4008508920669556
38108 2023-02-16,23:40:24.633 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3438353538513184
38362 2023-02-16,23:40:24.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.410154938697815
37983 2023-02-16,23:40:24.673 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3715474605560303
40433 2023-02-16,23:40:24.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 72/15000, loss = 1.4066593647003174
40293 2023-02-16,23:40:24.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
38487 2023-02-16,23:40:24.777 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 554/15000, loss = 1.3739690780639648
37533 2023-02-16,23:40:24.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4222460985183716
38255 2023-02-16,23:40:24.797 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3982372283935547
38108 2023-02-16,23:40:24.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3942490816116333
38362 2023-02-16,23:40:24.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.3772296905517578
37983 2023-02-16,23:40:24.878 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.38133704662323
40433 2023-02-16,23:40:24.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 73/15000, loss = 1.394375205039978
40293 2023-02-16,23:40:24.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
38487 2023-02-16,23:40:24.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 555/15000, loss = 1.3882933855056763
37533 2023-02-16,23:40:24.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.4151544570922852
38255 2023-02-16,23:40:25.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4336897134780884
38108 2023-02-16,23:40:25.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3893425464630127
38362 2023-02-16,23:40:25.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.4135890007019043
40433 2023-02-16,23:40:25.082 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 74/15000, loss = 1.373568058013916
37983 2023-02-16,23:40:25.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.399391531944275
40293 2023-02-16,23:40:25.140 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
38487 2023-02-16,23:40:25.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 556/15000, loss = 1.3910945653915405
37533 2023-02-16,23:40:25.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3922302722930908
38108 2023-02-16,23:40:25.224 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.384246826171875
38255 2023-02-16,23:40:25.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4415184259414673
38362 2023-02-16,23:40:25.255 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3836703300476074
40433 2023-02-16,23:40:25.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 75/15000, loss = 1.364786148071289
37983 2023-02-16,23:40:25.280 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4600332975387573
40293 2023-02-16,23:40:25.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
37533 2023-02-16,23:40:25.365 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3595523834228516
38487 2023-02-16,23:40:25.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 557/15000, loss = 1.383575439453125
38108 2023-02-16,23:40:25.412 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4147354364395142
38255 2023-02-16,23:40:25.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4095067977905273
38362 2023-02-16,23:40:25.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3610966205596924
40433 2023-02-16,23:40:25.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 76/15000, loss = 1.393369197845459
37983 2023-02-16,23:40:25.483 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.409303069114685
40293 2023-02-16,23:40:25.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
37533 2023-02-16,23:40:25.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4055076837539673
38487 2023-02-16,23:40:25.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 558/15000, loss = 1.3630613088607788
38108 2023-02-16,23:40:25.601 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4119656085968018
38255 2023-02-16,23:40:25.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.436255693435669
38362 2023-02-16,23:40:25.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.4273719787597656
40433 2023-02-16,23:40:25.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 77/15000, loss = 1.3908635377883911
37983 2023-02-16,23:40:25.675 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4097528457641602
40293 2023-02-16,23:40:25.721 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
37533 2023-02-16,23:40:25.775 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.3996587991714478
38487 2023-02-16,23:40:25.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 559/15000, loss = 1.3677233457565308
38108 2023-02-16,23:40:25.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4114245176315308
38255 2023-02-16,23:40:25.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4394007921218872
38362 2023-02-16,23:40:25.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4228131771087646
40433 2023-02-16,23:40:25.876 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 78/15000, loss = 1.4092423915863037
37983 2023-02-16,23:40:25.880 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.37270188331604
40293 2023-02-16,23:40:25.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
37533 2023-02-16,23:40:25.978 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3515520095825195
38487 2023-02-16,23:40:25.989 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 560/15000, loss = 1.3507925271987915
38108 2023-02-16,23:40:25.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.421961784362793
38255 2023-02-16,23:40:26.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3736543655395508
40433 2023-02-16,23:40:26.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 79/15000, loss = 1.4112857580184937
38362 2023-02-16,23:40:26.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.411150336265564
37983 2023-02-16,23:40:26.081 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3747719526290894
40293 2023-02-16,23:40:26.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
37533 2023-02-16,23:40:26.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.374329924583435
38487 2023-02-16,23:40:26.193 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 561/15000, loss = 1.4132016897201538
38108 2023-02-16,23:40:26.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4226306676864624
38255 2023-02-16,23:40:26.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4056273698806763
40433 2023-02-16,23:40:26.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 80/15000, loss = 1.3748090267181396
38362 2023-02-16,23:40:26.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.453879475593567
37983 2023-02-16,23:40:26.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4076766967773438
40293 2023-02-16,23:40:26.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
37533 2023-02-16,23:40:26.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.3733137845993042
38108 2023-02-16,23:40:26.389 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4021612405776978
38487 2023-02-16,23:40:26.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 562/15000, loss = 1.3815771341323853
38255 2023-02-16,23:40:26.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4223896265029907
40433 2023-02-16,23:40:26.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 81/15000, loss = 1.3895108699798584
38362 2023-02-16,23:40:26.481 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4049562215805054
37983 2023-02-16,23:40:26.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3998706340789795
40293 2023-02-16,23:40:26.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
37533 2023-02-16,23:40:26.591 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3817542791366577
38108 2023-02-16,23:40:26.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.405745029449463
38487 2023-02-16,23:40:26.599 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 563/15000, loss = 1.4370394945144653
38255 2023-02-16,23:40:26.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4197142124176025
40433 2023-02-16,23:40:26.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 82/15000, loss = 1.3865208625793457
38362 2023-02-16,23:40:26.681 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.392462968826294
37983 2023-02-16,23:40:26.689 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3824328184127808
40293 2023-02-16,23:40:26.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
37533 2023-02-16,23:40:26.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.4008443355560303
38108 2023-02-16,23:40:26.795 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3673040866851807
38487 2023-02-16,23:40:26.800 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 564/15000, loss = 1.4332654476165771
38255 2023-02-16,23:40:26.868 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4362784624099731
40433 2023-02-16,23:40:26.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 83/15000, loss = 1.3877358436584473
38362 2023-02-16,23:40:26.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4026192426681519
37983 2023-02-16,23:40:26.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4307595491409302
40293 2023-02-16,23:40:26.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
37533 2023-02-16,23:40:26.995 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4581159353256226
38108 2023-02-16,23:40:26.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4192161560058594
38487 2023-02-16,23:40:27.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 565/15000, loss = 1.4312156438827515
38255 2023-02-16,23:40:27.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4014065265655518
38362 2023-02-16,23:40:27.075 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4071720838546753
40433 2023-02-16,23:40:27.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 84/15000, loss = 1.3646918535232544
37983 2023-02-16,23:40:27.095 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.392021894454956
40293 2023-02-16,23:40:27.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
37533 2023-02-16,23:40:27.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.344679594039917
38108 2023-02-16,23:40:27.204 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4364334344863892
38487 2023-02-16,23:40:27.210 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 566/15000, loss = 1.36722731590271
38362 2023-02-16,23:40:27.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.404158592224121
40433 2023-02-16,23:40:27.269 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 85/15000, loss = 1.4353102445602417
38255 2023-02-16,23:40:27.277 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4215099811553955
37983 2023-02-16,23:40:27.297 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4120639562606812
40293 2023-02-16,23:40:27.335 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
37533 2023-02-16,23:40:27.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.386214256286621
38108 2023-02-16,23:40:27.411 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4205747842788696
38487 2023-02-16,23:40:27.417 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 567/15000, loss = 1.3783072233200073
38362 2023-02-16,23:40:27.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4008374214172363
40433 2023-02-16,23:40:27.458 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 86/15000, loss = 1.4054821729660034
38255 2023-02-16,23:40:27.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3963971138000488
37983 2023-02-16,23:40:27.503 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4327304363250732
40293 2023-02-16,23:40:27.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
38108 2023-02-16,23:40:27.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4116541147232056
37533 2023-02-16,23:40:27.615 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3989019393920898
38487 2023-02-16,23:40:27.621 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 568/15000, loss = 1.4201176166534424
38362 2023-02-16,23:40:27.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3982539176940918
40433 2023-02-16,23:40:27.659 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 87/15000, loss = 1.3593342304229736
38255 2023-02-16,23:40:27.680 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4570534229278564
37983 2023-02-16,23:40:27.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.364266276359558
40293 2023-02-16,23:40:27.742 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
38108 2023-02-16,23:40:27.816 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.379921793937683
38487 2023-02-16,23:40:27.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 569/15000, loss = 1.3473548889160156
37533 2023-02-16,23:40:27.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4052691459655762
38362 2023-02-16,23:40:27.857 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4336422681808472
40433 2023-02-16,23:40:27.860 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 88/15000, loss = 1.4161657094955444
38255 2023-02-16,23:40:27.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3536031246185303
37983 2023-02-16,23:40:27.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3688842058181763
40293 2023-02-16,23:40:27.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
38108 2023-02-16,23:40:28.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3889145851135254
38487 2023-02-16,23:40:28.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 570/15000, loss = 1.3325093984603882
37533 2023-02-16,23:40:28.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.3832941055297852
40433 2023-02-16,23:40:28.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 89/15000, loss = 1.382159948348999
38255 2023-02-16,23:40:28.091 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4202804565429688
38362 2023-02-16,23:40:28.092 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.441489338874817
37983 2023-02-16,23:40:28.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4079716205596924
40293 2023-02-16,23:40:28.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38108 2023-02-16,23:40:28.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3812096118927002
38487 2023-02-16,23:40:28.212 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 571/15000, loss = 1.4165945053100586
37533 2023-02-16,23:40:28.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3686259984970093
40433 2023-02-16,23:40:28.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 90/15000, loss = 1.4047553539276123
38362 2023-02-16,23:40:28.283 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4094513654708862
38255 2023-02-16,23:40:28.296 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3790146112442017
37983 2023-02-16,23:40:28.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.325958251953125
40293 2023-02-16,23:40:28.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
38108 2023-02-16,23:40:28.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.372475266456604
38487 2023-02-16,23:40:28.416 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 572/15000, loss = 1.4009419679641724
37533 2023-02-16,23:40:28.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4243793487548828
40433 2023-02-16,23:40:28.462 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 91/15000, loss = 1.3920073509216309
38362 2023-02-16,23:40:28.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.4362256526947021
38255 2023-02-16,23:40:28.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3173507452011108
37983 2023-02-16,23:40:28.510 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3804376125335693
40293 2023-02-16,23:40:28.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
38108 2023-02-16,23:40:28.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.4228801727294922
38487 2023-02-16,23:40:28.620 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 573/15000, loss = 1.3728504180908203
37533 2023-02-16,23:40:28.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4179506301879883
40433 2023-02-16,23:40:28.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 92/15000, loss = 1.420781135559082
38362 2023-02-16,23:40:28.690 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.439435601234436
38255 2023-02-16,23:40:28.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3648191690444946
37983 2023-02-16,23:40:28.713 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.367760419845581
40293 2023-02-16,23:40:28.747 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
38108 2023-02-16,23:40:28.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.398411512374878
38487 2023-02-16,23:40:28.823 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 574/15000, loss = 1.4174383878707886
37533 2023-02-16,23:40:28.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.4074862003326416
40433 2023-02-16,23:40:28.856 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 93/15000, loss = 1.398667812347412
38362 2023-02-16,23:40:28.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3736313581466675
37983 2023-02-16,23:40:28.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4181443452835083
38255 2023-02-16,23:40:28.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4391040802001953
40293 2023-02-16,23:40:28.951 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
38108 2023-02-16,23:40:28.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4024772644042969
38487 2023-02-16,23:40:29.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 575/15000, loss = 1.434401273727417
37533 2023-02-16,23:40:29.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3720942735671997
40433 2023-02-16,23:40:29.064 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 94/15000, loss = 1.3614026308059692
38362 2023-02-16,23:40:29.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.4055602550506592
37983 2023-02-16,23:40:29.104 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.426751732826233
38255 2023-02-16,23:40:29.106 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3971798419952393
40293 2023-02-16,23:40:29.153 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
38108 2023-02-16,23:40:29.177 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3680552244186401
38487 2023-02-16,23:40:29.228 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 576/15000, loss = 1.381629467010498
37533 2023-02-16,23:40:29.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3290811777114868
40433 2023-02-16,23:40:29.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 95/15000, loss = 1.3759891986846924
38362 2023-02-16,23:40:29.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4224544763565063
37983 2023-02-16,23:40:29.304 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.330913782119751
38255 2023-02-16,23:40:29.307 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3437626361846924
40293 2023-02-16,23:40:29.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
38108 2023-02-16,23:40:29.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.3715474605560303
38487 2023-02-16,23:40:29.428 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 577/15000, loss = 1.3614903688430786
37533 2023-02-16,23:40:29.450 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.365767478942871
40433 2023-02-16,23:40:29.478 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 96/15000, loss = 1.369752049446106
38362 2023-02-16,23:40:29.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4198155403137207
37983 2023-02-16,23:40:29.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4490482807159424
38255 2023-02-16,23:40:29.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3942688703536987
40293 2023-02-16,23:40:29.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
38108 2023-02-16,23:40:29.575 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.3813573122024536
38487 2023-02-16,23:40:29.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 578/15000, loss = 1.4242149591445923
37533 2023-02-16,23:40:29.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.3762074708938599
38362 2023-02-16,23:40:29.687 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.436387062072754
40433 2023-02-16,23:40:29.692 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 97/15000, loss = 1.4267817735671997
37983 2023-02-16,23:40:29.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3777467012405396
38255 2023-02-16,23:40:29.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3892161846160889
40293 2023-02-16,23:40:29.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
38108 2023-02-16,23:40:29.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3992363214492798
38487 2023-02-16,23:40:29.808 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 579/15000, loss = 1.3921955823898315
37533 2023-02-16,23:40:29.844 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.396573781967163
40433 2023-02-16,23:40:29.891 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 98/15000, loss = 1.4099290370941162
37983 2023-02-16,23:40:29.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4015798568725586
38362 2023-02-16,23:40:29.900 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4015446901321411
38255 2023-02-16,23:40:29.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3843711614608765
40293 2023-02-16,23:40:29.955 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
38108 2023-02-16,23:40:29.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4603701829910278
38487 2023-02-16,23:40:30.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 580/15000, loss = 1.387431025505066
37533 2023-02-16,23:40:30.053 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3624120950698853
40433 2023-02-16,23:40:30.097 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 99/15000, loss = 1.4151740074157715
37983 2023-02-16,23:40:30.100 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4360201358795166
38362 2023-02-16,23:40:30.107 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.4215651750564575
38255 2023-02-16,23:40:30.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4147083759307861
40293 2023-02-16,23:40:30.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
38108 2023-02-16,23:40:30.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.409456491470337
38487 2023-02-16,23:40:30.202 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 581/15000, loss = 1.321612000465393
37533 2023-02-16,23:40:30.235 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.360877513885498
40433 2023-02-16,23:40:30.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 100/15000, loss = 1.423954963684082
37983 2023-02-16,23:40:30.300 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4223010540008545
38362 2023-02-16,23:40:30.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3966070413589478
38255 2023-02-16,23:40:30.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4120957851409912
40293 2023-02-16,23:40:30.361 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
38108 2023-02-16,23:40:30.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4099578857421875
38487 2023-02-16,23:40:30.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 582/15000, loss = 1.4102191925048828
37533 2023-02-16,23:40:30.437 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.3954577445983887
40433 2023-02-16,23:40:30.484 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 101/15000, loss = 1.4258214235305786
37983 2023-02-16,23:40:30.502 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4046452045440674
38362 2023-02-16,23:40:30.508 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4569828510284424
38255 2023-02-16,23:40:30.518 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4114433526992798
40293 2023-02-16,23:40:30.564 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
38108 2023-02-16,23:40:30.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3729585409164429
38487 2023-02-16,23:40:30.606 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 583/15000, loss = 1.377341866493225
37533 2023-02-16,23:40:30.640 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3618919849395752
40433 2023-02-16,23:40:30.671 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 102/15000, loss = 1.3904650211334229
37983 2023-02-16,23:40:30.704 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4483609199523926
38362 2023-02-16,23:40:30.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3536889553070068
38255 2023-02-16,23:40:30.722 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4220476150512695
40293 2023-02-16,23:40:30.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
38108 2023-02-16,23:40:30.791 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3747767210006714
38487 2023-02-16,23:40:30.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 584/15000, loss = 1.41355299949646
37533 2023-02-16,23:40:30.843 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.3620871305465698
40433 2023-02-16,23:40:30.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 103/15000, loss = 1.3441903591156006
37983 2023-02-16,23:40:30.907 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4046733379364014
38362 2023-02-16,23:40:30.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4202513694763184
38255 2023-02-16,23:40:30.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4223926067352295
40293 2023-02-16,23:40:30.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
38108 2023-02-16,23:40:30.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4077156782150269
38487 2023-02-16,23:40:31.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 585/15000, loss = 1.3836908340454102
40433 2023-02-16,23:40:31.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 104/15000, loss = 1.3832846879959106
37533 2023-02-16,23:40:31.047 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.3902437686920166
37983 2023-02-16,23:40:31.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3683676719665527
38255 2023-02-16,23:40:31.116 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4020717144012451
38362 2023-02-16,23:40:31.118 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.37894606590271
40293 2023-02-16,23:40:31.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
38108 2023-02-16,23:40:31.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.399783730506897
38487 2023-02-16,23:40:31.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 586/15000, loss = 1.3611406087875366
40433 2023-02-16,23:40:31.239 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 105/15000, loss = 1.4044945240020752
37533 2023-02-16,23:40:31.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3771874904632568
37983 2023-02-16,23:40:31.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.373300552368164
38255 2023-02-16,23:40:31.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4057130813598633
38362 2023-02-16,23:40:31.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.317399024963379
40293 2023-02-16,23:40:31.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
38108 2023-02-16,23:40:31.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3825483322143555
38487 2023-02-16,23:40:31.425 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 587/15000, loss = 1.427369475364685
40433 2023-02-16,23:40:31.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 106/15000, loss = 1.3935617208480835
37533 2023-02-16,23:40:31.455 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4047024250030518
37983 2023-02-16,23:40:31.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4180066585540771
38255 2023-02-16,23:40:31.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3673015832901
38362 2023-02-16,23:40:31.524 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3648545742034912
40293 2023-02-16,23:40:31.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
38108 2023-02-16,23:40:31.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4308245182037354
38487 2023-02-16,23:40:31.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 588/15000, loss = 1.4226725101470947
40433 2023-02-16,23:40:31.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 107/15000, loss = 1.3698707818984985
37533 2023-02-16,23:40:31.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.4050315618515015
37983 2023-02-16,23:40:31.719 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3877562284469604
38255 2023-02-16,23:40:31.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.4191901683807373
38362 2023-02-16,23:40:31.725 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4391084909439087
40293 2023-02-16,23:40:31.751 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
38108 2023-02-16,23:40:31.805 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3919634819030762
38487 2023-02-16,23:40:31.826 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 589/15000, loss = 1.4111895561218262
40433 2023-02-16,23:40:31.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 108/15000, loss = 1.401772141456604
37533 2023-02-16,23:40:31.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.4068390130996704
37983 2023-02-16,23:40:31.923 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4209424257278442
38255 2023-02-16,23:40:31.927 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.4365044832229614
38362 2023-02-16,23:40:31.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3972160816192627
40293 2023-02-16,23:40:31.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
38108 2023-02-16,23:40:32.008 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4122236967086792
38487 2023-02-16,23:40:32.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 590/15000, loss = 1.4537849426269531
40433 2023-02-16,23:40:32.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 109/15000, loss = 1.3504347801208496
37533 2023-02-16,23:40:32.046 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.3521357774734497
37983 2023-02-16,23:40:32.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4105138778686523
38255 2023-02-16,23:40:32.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4206169843673706
38362 2023-02-16,23:40:32.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.343907117843628
40293 2023-02-16,23:40:32.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38108 2023-02-16,23:40:32.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4330700635910034
38487 2023-02-16,23:40:32.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 591/15000, loss = 1.4049715995788574
40433 2023-02-16,23:40:32.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 110/15000, loss = 1.4283493757247925
37533 2023-02-16,23:40:32.245 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4034727811813354
37983 2023-02-16,23:40:32.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4085909128189087
38362 2023-02-16,23:40:32.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3943257331848145
38255 2023-02-16,23:40:32.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4117894172668457
40293 2023-02-16,23:40:32.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38108 2023-02-16,23:40:32.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.364505410194397
38487 2023-02-16,23:40:32.431 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 592/15000, loss = 1.3923989534378052
40433 2023-02-16,23:40:32.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 111/15000, loss = 1.3965458869934082
37533 2023-02-16,23:40:32.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.3593658208847046
37983 2023-02-16,23:40:32.526 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3909072875976562
38362 2023-02-16,23:40:32.531 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.3893778324127197
38255 2023-02-16,23:40:32.544 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3798779249191284
40293 2023-02-16,23:40:32.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
38108 2023-02-16,23:40:32.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3689967393875122
38487 2023-02-16,23:40:32.632 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 593/15000, loss = 1.4027985334396362
40433 2023-02-16,23:40:32.635 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 112/15000, loss = 1.374855399131775
37533 2023-02-16,23:40:32.647 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.4190220832824707
37983 2023-02-16,23:40:32.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4114807844161987
38362 2023-02-16,23:40:32.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3842731714248657
38255 2023-02-16,23:40:32.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3889095783233643
40293 2023-02-16,23:40:32.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
38108 2023-02-16,23:40:32.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4082636833190918
38487 2023-02-16,23:40:32.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 594/15000, loss = 1.4072190523147583
40433 2023-02-16,23:40:32.839 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 113/15000, loss = 1.386115550994873
37533 2023-02-16,23:40:32.851 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.3645166158676147
37983 2023-02-16,23:40:32.931 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3873461484909058
38362 2023-02-16,23:40:32.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4148259162902832
40293 2023-02-16,23:40:32.941 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
38255 2023-02-16,23:40:32.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.381118655204773
38108 2023-02-16,23:40:33.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3260517120361328
38487 2023-02-16,23:40:33.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 595/15000, loss = 1.4040329456329346
40433 2023-02-16,23:40:33.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 114/15000, loss = 1.3819806575775146
37533 2023-02-16,23:40:33.057 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.379149079322815
40293 2023-02-16,23:40:33.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
37983 2023-02-16,23:40:33.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3311299085617065
38362 2023-02-16,23:40:33.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4120224714279175
38255 2023-02-16,23:40:33.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3725398778915405
40433 2023-02-16,23:40:33.232 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 115/15000, loss = 1.3948136568069458
38108 2023-02-16,23:40:33.238 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3804336786270142
38487 2023-02-16,23:40:33.244 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 596/15000, loss = 1.4008241891860962
37533 2023-02-16,23:40:33.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.3856024742126465
40293 2023-02-16,23:40:33.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
37983 2023-02-16,23:40:33.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4110560417175293
38255 2023-02-16,23:40:33.357 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.422616720199585
38362 2023-02-16,23:40:33.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.411393165588379
40433 2023-02-16,23:40:33.436 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 116/15000, loss = 1.3259618282318115
38108 2023-02-16,23:40:33.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3675346374511719
38487 2023-02-16,23:40:33.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 597/15000, loss = 1.3982926607131958
37533 2023-02-16,23:40:33.453 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.4392316341400146
40293 2023-02-16,23:40:33.500 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
37983 2023-02-16,23:40:33.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3854572772979736
38255 2023-02-16,23:40:33.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3983674049377441
38362 2023-02-16,23:40:33.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.422097086906433
40433 2023-02-16,23:40:33.641 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 117/15000, loss = 1.4109978675842285
38108 2023-02-16,23:40:33.648 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4182114601135254
38487 2023-02-16,23:40:33.655 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 598/15000, loss = 1.4336800575256348
37533 2023-02-16,23:40:33.658 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.3471863269805908
40293 2023-02-16,23:40:33.706 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
37983 2023-02-16,23:40:33.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.378546953201294
38255 2023-02-16,23:40:33.766 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4027422666549683
38362 2023-02-16,23:40:33.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.422616958618164
40433 2023-02-16,23:40:33.845 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 118/15000, loss = 1.424782395362854
38108 2023-02-16,23:40:33.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4267730712890625
37533 2023-02-16,23:40:33.862 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.378908395767212
38487 2023-02-16,23:40:33.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 599/15000, loss = 1.4414889812469482
40293 2023-02-16,23:40:33.910 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
37983 2023-02-16,23:40:33.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4172768592834473
38255 2023-02-16,23:40:33.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3680802583694458
38362 2023-02-16,23:40:33.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4021849632263184
40433 2023-02-16,23:40:34.030 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 119/15000, loss = 1.3382103443145752
38108 2023-02-16,23:40:34.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3308972120285034
37533 2023-02-16,23:40:34.062 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.426222562789917
38487 2023-02-16,23:40:34.084 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 600/15000, loss = 1.4095362424850464
40293 2023-02-16,23:40:34.098 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
37983 2023-02-16,23:40:34.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3679225444793701
38255 2023-02-16,23:40:34.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.371504545211792
38362 2023-02-16,23:40:34.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4057679176330566
40433 2023-02-16,23:40:34.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 120/15000, loss = 1.4303877353668213
38108 2023-02-16,23:40:34.256 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4492608308792114
37533 2023-02-16,23:40:34.268 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.3805499076843262
38487 2023-02-16,23:40:34.289 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 601/15000, loss = 1.436208963394165
40293 2023-02-16,23:40:34.303 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37983 2023-02-16,23:40:34.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3951927423477173
38255 2023-02-16,23:40:34.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.3813997507095337
38362 2023-02-16,23:40:34.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.367383599281311
40433 2023-02-16,23:40:34.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 121/15000, loss = 1.422236442565918
38108 2023-02-16,23:40:34.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3776681423187256
37533 2023-02-16,23:40:34.473 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.4356187582015991
38487 2023-02-16,23:40:34.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 602/15000, loss = 1.4393863677978516
40293 2023-02-16,23:40:34.507 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37983 2023-02-16,23:40:34.577 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3815995454788208
38255 2023-02-16,23:40:34.580 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3995119333267212
38362 2023-02-16,23:40:34.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.419128656387329
40433 2023-02-16,23:40:34.611 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 122/15000, loss = 1.4339179992675781
38108 2023-02-16,23:40:34.664 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4016033411026
37533 2023-02-16,23:40:34.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3960386514663696
38487 2023-02-16,23:40:34.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 603/15000, loss = 1.3737627267837524
40293 2023-02-16,23:40:34.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
38362 2023-02-16,23:40:34.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.436508297920227
37983 2023-02-16,23:40:34.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3872959613800049
38255 2023-02-16,23:40:34.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4602102041244507
40433 2023-02-16,23:40:34.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 123/15000, loss = 1.4243807792663574
38108 2023-02-16,23:40:34.869 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.435894250869751
37533 2023-02-16,23:40:34.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.363662600517273
38487 2023-02-16,23:40:34.903 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 604/15000, loss = 1.405666708946228
40293 2023-02-16,23:40:34.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
38362 2023-02-16,23:40:34.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4205958843231201
37983 2023-02-16,23:40:34.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3543472290039062
38255 2023-02-16,23:40:34.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.409489393234253
40433 2023-02-16,23:40:35.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 124/15000, loss = 1.3830498456954956
38108 2023-02-16,23:40:35.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4222228527069092
37533 2023-02-16,23:40:35.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4074146747589111
38487 2023-02-16,23:40:35.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 605/15000, loss = 1.4223732948303223
40293 2023-02-16,23:40:35.120 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
38362 2023-02-16,23:40:35.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4117412567138672
37983 2023-02-16,23:40:35.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.427877426147461
38255 2023-02-16,23:40:35.190 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4098881483078003
40433 2023-02-16,23:40:35.208 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 125/15000, loss = 1.3978954553604126
38108 2023-02-16,23:40:35.275 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.40467369556427
37533 2023-02-16,23:40:35.287 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.3970853090286255
38487 2023-02-16,23:40:35.309 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 606/15000, loss = 1.4197734594345093
40293 2023-02-16,23:40:35.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
38362 2023-02-16,23:40:35.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3800036907196045
37983 2023-02-16,23:40:35.386 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4077532291412354
38255 2023-02-16,23:40:35.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3727895021438599
40433 2023-02-16,23:40:35.398 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 126/15000, loss = 1.3935307264328003
38108 2023-02-16,23:40:35.479 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.448273777961731
37533 2023-02-16,23:40:35.492 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.4238269329071045
38487 2023-02-16,23:40:35.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 607/15000, loss = 1.4363023042678833
40293 2023-02-16,23:40:35.527 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
38362 2023-02-16,23:40:35.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.3889936208724976
40433 2023-02-16,23:40:35.585 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 127/15000, loss = 1.3959226608276367
37983 2023-02-16,23:40:35.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.366089105606079
38255 2023-02-16,23:40:35.593 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3748770952224731
38108 2023-02-16,23:40:35.685 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4048042297363281
37533 2023-02-16,23:40:35.698 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.4084771871566772
38487 2023-02-16,23:40:35.720 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 608/15000, loss = 1.4015144109725952
40293 2023-02-16,23:40:35.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
38362 2023-02-16,23:40:35.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3812310695648193
40433 2023-02-16,23:40:35.790 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 128/15000, loss = 1.3883386850357056
37983 2023-02-16,23:40:35.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4205186367034912
38255 2023-02-16,23:40:35.796 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.407724142074585
38108 2023-02-16,23:40:35.886 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3683555126190186
37533 2023-02-16,23:40:35.898 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.3793941736221313
38487 2023-02-16,23:40:35.921 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 609/15000, loss = 1.421521544456482
40293 2023-02-16,23:40:35.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
38362 2023-02-16,23:40:35.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3725082874298096
40433 2023-02-16,23:40:35.991 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 129/15000, loss = 1.3689310550689697
37983 2023-02-16,23:40:35.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.391156554222107
38255 2023-02-16,23:40:35.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3999171257019043
38108 2023-02-16,23:40:36.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3737536668777466
37533 2023-02-16,23:40:36.099 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3533813953399658
38487 2023-02-16,23:40:36.121 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 610/15000, loss = 1.3965160846710205
40293 2023-02-16,23:40:36.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
38362 2023-02-16,23:40:36.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.4228887557983398
40433 2023-02-16,23:40:36.178 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 130/15000, loss = 1.3568142652511597
37983 2023-02-16,23:40:36.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4803004264831543
38255 2023-02-16,23:40:36.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3826521635055542
38108 2023-02-16,23:40:36.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4183179140090942
37533 2023-02-16,23:40:36.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4126300811767578
38487 2023-02-16,23:40:36.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 611/15000, loss = 1.4569767713546753
40293 2023-02-16,23:40:36.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
38362 2023-02-16,23:40:36.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3984209299087524
40433 2023-02-16,23:40:36.382 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 131/15000, loss = 1.3873652219772339
37983 2023-02-16,23:40:36.401 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3683066368103027
38255 2023-02-16,23:40:36.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4308562278747559
38108 2023-02-16,23:40:36.497 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.388118863105774
37533 2023-02-16,23:40:36.509 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.4348663091659546
38487 2023-02-16,23:40:36.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 612/15000, loss = 1.3537408113479614
40293 2023-02-16,23:40:36.532 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
38362 2023-02-16,23:40:36.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.402436375617981
40433 2023-02-16,23:40:36.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 132/15000, loss = 1.3544719219207764
37983 2023-02-16,23:40:36.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3797509670257568
38255 2023-02-16,23:40:36.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3918980360031128
37533 2023-02-16,23:40:36.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4323819875717163
38108 2023-02-16,23:40:36.714 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.420999526977539
40293 2023-02-16,23:40:36.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
38487 2023-02-16,23:40:36.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 613/15000, loss = 1.4202888011932373
38362 2023-02-16,23:40:36.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3682221174240112
40433 2023-02-16,23:40:36.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 133/15000, loss = 1.3886247873306274
37983 2023-02-16,23:40:36.804 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.3746486902236938
38255 2023-02-16,23:40:36.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.4121299982070923
37533 2023-02-16,23:40:36.913 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.4094370603561401
38108 2023-02-16,23:40:36.916 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4108707904815674
40293 2023-02-16,23:40:36.924 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
38487 2023-02-16,23:40:36.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 614/15000, loss = 1.3791404962539673
38362 2023-02-16,23:40:36.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.371583104133606
40433 2023-02-16,23:40:36.984 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 134/15000, loss = 1.3642394542694092
37983 2023-02-16,23:40:37.004 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4513236284255981
38255 2023-02-16,23:40:37.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4328995943069458
37533 2023-02-16,23:40:37.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4539357423782349
38108 2023-02-16,23:40:37.117 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4088000059127808
40293 2023-02-16,23:40:37.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
38487 2023-02-16,23:40:37.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 615/15000, loss = 1.3175276517868042
38362 2023-02-16,23:40:37.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.3814061880111694
40433 2023-02-16,23:40:37.186 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 135/15000, loss = 1.393031358718872
37983 2023-02-16,23:40:37.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3736324310302734
38255 2023-02-16,23:40:37.209 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3644647598266602
37533 2023-02-16,23:40:37.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4143873453140259
38108 2023-02-16,23:40:37.319 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3909554481506348
40293 2023-02-16,23:40:37.326 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
38487 2023-02-16,23:40:37.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 616/15000, loss = 1.3648834228515625
38362 2023-02-16,23:40:37.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3994832038879395
40433 2023-02-16,23:40:37.387 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 136/15000, loss = 1.3334640264511108
37983 2023-02-16,23:40:37.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3819719552993774
38255 2023-02-16,23:40:37.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3689591884613037
40293 2023-02-16,23:40:37.512 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37533 2023-02-16,23:40:37.516 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.3606082201004028
38108 2023-02-16,23:40:37.519 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4112980365753174
38487 2023-02-16,23:40:37.537 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 617/15000, loss = 1.4391074180603027
38362 2023-02-16,23:40:37.545 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4603689908981323
40433 2023-02-16,23:40:37.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 137/15000, loss = 1.382919192314148
37983 2023-02-16,23:40:37.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.404927372932434
38255 2023-02-16,23:40:37.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.4080801010131836
40293 2023-02-16,23:40:37.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37533 2023-02-16,23:40:37.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.3919129371643066
38108 2023-02-16,23:40:37.730 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3872069120407104
38362 2023-02-16,23:40:37.745 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.4095637798309326
38487 2023-02-16,23:40:37.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 618/15000, loss = 1.3971829414367676
40433 2023-02-16,23:40:37.782 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 138/15000, loss = 1.3216511011123657
37983 2023-02-16,23:40:37.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3702082633972168
38255 2023-02-16,23:40:37.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.3259721994400024
40293 2023-02-16,23:40:37.926 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37533 2023-02-16,23:40:37.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.375605821609497
38108 2023-02-16,23:40:37.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3307875394821167
38362 2023-02-16,23:40:37.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4099774360656738
38487 2023-02-16,23:40:37.949 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 619/15000, loss = 1.3439797163009644
40433 2023-02-16,23:40:37.983 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 139/15000, loss = 1.4098161458969116
37983 2023-02-16,23:40:38.002 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.3165309429168701
38255 2023-02-16,23:40:38.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3805817365646362
40293 2023-02-16,23:40:38.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37533 2023-02-16,23:40:38.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.4147834777832031
38108 2023-02-16,23:40:38.134 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4111344814300537
38362 2023-02-16,23:40:38.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3728394508361816
38487 2023-02-16,23:40:38.150 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 620/15000, loss = 1.3943424224853516
40433 2023-02-16,23:40:38.183 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 140/15000, loss = 1.4195556640625
37983 2023-02-16,23:40:38.203 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3998888731002808
38255 2023-02-16,23:40:38.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.3675835132598877
40293 2023-02-16,23:40:38.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37533 2023-02-16,23:40:38.328 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.3709535598754883
38108 2023-02-16,23:40:38.345 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3856050968170166
38362 2023-02-16,23:40:38.349 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3747894763946533
38487 2023-02-16,23:40:38.350 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 621/15000, loss = 1.389326572418213
40433 2023-02-16,23:40:38.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 141/15000, loss = 1.3817135095596313
37983 2023-02-16,23:40:38.403 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.3811203241348267
38255 2023-02-16,23:40:38.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4182496070861816
40293 2023-02-16,23:40:38.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37533 2023-02-16,23:40:38.530 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.395496129989624
38108 2023-02-16,23:40:38.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3787469863891602
38362 2023-02-16,23:40:38.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.4077227115631104
38487 2023-02-16,23:40:38.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 622/15000, loss = 1.3844338655471802
40433 2023-02-16,23:40:38.586 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 142/15000, loss = 1.329262614250183
38255 2023-02-16,23:40:38.609 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4266939163208008
37983 2023-02-16,23:40:38.610 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4409692287445068
40293 2023-02-16,23:40:38.729 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37533 2023-02-16,23:40:38.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.4324404001235962
38108 2023-02-16,23:40:38.750 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4175201654434204
38362 2023-02-16,23:40:38.754 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3998078107833862
38487 2023-02-16,23:40:38.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 623/15000, loss = 1.4147355556488037
40433 2023-02-16,23:40:38.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 143/15000, loss = 1.4232449531555176
37983 2023-02-16,23:40:38.809 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.314314603805542
38255 2023-02-16,23:40:38.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3308789730072021
40293 2023-02-16,23:40:38.933 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37533 2023-02-16,23:40:38.937 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.3989075422286987
38362 2023-02-16,23:40:38.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3826500177383423
38108 2023-02-16,23:40:38.954 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3680044412612915
38487 2023-02-16,23:40:38.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 624/15000, loss = 1.4120911359786987
40433 2023-02-16,23:40:38.994 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 144/15000, loss = 1.3894246816635132
37983 2023-02-16,23:40:39.014 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3375813961029053
38255 2023-02-16,23:40:39.016 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4492589235305786
40293 2023-02-16,23:40:39.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37533 2023-02-16,23:40:39.139 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.3524142503738403
38362 2023-02-16,23:40:39.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4308860301971436
38487 2023-02-16,23:40:39.149 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 625/15000, loss = 1.4114415645599365
38108 2023-02-16,23:40:39.155 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3953825235366821
40433 2023-02-16,23:40:39.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 145/15000, loss = 1.407400369644165
37983 2023-02-16,23:40:39.216 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3702540397644043
38255 2023-02-16,23:40:39.220 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3777542114257812
38362 2023-02-16,23:40:39.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.3920241594314575
40293 2023-02-16,23:40:39.340 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37533 2023-02-16,23:40:39.343 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.4223761558532715
38487 2023-02-16,23:40:39.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 626/15000, loss = 1.4220682382583618
38108 2023-02-16,23:40:39.356 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3810884952545166
40433 2023-02-16,23:40:39.400 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 146/15000, loss = 1.4114470481872559
37983 2023-02-16,23:40:39.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3399173021316528
38255 2023-02-16,23:40:39.422 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4016350507736206
38362 2023-02-16,23:40:39.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.412202000617981
40293 2023-02-16,23:40:39.543 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37533 2023-02-16,23:40:39.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3953146934509277
38487 2023-02-16,23:40:39.554 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 627/15000, loss = 1.4224485158920288
38108 2023-02-16,23:40:39.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3871115446090698
40433 2023-02-16,23:40:39.602 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 147/15000, loss = 1.415766954421997
37983 2023-02-16,23:40:39.622 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4060524702072144
38255 2023-02-16,23:40:39.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.4361375570297241
37533 2023-02-16,23:40:39.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.3369895219802856
38362 2023-02-16,23:40:39.741 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.4330781698226929
40293 2023-02-16,23:40:39.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
38487 2023-02-16,23:40:39.755 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 628/15000, loss = 1.4021759033203125
38108 2023-02-16,23:40:39.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3541938066482544
40433 2023-02-16,23:40:39.806 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 148/15000, loss = 1.434118628501892
37983 2023-02-16,23:40:39.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4234098196029663
38255 2023-02-16,23:40:39.828 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4222233295440674
37533 2023-02-16,23:40:39.934 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.3692256212234497
38362 2023-02-16,23:40:39.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3645485639572144
40293 2023-02-16,23:40:39.946 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
38487 2023-02-16,23:40:39.957 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 629/15000, loss = 1.4056612253189087
38108 2023-02-16,23:40:39.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.427868366241455
40433 2023-02-16,23:40:40.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 149/15000, loss = 1.361600399017334
37983 2023-02-16,23:40:40.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.4171580076217651
38255 2023-02-16,23:40:40.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.4045976400375366
37533 2023-02-16,23:40:40.135 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3893039226531982
38362 2023-02-16,23:40:40.142 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3689450025558472
40293 2023-02-16,23:40:40.146 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
38487 2023-02-16,23:40:40.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 630/15000, loss = 1.3674930334091187
38108 2023-02-16,23:40:40.162 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4078772068023682
40433 2023-02-16,23:40:40.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 150/15000, loss = 1.3966424465179443
37983 2023-02-16,23:40:40.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3923505544662476
38255 2023-02-16,23:40:40.230 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4485671520233154
37533 2023-02-16,23:40:40.336 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.3594508171081543
38362 2023-02-16,23:40:40.344 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 660/15000, loss = 1.408331036567688
40293 2023-02-16,23:40:40.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
38487 2023-02-16,23:40:40.358 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 631/15000, loss = 1.419219970703125
38108 2023-02-16,23:40:40.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3662285804748535
40433 2023-02-16,23:40:40.407 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 151/15000, loss = 1.4336442947387695
37983 2023-02-16,23:40:40.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.359607458114624
38255 2023-02-16,23:40:40.429 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4048359394073486
37533 2023-02-16,23:40:40.535 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.3899933099746704
38362 2023-02-16,23:40:40.542 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 661/15000, loss = 1.326148509979248
40293 2023-02-16,23:40:40.546 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
38487 2023-02-16,23:40:40.556 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 632/15000, loss = 1.436482548713684
38108 2023-02-16,23:40:40.562 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.420668601989746
40433 2023-02-16,23:40:40.590 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 152/15000, loss = 1.3714122772216797
37983 2023-02-16,23:40:40.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4074972867965698
38255 2023-02-16,23:40:40.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.368382453918457
37533 2023-02-16,23:40:40.734 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.3422629833221436
38362 2023-02-16,23:40:40.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 662/15000, loss = 1.3805453777313232
40293 2023-02-16,23:40:40.746 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
38487 2023-02-16,23:40:40.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 633/15000, loss = 1.4206123352050781
38108 2023-02-16,23:40:40.763 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3915221691131592
40433 2023-02-16,23:40:40.789 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 153/15000, loss = 1.445770025253296
37983 2023-02-16,23:40:40.824 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.4009473323822021
38255 2023-02-16,23:40:40.827 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.373474359512329
38362 2023-02-16,23:40:40.944 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 663/15000, loss = 1.367572546005249
40293 2023-02-16,23:40:40.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37533 2023-02-16,23:40:40.950 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.3967360258102417
38487 2023-02-16,23:40:40.958 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 634/15000, loss = 1.4118373394012451
38108 2023-02-16,23:40:40.964 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4803658723831177
40433 2023-02-16,23:40:40.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 154/15000, loss = 1.4114140272140503
37983 2023-02-16,23:40:41.024 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3507680892944336
38255 2023-02-16,23:40:41.027 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4182417392730713
38362 2023-02-16,23:40:41.145 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 664/15000, loss = 1.4182562828063965
40293 2023-02-16,23:40:41.148 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
38487 2023-02-16,23:40:41.161 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 635/15000, loss = 1.3799301385879517
38108 2023-02-16,23:40:41.170 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3683197498321533
37533 2023-02-16,23:40:41.176 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.3915233612060547
40433 2023-02-16,23:40:41.196 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 155/15000, loss = 1.4042142629623413
37983 2023-02-16,23:40:41.229 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3744182586669922
38255 2023-02-16,23:40:41.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.388059377670288
38362 2023-02-16,23:40:41.347 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 665/15000, loss = 1.4268803596496582
40293 2023-02-16,23:40:41.351 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
38108 2023-02-16,23:40:41.359 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3798271417617798
38487 2023-02-16,23:40:41.363 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 636/15000, loss = 1.38897705078125
37533 2023-02-16,23:40:41.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.3586854934692383
40433 2023-02-16,23:40:41.395 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 156/15000, loss = 1.4172922372817993
37983 2023-02-16,23:40:41.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.3727437257766724
38255 2023-02-16,23:40:41.430 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4210340976715088
40293 2023-02-16,23:40:41.552 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
38108 2023-02-16,23:40:41.560 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.3746731281280518
38362 2023-02-16,23:40:41.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 666/15000, loss = 1.3309956789016724
38487 2023-02-16,23:40:41.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 637/15000, loss = 1.3812144994735718
37533 2023-02-16,23:40:41.578 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.3721638917922974
40433 2023-02-16,23:40:41.597 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 157/15000, loss = 1.3817410469055176
37983 2023-02-16,23:40:41.628 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3821134567260742
38255 2023-02-16,23:40:41.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.410624623298645
40293 2023-02-16,23:40:41.757 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
38108 2023-02-16,23:40:41.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4515033960342407
38362 2023-02-16,23:40:41.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 667/15000, loss = 1.4493074417114258
38487 2023-02-16,23:40:41.767 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 638/15000, loss = 1.3726377487182617
37533 2023-02-16,23:40:41.780 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.3945002555847168
40433 2023-02-16,23:40:41.801 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 158/15000, loss = 1.3713648319244385
37983 2023-02-16,23:40:41.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.4009859561920166
38255 2023-02-16,23:40:41.836 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4087966680526733
40293 2023-02-16,23:40:41.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
38108 2023-02-16,23:40:41.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3734862804412842
38362 2023-02-16,23:40:41.970 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 668/15000, loss = 1.3777612447738647
38487 2023-02-16,23:40:41.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 639/15000, loss = 1.4227962493896484
37533 2023-02-16,23:40:41.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.4366024732589722
40433 2023-02-16,23:40:42.007 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 159/15000, loss = 1.3865320682525635
37983 2023-02-16,23:40:42.038 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4599921703338623
38255 2023-02-16,23:40:42.041 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3910377025604248
40293 2023-02-16,23:40:42.129 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
38108 2023-02-16,23:40:42.169 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3819326162338257
38362 2023-02-16,23:40:42.172 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 669/15000, loss = 1.4016788005828857
38487 2023-02-16,23:40:42.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 640/15000, loss = 1.3984159231185913
37533 2023-02-16,23:40:42.191 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.4302291870117188
40433 2023-02-16,23:40:42.211 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 160/15000, loss = 1.4181797504425049
37983 2023-02-16,23:40:42.240 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.34323251247406
38255 2023-02-16,23:40:42.243 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4114224910736084
40293 2023-02-16,23:40:42.311 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
38362 2023-02-16,23:40:42.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 670/15000, loss = 1.435981035232544
38487 2023-02-16,23:40:42.375 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 641/15000, loss = 1.4026941061019897
38108 2023-02-16,23:40:42.385 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.405090570449829
37533 2023-02-16,23:40:42.394 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 805/15000, loss = 1.3895373344421387
40433 2023-02-16,23:40:42.413 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 161/15000, loss = 1.4193931818008423
37983 2023-02-16,23:40:42.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3868205547332764
38255 2023-02-16,23:40:42.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.387221097946167
40293 2023-02-16,23:40:42.501 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
38362 2023-02-16,23:40:42.582 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 671/15000, loss = 1.4222047328948975
38487 2023-02-16,23:40:42.583 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 642/15000, loss = 1.3681317567825317
38108 2023-02-16,23:40:42.592 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.3701035976409912
37533 2023-02-16,23:40:42.600 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 806/15000, loss = 1.3840370178222656
40433 2023-02-16,23:40:42.618 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 162/15000, loss = 1.3590139150619507
37983 2023-02-16,23:40:42.651 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3984308242797852
38255 2023-02-16,23:40:42.654 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3310182094573975
40293 2023-02-16,23:40:42.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
38362 2023-02-16,23:40:42.784 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 672/15000, loss = 1.404771089553833
38487 2023-02-16,23:40:42.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 643/15000, loss = 1.371677279472351
38108 2023-02-16,23:40:42.794 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.3164159059524536
37533 2023-02-16,23:40:42.802 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 807/15000, loss = 1.3745416402816772
40433 2023-02-16,23:40:42.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 163/15000, loss = 1.3907079696655273
37983 2023-02-16,23:40:42.852 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4066168069839478
38255 2023-02-16,23:40:42.855 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.411154866218567
40293 2023-02-16,23:40:42.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
38487 2023-02-16,23:40:42.987 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 644/15000, loss = 1.381479263305664
38108 2023-02-16,23:40:42.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3996777534484863
38362 2023-02-16,23:40:42.999 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 673/15000, loss = 1.4483855962753296
37533 2023-02-16,23:40:43.005 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 808/15000, loss = 1.417356252670288
40433 2023-02-16,23:40:43.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 164/15000, loss = 1.3653016090393066
37983 2023-02-16,23:40:43.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.382621169090271
38255 2023-02-16,23:40:43.058 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3854575157165527
40293 2023-02-16,23:40:43.108 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38487 2023-02-16,23:40:43.173 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 645/15000, loss = 1.3995399475097656
38108 2023-02-16,23:40:43.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.381184697151184
38362 2023-02-16,23:40:43.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 674/15000, loss = 1.4048981666564941
37533 2023-02-16,23:40:43.207 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 809/15000, loss = 1.4156194925308228
40433 2023-02-16,23:40:43.231 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 165/15000, loss = 1.4585413932800293
37983 2023-02-16,23:40:43.261 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3677102327346802
38255 2023-02-16,23:40:43.263 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3786540031433105
40293 2023-02-16,23:40:43.315 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
38487 2023-02-16,23:40:43.362 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 646/15000, loss = 1.4602313041687012
38108 2023-02-16,23:40:43.371 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.4412051439285278
38362 2023-02-16,23:40:43.402 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 675/15000, loss = 1.3683887720108032
37533 2023-02-16,23:40:43.410 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 810/15000, loss = 1.3916738033294678
40433 2023-02-16,23:40:43.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 166/15000, loss = 1.4277206659317017
37983 2023-02-16,23:40:43.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4253180027008057
38255 2023-02-16,23:40:43.468 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4173099994659424
40293 2023-02-16,23:40:43.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
38487 2023-02-16,23:40:43.551 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 647/15000, loss = 1.409595251083374
38108 2023-02-16,23:40:43.561 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.314170241355896
38362 2023-02-16,23:40:43.608 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 676/15000, loss = 1.3736721277236938
37533 2023-02-16,23:40:43.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 811/15000, loss = 1.2982089519500732
40433 2023-02-16,23:40:43.642 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 167/15000, loss = 1.40604829788208
37983 2023-02-16,23:40:43.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.419421672821045
38255 2023-02-16,23:40:43.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3679828643798828
40293 2023-02-16,23:40:43.723 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
38487 2023-02-16,23:40:43.738 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 648/15000, loss = 1.4099096059799194
38108 2023-02-16,23:40:43.759 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.3374136686325073
38362 2023-02-16,23:40:43.811 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 677/15000, loss = 1.4182634353637695
37533 2023-02-16,23:40:43.819 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 812/15000, loss = 1.3452231884002686
40433 2023-02-16,23:40:43.846 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 168/15000, loss = 1.4105503559112549
37983 2023-02-16,23:40:43.879 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.4089899063110352
38255 2023-02-16,23:40:43.881 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3952679634094238
40293 2023-02-16,23:40:43.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
38487 2023-02-16,23:40:43.942 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 649/15000, loss = 1.3729431629180908
38108 2023-02-16,23:40:43.963 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.370302677154541
38362 2023-02-16,23:40:43.996 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 678/15000, loss = 1.3880894184112549
37533 2023-02-16,23:40:44.019 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 813/15000, loss = 1.3874518871307373
40433 2023-02-16,23:40:44.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 169/15000, loss = 1.386922001838684
37983 2023-02-16,23:40:44.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3731170892715454
38255 2023-02-16,23:40:44.083 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.3814218044281006
40293 2023-02-16,23:40:44.133 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
38487 2023-02-16,23:40:44.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 650/15000, loss = 1.3749370574951172
38108 2023-02-16,23:40:44.168 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.339796781539917
38362 2023-02-16,23:40:44.201 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 679/15000, loss = 1.4211851358413696
37533 2023-02-16,23:40:44.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 814/15000, loss = 1.3770699501037598
40433 2023-02-16,23:40:44.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 170/15000, loss = 1.4025115966796875
38255 2023-02-16,23:40:44.273 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3873542547225952
37983 2023-02-16,23:40:44.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.328273057937622
40293 2023-02-16,23:40:44.321 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
38487 2023-02-16,23:40:44.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 651/15000, loss = 1.407762885093689
38108 2023-02-16,23:40:44.373 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4062806367874146
38362 2023-02-16,23:40:44.409 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 680/15000, loss = 1.4107657670974731
37533 2023-02-16,23:40:44.418 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 815/15000, loss = 1.4154105186462402
40433 2023-02-16,23:40:44.460 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 171/15000, loss = 1.4000447988510132
38255 2023-02-16,23:40:44.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3543018102645874
37983 2023-02-16,23:40:44.491 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.3650294542312622
38487 2023-02-16,23:40:44.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 652/15000, loss = 1.3999643325805664
40293 2023-02-16,23:40:44.528 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
38108 2023-02-16,23:40:44.563 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4237141609191895
38362 2023-02-16,23:40:44.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 681/15000, loss = 1.4088701009750366
37533 2023-02-16,23:40:44.624 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 816/15000, loss = 1.422117829322815
40433 2023-02-16,23:40:44.666 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 172/15000, loss = 1.4622737169265747
38255 2023-02-16,23:40:44.672 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4279060363769531
37983 2023-02-16,23:40:44.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.376560926437378
38487 2023-02-16,23:40:44.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 653/15000, loss = 1.3827564716339111
40293 2023-02-16,23:40:44.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
38108 2023-02-16,23:40:44.765 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.41716730594635
38362 2023-02-16,23:40:44.818 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 682/15000, loss = 1.3910620212554932
37533 2023-02-16,23:40:44.825 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 817/15000, loss = 1.4217777252197266
38255 2023-02-16,23:40:44.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.407955288887024
40433 2023-02-16,23:40:44.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 173/15000, loss = 1.3354815244674683
37983 2023-02-16,23:40:44.915 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.397192120552063
38487 2023-02-16,23:40:44.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 654/15000, loss = 1.4308747053146362
40293 2023-02-16,23:40:44.938 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 180/15000, loss = 1.4110268354415894
38108 2023-02-16,23:40:44.968 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.392320156097412
37533 2023-02-16,23:40:45.013 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 818/15000, loss = 1.3886348009109497
38362 2023-02-16,23:40:45.021 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 683/15000, loss = 1.4114189147949219
38255 2023-02-16,23:40:45.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.3661168813705444
40433 2023-02-16,23:40:45.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 174/15000, loss = 1.4375747442245483
40293 2023-02-16,23:40:45.130 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 181/15000, loss = 1.3839552402496338
37983 2023-02-16,23:40:45.136 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.3626365661621094
38487 2023-02-16,23:40:45.143 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 655/15000, loss = 1.391998052597046
38108 2023-02-16,23:40:45.174 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.3596765995025635
37533 2023-02-16,23:40:45.218 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 819/15000, loss = 1.4197841882705688
38362 2023-02-16,23:40:45.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 684/15000, loss = 1.3872699737548828
38255 2023-02-16,23:40:45.252 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.4206680059432983
40433 2023-02-16,23:40:45.258 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 175/15000, loss = 1.3733946084976196
40293 2023-02-16,23:40:45.316 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 182/15000, loss = 1.3928282260894775
38487 2023-02-16,23:40:45.331 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 656/15000, loss = 1.412222146987915
37983 2023-02-16,23:40:45.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3610249757766724
38108 2023-02-16,23:40:45.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.4079585075378418
37533 2023-02-16,23:40:45.425 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 820/15000, loss = 1.389255404472351
38362 2023-02-16,23:40:45.434 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 685/15000, loss = 1.3310539722442627
38255 2023-02-16,23:40:45.459 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3913637399673462
40433 2023-02-16,23:40:45.465 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 176/15000, loss = 1.3783527612686157
40293 2023-02-16,23:40:45.506 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 183/15000, loss = 1.4150488376617432
38487 2023-02-16,23:40:45.520 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 657/15000, loss = 1.432945966720581
37983 2023-02-16,23:40:45.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.3965916633605957
38108 2023-02-16,23:40:45.570 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.401199221611023
37533 2023-02-16,23:40:45.631 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 821/15000, loss = 1.3601727485656738
38362 2023-02-16,23:40:45.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 686/15000, loss = 1.4112367630004883
38255 2023-02-16,23:40:45.653 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.480365514755249
40433 2023-02-16,23:40:45.670 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 177/15000, loss = 1.3589351177215576
40293 2023-02-16,23:40:45.711 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 184/15000, loss = 1.3693569898605347
38487 2023-02-16,23:40:45.724 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 658/15000, loss = 1.3646352291107178
37983 2023-02-16,23:40:45.743 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3625001907348633
38108 2023-02-16,23:40:45.773 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3507473468780518
37533 2023-02-16,23:40:45.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 822/15000, loss = 1.4174436330795288
38255 2023-02-16,23:40:45.841 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3683550357818604
38362 2023-02-16,23:40:45.842 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 687/15000, loss = 1.3855966329574585
40433 2023-02-16,23:40:45.871 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 178/15000, loss = 1.3636393547058105
40293 2023-02-16,23:40:45.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 185/15000, loss = 1.4168565273284912
38487 2023-02-16,23:40:45.914 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 659/15000, loss = 1.3690228462219238
37983 2023-02-16,23:40:45.948 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.3614885807037354
38108 2023-02-16,23:40:45.966 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.374485969543457
38255 2023-02-16,23:40:46.033 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.379833459854126
37533 2023-02-16,23:40:46.044 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 823/15000, loss = 1.4318907260894775
38362 2023-02-16,23:40:46.049 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 688/15000, loss = 1.3788033723831177
40433 2023-02-16,23:40:46.080 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 179/15000, loss = 1.362707257270813
38362 2023-02-16,23:40:46.147 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 689/15000, loss = 1.4175488948822021
38362 2023-02-16,23:40:46.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 690/15000, loss = 1.3680949211120605
38362 2023-02-16,23:40:46.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 691/15000, loss = 1.3953546285629272
38362 2023-02-16,23:40:46.286 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 692/15000, loss = 1.381332516670227
38362 2023-02-16,23:40:46.330 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 693/15000, loss = 1.3872700929641724
38362 2023-02-16,23:40:46.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 694/15000, loss = 1.3543566465377808
38362 2023-02-16,23:40:46.421 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 695/15000, loss = 1.4280048608779907
38362 2023-02-16,23:40:46.466 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 696/15000, loss = 1.4080195426940918
38362 2023-02-16,23:40:46.511 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 697/15000, loss = 1.366335391998291
38362 2023-02-16,23:40:46.558 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 698/15000, loss = 1.420676350593567
Terminated
38362 2023-02-16,23:40:46.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 699/15000, loss = 1.3915495872497559
38362 2023-02-16,23:40:46.652 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 700/15000, loss = 1.4802587032318115
38362 2023-02-16,23:40:46.699 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 701/15000, loss = 1.3683494329452515
38362 2023-02-16,23:40:46.744 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 702/15000, loss = 1.3798408508300781
38362 2023-02-16,23:40:46.788 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 703/15000, loss = 1.374862790107727
Terminated
38362 2023-02-16,23:40:46.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 704/15000, loss = 1.4515836238861084
38362 2023-02-16,23:40:46.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 705/15000, loss = 1.3736201524734497
38362 2023-02-16,23:40:46.930 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 706/15000, loss = 1.3820395469665527
38362 2023-02-16,23:40:46.979 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 707/15000, loss = 1.4051074981689453
38362 2023-02-16,23:40:47.028 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 708/15000, loss = 1.370205283164978
38362 2023-02-16,23:40:47.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 709/15000, loss = 1.316502332687378
38362 2023-02-16,23:40:47.128 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 710/15000, loss = 1.3997647762298584
38362 2023-02-16,23:40:47.182 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 711/15000, loss = 1.381234884262085
Terminated
Terminated
38362 2023-02-16,23:40:47.234 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 712/15000, loss = 1.441159963607788
38362 2023-02-16,23:40:47.284 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 713/15000, loss = 1.314253807067871
Terminated
38362 2023-02-16,23:40:47.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 714/15000, loss = 1.337533950805664
38362 2023-02-16,23:40:47.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 715/15000, loss = 1.3703243732452393
Terminated
38362 2023-02-16,23:40:47.419 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 716/15000, loss = 1.3400020599365234
38362 2023-02-16,23:40:47.467 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 717/15000, loss = 1.4063152074813843
Terminated
38362 2023-02-16,23:40:47.522 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 718/15000, loss = 1.4236698150634766
38362 2023-02-16,23:40:47.568 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 719/15000, loss = 1.417420744895935
38362 2023-02-16,23:40:47.617 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 720/15000, loss = 1.3923656940460205
38362 2023-02-16,23:40:47.662 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 721/15000, loss = 1.359732985496521
38362 2023-02-16,23:40:47.707 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 722/15000, loss = 1.407931923866272
38362 2023-02-16,23:40:47.752 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 723/15000, loss = 1.4012261629104614
38362 2023-02-16,23:40:47.803 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 724/15000, loss = 1.3508398532867432
38362 2023-02-16,23:40:47.854 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 725/15000, loss = 1.3744456768035889
38362 2023-02-16,23:40:47.901 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 726/15000, loss = 1.3727914094924927
38362 2023-02-16,23:40:47.945 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 727/15000, loss = 1.3820744752883911
38362 2023-02-16,23:40:47.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 728/15000, loss = 1.4011584520339966
38362 2023-02-16,23:40:48.040 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 729/15000, loss = 1.4600207805633545
38362 2023-02-16,23:40:48.086 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 730/15000, loss = 1.34332275390625
38362 2023-02-16,23:40:48.131 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 731/15000, loss = 1.3866827487945557
38362 2023-02-16,23:40:48.175 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 732/15000, loss = 1.3985035419464111
38362 2023-02-16,23:40:48.219 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 733/15000, loss = 1.4067656993865967
38362 2023-02-16,23:40:48.264 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 734/15000, loss = 1.3826324939727783
38362 2023-02-16,23:40:48.310 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 735/15000, loss = 1.3679289817810059
38362 2023-02-16,23:40:48.355 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 736/15000, loss = 1.4254488945007324
38362 2023-02-16,23:40:48.399 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 737/15000, loss = 1.4193824529647827
38362 2023-02-16,23:40:48.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 738/15000, loss = 1.4090698957443237
38362 2023-02-16,23:40:48.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 739/15000, loss = 1.3732037544250488
38362 2023-02-16,23:40:48.536 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 740/15000, loss = 1.3282748460769653
38362 2023-02-16,23:40:48.587 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 741/15000, loss = 1.3651000261306763
38362 2023-02-16,23:40:48.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 742/15000, loss = 1.3768048286437988
38362 2023-02-16,23:40:48.684 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 743/15000, loss = 1.3974345922470093
38362 2023-02-16,23:40:48.733 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 744/15000, loss = 1.362877607345581
38362 2023-02-16,23:40:48.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 745/15000, loss = 1.3610141277313232
38362 2023-02-16,23:40:48.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 746/15000, loss = 1.3966426849365234
38362 2023-02-16,23:40:48.867 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 747/15000, loss = 1.3627684116363525
38362 2023-02-16,23:40:48.911 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 748/15000, loss = 1.3614277839660645
38362 2023-02-16,23:40:48.956 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 749/15000, loss = 1.3913822174072266
38362 2023-02-16,23:40:49.000 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 750/15000, loss = 1.3764721155166626
38362 2023-02-16,23:40:49.048 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 751/15000, loss = 1.4061206579208374
38362 2023-02-16,23:40:49.103 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 752/15000, loss = 1.406883955001831
38362 2023-02-16,23:40:49.180 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 753/15000, loss = 1.4081748723983765
38362 2023-02-16,23:40:49.246 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 754/15000, loss = 1.3514357805252075
38362 2023-02-16,23:40:49.290 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 755/15000, loss = 1.4040721654891968
38362 2023-02-16,23:40:49.334 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 756/15000, loss = 1.3602008819580078
38362 2023-02-16,23:40:49.378 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 757/15000, loss = 1.4199873208999634
38362 2023-02-16,23:40:49.425 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 758/15000, loss = 1.3649606704711914
38362 2023-02-16,23:40:49.469 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 759/15000, loss = 1.3792927265167236
38362 2023-02-16,23:40:49.513 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 760/15000, loss = 1.38604736328125
38362 2023-02-16,23:40:49.565 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 761/15000, loss = 1.441044569015503
38362 2023-02-16,23:40:49.646 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 762/15000, loss = 1.3458547592163086
38362 2023-02-16,23:40:49.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 763/15000, loss = 1.379146933555603
38362 2023-02-16,23:40:49.820 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 764/15000, loss = 1.428247332572937
38362 2023-02-16,23:40:49.895 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 765/15000, loss = 1.3806087970733643
38362 2023-02-16,23:40:49.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 766/15000, loss = 1.4389896392822266
38362 2023-02-16,23:40:50.043 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 767/15000, loss = 1.3958144187927246
38362 2023-02-16,23:40:50.123 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 768/15000, loss = 1.3634850978851318
38362 2023-02-16,23:40:50.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 769/15000, loss = 1.4101333618164062
38362 2023-02-16,23:40:50.299 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 770/15000, loss = 1.397072434425354
38362 2023-02-16,23:40:50.374 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 771/15000, loss = 1.426038384437561
38362 2023-02-16,23:40:50.447 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 772/15000, loss = 1.4095944166183472
38362 2023-02-16,23:40:50.523 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 773/15000, loss = 1.3798959255218506
38362 2023-02-16,23:40:50.604 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 774/15000, loss = 1.3523457050323486
38362 2023-02-16,23:40:50.678 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 775/15000, loss = 1.4124195575714111
38362 2023-02-16,23:40:50.731 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 776/15000, loss = 1.4364140033721924
38362 2023-02-16,23:40:50.785 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 777/15000, loss = 1.4339362382888794
38362 2023-02-16,23:40:50.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 778/15000, loss = 1.41030752658844
38362 2023-02-16,23:40:50.894 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 779/15000, loss = 1.4565192461013794
38362 2023-02-16,23:40:50.947 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 780/15000, loss = 1.4145749807357788
38362 2023-02-16,23:40:51.001 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 781/15000, loss = 1.3615305423736572
38362 2023-02-16,23:40:51.055 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 782/15000, loss = 1.392224669456482
38362 2023-02-16,23:40:51.109 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 783/15000, loss = 1.3746490478515625
38362 2023-02-16,23:40:51.166 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 784/15000, loss = 1.416008710861206
38362 2023-02-16,23:40:51.227 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 785/15000, loss = 1.3723084926605225
38362 2023-02-16,23:40:51.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 786/15000, loss = 1.396720290184021
38362 2023-02-16,23:40:51.337 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 787/15000, loss = 1.4343221187591553
38362 2023-02-16,23:40:51.390 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 788/15000, loss = 1.4013456106185913
38362 2023-02-16,23:40:51.443 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 789/15000, loss = 1.3527963161468506
38362 2023-02-16,23:40:51.495 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 790/15000, loss = 1.4232735633850098
38362 2023-02-16,23:40:51.548 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 791/15000, loss = 1.3967485427856445
38362 2023-02-16,23:40:51.603 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 792/15000, loss = 1.3347325325012207
38362 2023-02-16,23:40:51.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 793/15000, loss = 1.3692296743392944
38362 2023-02-16,23:40:51.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 794/15000, loss = 1.3889219760894775
38362 2023-02-16,23:40:51.770 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 795/15000, loss = 1.3583133220672607
38362 2023-02-16,23:40:51.830 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 796/15000, loss = 1.3909223079681396
38362 2023-02-16,23:40:51.882 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 797/15000, loss = 1.34161376953125
38362 2023-02-16,23:40:51.935 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 798/15000, loss = 1.3979127407073975
38362 2023-02-16,23:40:52.003 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 799/15000, loss = 1.3917690515518188
38362 2023-02-16,23:40:52.056 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 800/15000, loss = 1.3580453395843506
38362 2023-02-16,23:40:52.109 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 801/15000, loss = 1.3734210729599
38362 2023-02-16,23:40:52.163 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 802/15000, loss = 1.396525263786316
38362 2023-02-16,23:40:52.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 803/15000, loss = 1.4403226375579834
38362 2023-02-16,23:40:52.271 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 804/15000, loss = 1.4323455095291138
38362 2023-02-16,23:40:52.325 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 805/15000, loss = 1.3899016380310059
38362 2023-02-16,23:40:52.381 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 806/15000, loss = 1.3838722705841064
38362 2023-02-16,23:40:52.435 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 807/15000, loss = 1.3735334873199463
38362 2023-02-16,23:40:52.493 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 808/15000, loss = 1.4210162162780762
38362 2023-02-16,23:40:52.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 809/15000, loss = 1.4181623458862305
38362 2023-02-16,23:40:52.612 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 810/15000, loss = 1.391335368156433
38362 2023-02-16,23:40:52.674 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 811/15000, loss = 1.2971527576446533
38362 2023-02-16,23:40:52.727 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 812/15000, loss = 1.3438563346862793
38362 2023-02-16,23:40:52.781 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 813/15000, loss = 1.3876174688339233
38362 2023-02-16,23:40:52.835 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 814/15000, loss = 1.377070426940918
38362 2023-02-16,23:40:52.890 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 815/15000, loss = 1.4184932708740234
38362 2023-02-16,23:40:52.943 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 816/15000, loss = 1.4253357648849487
38362 2023-02-16,23:40:52.997 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 817/15000, loss = 1.4236640930175781
38362 2023-02-16,23:40:53.050 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 818/15000, loss = 1.3894920349121094
38362 2023-02-16,23:40:53.105 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 819/15000, loss = 1.4221844673156738
38362 2023-02-16,23:40:53.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 820/15000, loss = 1.389694333076477
38362 2023-02-16,23:40:53.223 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 821/15000, loss = 1.3593499660491943
38362 2023-02-16,23:40:53.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 822/15000, loss = 1.4186681509017944
38362 2023-02-16,23:40:53.338 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 823/15000, loss = 1.4363012313842773
38362 2023-02-16,23:40:53.392 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 824/15000, loss = 1.4125127792358398
38362 2023-02-16,23:40:53.446 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 825/15000, loss = 1.4022372961044312
38362 2023-02-16,23:40:53.499 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 826/15000, loss = 1.4180399179458618
38362 2023-02-16,23:40:53.553 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 827/15000, loss = 1.3996596336364746
38362 2023-02-16,23:40:53.605 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 828/15000, loss = 1.4107816219329834
38362 2023-02-16,23:40:53.656 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 829/15000, loss = 1.39859139919281
38362 2023-02-16,23:40:53.710 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 830/15000, loss = 1.417886734008789
38362 2023-02-16,23:40:53.771 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 831/15000, loss = 1.368504524230957
38362 2023-02-16,23:40:53.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 832/15000, loss = 1.4476431608200073
38362 2023-02-16,23:40:53.887 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 833/15000, loss = 1.3581550121307373
38362 2023-02-16,23:40:53.939 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 834/15000, loss = 1.373698115348816
38362 2023-02-16,23:40:53.990 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 835/15000, loss = 1.3633689880371094
38362 2023-02-16,23:40:54.042 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 836/15000, loss = 1.4349539279937744
38362 2023-02-16,23:40:54.094 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 837/15000, loss = 1.3426142930984497
38362 2023-02-16,23:40:54.159 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 838/15000, loss = 1.3641198873519897
38362 2023-02-16,23:40:54.221 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 839/15000, loss = 1.413094162940979
38362 2023-02-16,23:40:54.282 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 840/15000, loss = 1.415623664855957
38362 2023-02-16,23:40:54.348 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 841/15000, loss = 1.3933897018432617
38362 2023-02-16,23:40:54.427 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 842/15000, loss = 1.3841001987457275
38362 2023-02-16,23:40:54.496 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 843/15000, loss = 1.3483059406280518
38362 2023-02-16,23:40:54.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 844/15000, loss = 1.378815770149231
38362 2023-02-16,23:40:54.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 845/15000, loss = 1.3624882698059082
38362 2023-02-16,23:40:54.697 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 846/15000, loss = 1.433453917503357
38362 2023-02-16,23:40:54.762 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 847/15000, loss = 1.4265756607055664
38362 2023-02-16,23:40:54.822 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 848/15000, loss = 1.4138405323028564
38362 2023-02-16,23:40:54.889 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 849/15000, loss = 1.4054641723632812
38362 2023-02-16,23:40:54.971 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 850/15000, loss = 1.3969457149505615
38362 2023-02-16,23:40:55.025 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 851/15000, loss = 1.4545488357543945
38362 2023-02-16,23:40:55.069 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 852/15000, loss = 1.3987514972686768
38362 2023-02-16,23:40:55.113 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 853/15000, loss = 1.3801698684692383
38362 2023-02-16,23:40:55.157 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 854/15000, loss = 1.3602224588394165
38362 2023-02-16,23:40:55.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 855/15000, loss = 1.3781834840774536
38362 2023-02-16,23:40:55.250 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 856/15000, loss = 1.4303303956985474
38362 2023-02-16,23:40:55.295 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 857/15000, loss = 1.3927106857299805
38362 2023-02-16,23:40:55.339 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 858/15000, loss = 1.4168994426727295
38362 2023-02-16,23:40:55.384 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 859/15000, loss = 1.4145172834396362
38362 2023-02-16,23:40:55.432 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 860/15000, loss = 1.410897135734558
38362 2023-02-16,23:40:55.477 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 861/15000, loss = 1.4047942161560059
38362 2023-02-16,23:40:55.521 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 862/15000, loss = 1.383474588394165
38362 2023-02-16,23:40:55.569 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 863/15000, loss = 1.4157859086990356
38362 2023-02-16,23:40:55.613 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 864/15000, loss = 1.412616491317749
38362 2023-02-16,23:40:55.657 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 865/15000, loss = 1.3924895524978638
38362 2023-02-16,23:40:55.703 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 866/15000, loss = 1.365246295928955
38362 2023-02-16,23:40:55.748 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 867/15000, loss = 1.3608691692352295
38362 2023-02-16,23:40:55.793 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 868/15000, loss = 1.4368500709533691
38362 2023-02-16,23:40:55.840 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 869/15000, loss = 1.3441598415374756
38362 2023-02-16,23:40:55.884 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 870/15000, loss = 1.3539899587631226
38362 2023-02-16,23:40:55.928 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 871/15000, loss = 1.362440824508667
38362 2023-02-16,23:40:55.974 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 872/15000, loss = 1.4041728973388672
38362 2023-02-16,23:40:56.020 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 873/15000, loss = 1.381582260131836
38362 2023-02-16,23:40:56.073 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 874/15000, loss = 1.4180821180343628
38362 2023-02-16,23:40:56.124 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 875/15000, loss = 1.4158364534378052
38362 2023-02-16,23:40:56.171 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 876/15000, loss = 1.3339509963989258
38362 2023-02-16,23:40:56.217 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 877/15000, loss = 1.4298750162124634
38362 2023-02-16,23:40:56.262 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 878/15000, loss = 1.3724197149276733
38362 2023-02-16,23:40:56.306 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 879/15000, loss = 1.3566055297851562
38362 2023-02-16,23:40:56.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 880/15000, loss = 1.3890413045883179
38362 2023-02-16,23:40:56.397 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 881/15000, loss = 1.3936541080474854
38362 2023-02-16,23:40:56.441 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 882/15000, loss = 1.3906853199005127
38362 2023-02-16,23:40:56.489 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 883/15000, loss = 1.3873406648635864
38362 2023-02-16,23:40:56.533 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 884/15000, loss = 1.3816261291503906
38362 2023-02-16,23:40:56.581 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 885/15000, loss = 1.3787903785705566
38362 2023-02-16,23:40:56.627 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 886/15000, loss = 1.4352006912231445
38362 2023-02-16,23:40:56.677 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 887/15000, loss = 1.380509614944458
38362 2023-02-16,23:40:56.735 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 888/15000, loss = 1.3661595582962036
38362 2023-02-16,23:40:56.786 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 889/15000, loss = 1.3839777708053589
38362 2023-02-16,23:40:56.831 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 890/15000, loss = 1.380030632019043
38362 2023-02-16,23:40:56.877 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 891/15000, loss = 1.3722862005233765
38362 2023-02-16,23:40:56.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 892/15000, loss = 1.4119043350219727
38362 2023-02-16,23:40:56.967 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 893/15000, loss = 1.38735830783844
38362 2023-02-16,23:40:57.017 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 894/15000, loss = 1.3569207191467285
38362 2023-02-16,23:40:57.065 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 895/15000, loss = 1.390302062034607
38362 2023-02-16,23:40:57.112 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 896/15000, loss = 1.3728729486465454
38362 2023-02-16,23:40:57.160 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 897/15000, loss = 1.398323655128479
38362 2023-02-16,23:40:57.206 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 898/15000, loss = 1.4212342500686646
38362 2023-02-16,23:40:57.251 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 899/15000, loss = 1.3648786544799805
38362 2023-02-16,23:40:57.302 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 900/15000, loss = 1.384048342704773
38362 2023-02-16,23:40:57.353 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 901/15000, loss = 1.3764675855636597
38362 2023-02-16,23:40:57.404 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 902/15000, loss = 1.3802151679992676
38362 2023-02-16,23:40:57.449 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 903/15000, loss = 1.3800745010375977
38362 2023-02-16,23:40:57.494 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 904/15000, loss = 1.4501726627349854
38362 2023-02-16,23:40:57.539 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 905/15000, loss = 1.3605767488479614
38362 2023-02-16,23:40:57.607 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 906/15000, loss = 1.4071613550186157
38362 2023-02-16,23:40:57.667 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 907/15000, loss = 1.4107677936553955
38362 2023-02-16,23:40:57.726 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 908/15000, loss = 1.4301707744598389
38362 2023-02-16,23:40:57.799 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 909/15000, loss = 1.4354605674743652
38362 2023-02-16,23:40:57.859 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 910/15000, loss = 1.4251487255096436
38362 2023-02-16,23:40:57.922 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 911/15000, loss = 1.4138075113296509
38362 2023-02-16,23:40:57.992 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 912/15000, loss = 1.4130233526229858
38362 2023-02-16,23:40:58.051 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 913/15000, loss = 1.3661373853683472
38362 2023-02-16,23:40:58.114 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 914/15000, loss = 1.406760811805725
38362 2023-02-16,23:40:58.184 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 915/15000, loss = 1.385472059249878
38362 2023-02-16,23:40:58.248 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 916/15000, loss = 1.4640504121780396
38362 2023-02-16,23:40:58.308 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 917/15000, loss = 1.3767883777618408
38362 2023-02-16,23:40:58.383 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 918/15000, loss = 1.3438721895217896
38362 2023-02-16,23:40:58.444 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 919/15000, loss = 1.381253957748413
38362 2023-02-16,23:40:58.505 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 920/15000, loss = 1.4637093544006348
38362 2023-02-16,23:40:58.574 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 921/15000, loss = 1.380480170249939
38362 2023-02-16,23:40:58.636 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 922/15000, loss = 1.3468873500823975
38362 2023-02-16,23:40:58.701 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 923/15000, loss = 1.3930623531341553
38362 2023-02-16,23:40:58.768 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 924/15000, loss = 1.4151378870010376
38362 2023-02-16,23:40:58.833 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 925/15000, loss = 1.3707423210144043
38362 2023-02-16,23:40:58.893 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 926/15000, loss = 1.391926884651184
38362 2023-02-16,23:40:58.962 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 927/15000, loss = 1.3928427696228027
38362 2023-02-16,23:40:59.029 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 928/15000, loss = 1.4077112674713135
38362 2023-02-16,23:40:59.088 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 929/15000, loss = 1.3184027671813965
38362 2023-02-16,23:40:59.156 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 930/15000, loss = 1.3602951765060425
38362 2023-02-16,23:40:59.226 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 931/15000, loss = 1.4360482692718506
38362 2023-02-16,23:40:59.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 932/15000, loss = 1.3660331964492798
38362 2023-02-16,23:40:59.352 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 933/15000, loss = 1.3883700370788574
38362 2023-02-16,23:40:59.420 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 934/15000, loss = 1.437086820602417
38362 2023-02-16,23:40:59.480 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 935/15000, loss = 1.4078881740570068
38362 2023-02-16,23:40:59.540 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 936/15000, loss = 1.3861396312713623
38362 2023-02-16,23:40:59.616 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 937/15000, loss = 1.3824814558029175
38362 2023-02-16,23:40:59.676 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 938/15000, loss = 1.4475053548812866
38362 2023-02-16,23:40:59.736 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 939/15000, loss = 1.3870750665664673
38362 2023-02-16,23:40:59.813 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 940/15000, loss = 1.4035457372665405
38362 2023-02-16,23:40:59.872 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 941/15000, loss = 1.4008901119232178
38362 2023-02-16,23:40:59.932 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 942/15000, loss = 1.3952611684799194
38362 2023-02-16,23:41:00.006 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 943/15000, loss = 1.486549735069275
38362 2023-02-16,23:41:00.066 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 944/15000, loss = 1.3577960729599
38362 2023-02-16,23:41:00.126 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 945/15000, loss = 1.3649543523788452
38362 2023-02-16,23:41:00.200 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 946/15000, loss = 1.368782877922058
38362 2023-02-16,23:41:00.260 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 947/15000, loss = 1.3455286026000977
38362 2023-02-16,23:41:00.320 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 948/15000, loss = 1.4110791683197021
38362 2023-02-16,23:41:00.393 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 949/15000, loss = 1.3778190612792969
38362 2023-02-16,23:41:00.454 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 950/15000, loss = 1.3551692962646484
38362 2023-02-16,23:41:00.514 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 951/15000, loss = 1.4259203672409058
38362 2023-02-16,23:41:00.589 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 952/15000, loss = 1.4068865776062012
38362 2023-02-16,23:41:00.649 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 953/15000, loss = 1.385646104812622
38362 2023-02-16,23:41:00.709 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 954/15000, loss = 1.416987419128418
38362 2023-02-16,23:41:00.778 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 955/15000, loss = 1.38456392288208
38362 2023-02-16,23:41:00.838 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 956/15000, loss = 1.4067033529281616
38362 2023-02-16,23:41:00.904 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 957/15000, loss = 1.4356647729873657
38362 2023-02-16,23:41:00.975 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 958/15000, loss = 1.3641403913497925
38362 2023-02-16,23:41:01.036 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 959/15000, loss = 1.4170191287994385
38362 2023-02-16,23:41:01.096 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 960/15000, loss = 1.3798258304595947
38362 2023-02-16,23:41:01.164 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 961/15000, loss = 1.379127860069275
38362 2023-02-16,23:41:01.233 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 962/15000, loss = 1.3911596536636353
38362 2023-02-16,23:41:01.294 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 963/15000, loss = 1.3698797225952148
38362 2023-02-16,23:41:01.364 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 964/15000, loss = 1.3921282291412354
38362 2023-02-16,23:41:01.426 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 965/15000, loss = 1.3641796112060547
38362 2023-02-16,23:41:01.486 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 966/15000, loss = 1.335785150527954
38362 2023-02-16,23:41:01.550 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 967/15000, loss = 1.430108904838562
38362 2023-02-16,23:41:01.594 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 968/15000, loss = 1.3549063205718994
38362 2023-02-16,23:41:01.639 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 969/15000, loss = 1.4060667753219604
38362 2023-02-16,23:41:01.688 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 970/15000, loss = 1.3835675716400146
38362 2023-02-16,23:41:01.732 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 971/15000, loss = 1.3356446027755737
38362 2023-02-16,23:41:01.776 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 972/15000, loss = 1.3929269313812256
38362 2023-02-16,23:41:01.821 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 973/15000, loss = 1.3490504026412964
38362 2023-02-16,23:41:01.866 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 974/15000, loss = 1.418309211730957
38362 2023-02-16,23:41:01.918 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 975/15000, loss = 1.3819317817687988
38362 2023-02-16,23:41:01.969 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 976/15000, loss = 1.388106107711792
38362 2023-02-16,23:41:02.018 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 977/15000, loss = 1.4167890548706055
38362 2023-02-16,23:41:02.061 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 978/15000, loss = 1.336047887802124
38362 2023-02-16,23:41:02.105 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 979/15000, loss = 1.3893179893493652
38362 2023-02-16,23:41:02.152 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 980/15000, loss = 1.3769505023956299
38362 2023-02-16,23:41:02.197 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 981/15000, loss = 1.440612554550171
38362 2023-02-16,23:41:02.241 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 982/15000, loss = 1.4333502054214478
38362 2023-02-16,23:41:02.285 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 983/15000, loss = 1.3506654500961304
38362 2023-02-16,23:41:02.329 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 984/15000, loss = 1.4189893007278442
38362 2023-02-16,23:41:02.376 - {tc_transformer_trainer.py (121)} - train_model(): epoch = 0, batch_idx = 985/15000, loss = 1.4055075645446777
Terminated
run_text_classification_sweep.sh: 7: run_text_classification_sweep.sh: cannot create ./log/distilbert/lr_1e-2/agnews_forward_lr=0.01_layer=e_v_num=1.log: Directory nonexistent
run_text_classification_sweep.sh: 7: run_text_classification_sweep.sh: cannot create ./log/distilbert/lr_1e-2/agnews_forward_lr=0.01_layer=e_v_num=50.log: Directory nonexistent
run_text_classification_sweep.sh: 7: run_text_classification_sweep.sh: cannot create ./log/distilbert/lr_1e-2/agnews_forward_lr=0.01_layer=e_v_num=200.log: Directory nonexistent
