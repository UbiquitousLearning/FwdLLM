{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.21214069426059723,\n",
       "  'token': 2402,\n",
       "  'token_str': 'young',\n",
       "  'sequence': 'yaozong wu is a young boy.'},\n",
       " {'score': 0.03464598208665848,\n",
       "  'token': 2822,\n",
       "  'token_str': 'chinese',\n",
       "  'sequence': 'yaozong wu is a chinese boy.'},\n",
       " {'score': 0.028944874182343483,\n",
       "  'token': 9454,\n",
       "  'token_str': 'teenage',\n",
       "  'sequence': 'yaozong wu is a teenage boy.'},\n",
       " {'score': 0.028627987951040268,\n",
       "  'token': 4138,\n",
       "  'token_str': 'rich',\n",
       "  'sequence': 'yaozong wu is a rich boy.'},\n",
       " {'score': 0.02324806898832321,\n",
       "  'token': 3532,\n",
       "  'token_str': 'poor',\n",
       "  'sequence': 'yaozong wu is a poor boy.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-large-uncased')\n",
    "unmasker(\"Yaozong Wu is a [MASK] boy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8262e-05)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "total = 0\n",
    "for _ in range(10):\n",
    "    true_grad = torch.randn((768,768))\n",
    "    forward_grad = torch.randn((768,768))\n",
    "    cos_sim = torch.cosine_similarity(true_grad, forward_grad,dim=-1).mean()\n",
    "    # print(cos_sim)\n",
    "    total += cos_sim.abs()\n",
    "print(total/100)\n",
    "# torch.cosine_similarity(torch.Tensor([[1,2,3],[3,4,5]]), torch.Tensor([[3,4,5],[1,2,3]]),dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(torch.Tensor([1,3]), torch.Tensor([3,1]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0080)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# for i in range(100):\n",
    "a = torch.randn(10000,1)\n",
    "print(a.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6639) tensor(0.1169)\n",
      "tensor(0.9230) tensor(0.1915)\n",
      "tensor(0.9826) tensor(0.2130)\n",
      "tensor(1.0929) tensor(0.1548)\n",
      "tensor(1.1148) tensor(0.1316)\n",
      "tensor(1.1217) tensor(0.2309)\n",
      "tensor(0.8108) tensor(0.1768)\n",
      "tensor(0.9486) tensor(0.1985)\n",
      "tensor(1.2245) tensor(0.2033)\n",
      "tensor(1.2099) tensor(0.1858)\n",
      "tensor(0.9397) tensor(0.2479)\n",
      "tensor(0.8445) tensor(0.1675)\n",
      "tensor(0.8949) tensor(0.2376)\n",
      "tensor(1.4680) tensor(0.2504)\n",
      "tensor(0.8640) tensor(0.1830)\n",
      "tensor(0.7918) tensor(0.2190)\n",
      "tensor(0.8639) tensor(0.1818)\n",
      "tensor(1.0801) tensor(0.2030)\n",
      "tensor(1.0057) tensor(0.3008)\n",
      "tensor(1.1615) tensor(0.1893)\n",
      "tensor(1.2675) tensor(0.1878)\n",
      "tensor(0.9095) tensor(0.2197)\n",
      "tensor(0.8996) tensor(0.1980)\n",
      "tensor(1.6059) tensor(0.2267)\n",
      "tensor(1.0761) tensor(0.2354)\n",
      "tensor(0.9509) tensor(0.1773)\n",
      "tensor(1.2761) tensor(0.2533)\n",
      "tensor(1.3041) tensor(0.1445)\n",
      "tensor(1.2739) tensor(0.1836)\n",
      "tensor(1.1899) tensor(0.2138)\n",
      "tensor(0.8185) tensor(0.1388)\n",
      "tensor(0.8679) tensor(0.3173)\n",
      "tensor(0.6584) tensor(0.1972)\n",
      "tensor(1.0205) tensor(0.2055)\n",
      "tensor(1.0997) tensor(0.1698)\n",
      "tensor(1.2388) tensor(0.2157)\n",
      "tensor(1.1626) tensor(0.1167)\n",
      "tensor(0.6604) tensor(0.1644)\n",
      "tensor(1.1326) tensor(0.1485)\n",
      "tensor(1.1813) tensor(0.1943)\n",
      "tensor(0.8579) tensor(0.1868)\n",
      "tensor(0.7916) tensor(0.1422)\n",
      "tensor(0.8744) tensor(0.2318)\n",
      "tensor(0.7942) tensor(0.1718)\n",
      "tensor(0.8377) tensor(0.2974)\n",
      "tensor(0.6724) tensor(0.1852)\n",
      "tensor(0.8261) tensor(0.2088)\n",
      "tensor(0.7842) tensor(0.1221)\n",
      "tensor(0.9323) tensor(0.2587)\n",
      "tensor(0.7776) tensor(0.1676)\n",
      "tensor(0.8575) tensor(0.2202)\n",
      "tensor(1.2605) tensor(0.1911)\n",
      "tensor(0.7724) tensor(0.1837)\n",
      "tensor(0.7759) tensor(0.1460)\n",
      "tensor(0.6629) tensor(0.2243)\n",
      "tensor(1.2207) tensor(0.1502)\n",
      "tensor(0.8840) tensor(0.1398)\n",
      "tensor(0.8878) tensor(0.2553)\n",
      "tensor(0.9398) tensor(0.2081)\n",
      "tensor(0.9020) tensor(0.2833)\n",
      "tensor(1.1474) tensor(0.2606)\n",
      "tensor(0.8453) tensor(0.2121)\n",
      "tensor(1.2894) tensor(0.1828)\n",
      "tensor(1.2249) tensor(0.2176)\n",
      "tensor(0.8775) tensor(0.2116)\n",
      "tensor(0.9928) tensor(0.1600)\n",
      "tensor(0.6880) tensor(0.1784)\n",
      "tensor(0.7755) tensor(0.1683)\n",
      "tensor(1.3374) tensor(0.1477)\n",
      "tensor(0.9399) tensor(0.1579)\n",
      "tensor(1.0976) tensor(0.2428)\n",
      "tensor(0.9111) tensor(0.2556)\n",
      "tensor(0.6749) tensor(0.2301)\n",
      "tensor(0.9389) tensor(0.1664)\n",
      "tensor(0.9990) tensor(0.2238)\n",
      "tensor(1.2088) tensor(0.1804)\n",
      "tensor(1.3103) tensor(0.1795)\n",
      "tensor(0.9282) tensor(0.1473)\n",
      "tensor(1.1801) tensor(0.1974)\n",
      "tensor(1.0143) tensor(0.2991)\n",
      "tensor(1.1201) tensor(0.1932)\n",
      "tensor(0.5132) tensor(0.2753)\n",
      "tensor(0.8621) tensor(0.1453)\n",
      "tensor(1.0339) tensor(0.1750)\n",
      "tensor(0.7014) tensor(0.1680)\n",
      "tensor(1.0176) tensor(0.1614)\n",
      "tensor(0.7936) tensor(0.2358)\n",
      "tensor(0.7960) tensor(0.2238)\n",
      "tensor(1.1926) tensor(0.1721)\n",
      "tensor(1.0088) tensor(0.2451)\n",
      "tensor(1.0659) tensor(0.1536)\n",
      "tensor(1.0999) tensor(0.2221)\n",
      "tensor(0.8484) tensor(0.2355)\n",
      "tensor(1.0167) tensor(0.2092)\n",
      "tensor(1.1675) tensor(0.2075)\n",
      "tensor(0.9660) tensor(0.1578)\n",
      "tensor(0.7187) tensor(0.2433)\n",
      "tensor(1.0785) tensor(0.2080)\n",
      "tensor(0.8240) tensor(0.1933)\n",
      "tensor(0.9715) tensor(0.2370)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in range(100):\n",
    "    a = torch.randn(2,1)\n",
    "    b = torch.randn(2,5).mean(axis=1)\n",
    "    print(a.var(),b.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((100,768,48))\n",
    "b = torch.randn((100,768,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 48])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.var(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 48])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:a.shape[0]//2].mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2368e-01, -2.2846e-01,  4.8335e-02,  6.0415e-01,  4.7783e-01,\n",
       "         1.0801e+00,  1.4335e-01, -1.5016e-01, -6.4010e-01,  5.1718e-01,\n",
       "         8.6938e-01, -4.7446e-01,  2.6039e-01,  1.1437e+00,  6.7671e-01,\n",
       "         4.0299e-01,  1.9367e-01,  5.3300e-01, -2.1376e-01,  2.8265e-01,\n",
       "         8.8841e-02, -7.0279e-02,  1.9999e-01, -4.2204e-01, -4.6889e-01,\n",
       "         3.8175e-01,  3.9391e-01,  2.2505e-01, -1.2584e-01,  3.1810e-01,\n",
       "         7.1885e-02,  9.2673e-01, -2.1289e-01, -6.2733e-02,  7.2470e-01,\n",
       "         1.7849e-01, -2.0853e-01, -3.0656e-01,  2.9911e-01, -3.8559e-01,\n",
       "        -3.8201e-01, -2.4964e-01, -6.0718e-01,  4.4502e-01,  7.0829e-01,\n",
       "        -4.3230e-01, -2.0777e-01,  7.9456e-02, -3.4098e-01, -4.0319e-01,\n",
       "         7.2587e-01,  5.8456e-01, -2.3669e-01,  1.6427e-01, -2.0924e-01,\n",
       "        -6.2952e-01,  2.3572e-01, -2.6317e-01,  1.3650e-02, -2.4898e-01,\n",
       "        -3.9227e-01,  5.8637e-01,  5.3582e-01,  1.5643e-01,  4.7532e-01,\n",
       "        -2.2400e-01,  9.8966e-01,  1.9008e-01,  1.5180e-01, -3.1342e-01,\n",
       "         6.8544e-01,  7.0244e-01, -5.8958e-01, -4.5197e-01, -3.5785e-01,\n",
       "         5.8307e-01,  7.3010e-01, -1.1411e-01, -6.2568e-01, -5.9884e-01,\n",
       "        -9.4612e-01,  8.1522e-01,  4.2831e-01, -2.3682e-01, -1.3795e-01,\n",
       "        -3.1223e-01,  4.0393e-01,  3.3337e-01, -3.5584e-02,  2.0441e-01,\n",
       "        -7.3829e-02,  4.9183e-01, -4.5968e-01,  3.6596e-01,  2.7417e-01,\n",
       "        -2.3236e-02, -4.6528e-01,  2.9854e-01, -3.7491e-01, -8.1147e-02,\n",
       "        -5.2706e-01,  2.9279e-01, -2.6907e-01,  9.2631e-02,  3.3257e-01,\n",
       "         6.1906e-01,  4.0344e-01,  2.7413e-01,  1.2260e-01,  3.1414e-01,\n",
       "        -1.4006e-01, -2.8037e-01, -5.8885e-01, -2.1859e-01, -4.2193e-01,\n",
       "        -3.9761e-01,  4.2744e-01,  8.8313e-02, -5.5723e-01,  6.3871e-01,\n",
       "         4.0905e-01, -4.4855e-01, -3.2102e-01, -5.5301e-01,  2.2832e-01,\n",
       "         4.2196e-01,  7.0823e-01, -6.1729e-01,  1.7905e-01, -6.0152e-01,\n",
       "         7.3408e-01,  1.9730e-01,  4.3276e-01,  5.4897e-01,  3.7674e-01,\n",
       "         8.0838e-01,  2.7086e-01,  4.7850e-01, -1.8655e-01,  2.2442e-02,\n",
       "         2.5819e-01,  1.9779e-01, -1.5211e-01,  3.3370e-01, -1.8260e-01,\n",
       "        -2.2761e-01, -4.4214e-01,  7.6310e-01,  3.4936e-03,  2.9495e-01,\n",
       "         1.9116e-01,  1.3487e-01,  4.9486e-01, -1.8511e-01,  4.6052e-01,\n",
       "         4.7580e-01, -4.3838e-01, -2.9512e-02, -1.0620e-01,  3.4028e-01,\n",
       "        -2.3288e-01,  3.4052e-02,  2.0157e-01,  8.8183e-01, -2.9133e-01,\n",
       "         3.4312e-02, -8.4217e-02, -2.3689e-01, -1.4380e-01, -9.5894e-02,\n",
       "         3.5607e-02,  2.2426e-01, -7.0993e-01, -5.3827e-03, -1.3119e-01,\n",
       "         4.6764e-01, -1.8723e-01, -1.2798e-01, -1.5073e-01,  2.1324e-01,\n",
       "        -4.7901e-01,  3.2742e-02, -1.2956e-01, -2.7644e-01,  6.6568e-01,\n",
       "        -8.6692e-01, -2.1601e-01,  1.1804e-01, -1.3336e-01, -7.2030e-01,\n",
       "        -7.8590e-01,  5.9729e-02,  2.1474e-01,  4.2419e-01, -1.4096e-01,\n",
       "        -8.4533e-01,  3.5528e-01, -1.8222e-01, -3.1918e-01, -2.6002e-01,\n",
       "         6.3141e-01,  2.9807e-01,  6.4523e-01,  3.8945e-01, -5.7619e-01,\n",
       "         5.4732e-01,  4.1508e-01, -2.2819e-01,  7.0623e-01, -1.4397e-01,\n",
       "         6.4794e-02,  1.0229e-01,  3.3458e-01,  8.5982e-02,  2.9658e-01,\n",
       "        -1.5036e-02, -2.8947e-01, -3.0701e-01, -3.6414e-02, -2.6110e-01,\n",
       "        -3.4520e-01, -2.2146e-01, -2.9373e-01, -1.7604e-01, -5.6408e-01,\n",
       "        -3.1799e-02, -4.1271e-01, -3.0034e-01,  1.4182e-01,  7.2382e-01,\n",
       "         9.3966e-01,  3.4790e-01,  1.7625e-01,  2.5336e-01, -4.7267e-01,\n",
       "        -5.7967e-01,  7.9732e-01, -3.0743e-01,  5.2436e-02, -4.5990e-01,\n",
       "        -3.4742e-01, -5.9996e-01, -4.2352e-01, -4.9385e-02,  3.2672e-01,\n",
       "        -4.7834e-01,  1.1560e-01,  3.4346e-01,  3.6847e-01, -5.5460e-01,\n",
       "        -6.0369e-01,  3.9892e-01, -2.1757e-01, -4.0223e-01, -1.0818e-01,\n",
       "        -6.9247e-01,  8.3561e-01, -4.9696e-01,  3.4770e-01, -1.8199e-01,\n",
       "        -6.3406e-01, -1.7887e-01,  5.8137e-01,  3.3182e-01, -5.5969e-01,\n",
       "         3.4676e-01,  2.8580e-01, -6.8405e-01, -7.4100e-01,  4.0152e-01,\n",
       "        -2.9841e-01, -4.4798e-01,  3.9014e-01,  4.0808e-01,  3.8869e-01,\n",
       "         9.5158e-02, -9.1598e-02, -2.2667e-02,  1.5637e-01, -1.0232e-01,\n",
       "         8.4607e-02,  3.5571e-01, -2.9672e-01, -3.2153e-01,  2.1795e-01,\n",
       "         1.7549e-01,  9.1075e-01,  2.3048e-02,  2.6766e-01, -3.9277e-02,\n",
       "         4.3479e-01,  1.4994e-01,  1.0063e+00,  2.5217e-01,  5.3731e-01,\n",
       "        -1.3934e-01,  1.0199e-01, -8.6733e-01,  3.8432e-01, -9.8292e-01,\n",
       "        -6.2048e-01,  4.2674e-01, -1.2109e-01,  1.2542e+00, -4.4679e-01,\n",
       "         1.2550e-03,  3.4309e-01,  5.8967e-01, -5.7378e-01,  2.4735e-01,\n",
       "        -6.3192e-01,  4.0735e-01,  8.5219e-01, -2.7765e-01, -1.9813e-01,\n",
       "        -2.7612e-01, -6.8018e-02, -1.9817e-01, -5.4325e-01,  3.6908e-01,\n",
       "         1.4523e-01, -2.4305e-01, -4.7775e-01,  3.2926e-01, -9.5626e-02,\n",
       "        -1.7742e-01,  4.8498e-01,  1.6452e-01, -2.5621e-01, -1.4268e-01,\n",
       "        -2.5996e-01,  2.5913e-01, -3.0272e-01,  8.5153e-01, -1.5105e-01,\n",
       "        -1.4827e-01,  5.7356e-02, -2.0930e-01,  1.6973e-01,  5.4911e-01,\n",
       "         5.4470e-01, -3.6937e-01,  5.9262e-01,  1.1067e-01,  3.8317e-01,\n",
       "        -5.7174e-01,  1.2197e-01, -3.4964e-01, -3.8735e-01, -1.4003e-01,\n",
       "        -2.5644e-01, -1.4278e-01,  8.6738e-02, -8.0759e-01,  1.1882e+00,\n",
       "        -7.9684e-01, -5.4397e-02,  9.8707e-01, -1.1414e-01, -3.8554e-01,\n",
       "        -3.5523e-01,  2.6458e-01, -6.9217e-01, -5.7506e-01,  7.6949e-02,\n",
       "        -2.9207e-01, -1.0485e-01, -4.7843e-01, -2.2541e-01,  1.4096e-01,\n",
       "        -1.5623e-01,  5.1358e-02,  1.0355e+00, -1.8748e-03,  4.6149e-01,\n",
       "        -7.9683e-01,  2.3982e-01,  3.8355e-02,  2.7551e-01,  1.7800e-02,\n",
       "         9.7765e-01, -5.7838e-02,  3.4864e-01,  2.1965e-01,  1.7087e-01,\n",
       "         2.5256e-01,  4.1525e-01,  1.9362e-01, -4.3518e-01,  2.4768e-01,\n",
       "        -3.8283e-02,  4.0292e-01,  3.8730e-02,  2.7379e-01,  8.9480e-02,\n",
       "         2.0870e-02,  1.4839e-01,  1.4732e-01,  1.8175e-01,  1.7445e-01,\n",
       "        -3.0824e-01,  4.4839e-02, -2.7641e-02, -6.8430e-02,  7.5608e-01,\n",
       "        -8.3258e-02,  5.5291e-01,  3.6834e-01,  4.6700e-01,  3.3072e-02,\n",
       "         7.4162e-01,  1.1574e-01, -2.7449e-03, -2.5509e-01, -2.0695e-02,\n",
       "        -1.0007e-01, -3.6488e-01, -1.6433e-01, -4.5397e-01,  3.9306e-01,\n",
       "        -5.4925e-01,  5.1384e-01, -1.1140e-02, -4.3404e-01, -8.9387e-01,\n",
       "         5.8304e-01, -1.0560e-01,  1.7748e-02, -3.7452e-01, -1.1489e+00,\n",
       "         7.7153e-01,  3.3972e-01,  2.9379e-03, -3.4312e-02,  2.9219e-01,\n",
       "        -5.7791e-02, -5.8049e-01,  3.1519e-01, -3.6078e-01, -6.3721e-01,\n",
       "        -6.1790e-01, -5.0332e-01, -2.8871e-01,  2.6152e-01, -5.3033e-01,\n",
       "        -4.3361e-01, -2.8594e-01, -3.7182e-01,  6.5639e-02,  6.8031e-01,\n",
       "        -3.6854e-01,  2.1859e-01,  4.5721e-01, -5.6418e-01,  2.0837e-01,\n",
       "        -6.4298e-01, -7.9845e-02,  1.7344e-01, -1.9184e-01,  2.1414e-01,\n",
       "         3.1710e-01,  4.3137e-01,  3.2118e-01,  7.6623e-01, -4.9127e-02,\n",
       "         4.4651e-01, -9.7134e-02, -3.5134e-01,  1.0127e-01,  3.5306e-01,\n",
       "        -2.5547e-01, -8.4778e-01, -1.0451e+00,  1.6533e-01, -1.4497e+00,\n",
       "         7.2147e-01,  8.6159e-01, -2.4985e-01, -1.9400e-02,  7.9685e-01,\n",
       "         2.9580e-01, -1.9360e-02, -1.9958e-01,  5.5389e-02,  5.6676e-01,\n",
       "         1.1400e-01,  4.2336e-01, -8.5434e-01,  1.9508e-01,  2.3829e-01,\n",
       "         2.5319e-01,  1.2184e-01,  2.5513e-01,  3.7221e-02,  4.2346e-01,\n",
       "         7.5930e-01,  3.8709e-01, -9.8861e-02, -7.5298e-01, -4.3081e-01,\n",
       "         6.1378e-01, -2.2426e-01,  9.2779e-01, -1.7229e-01,  4.8122e-01,\n",
       "         5.1761e-01,  1.4015e-01, -8.7124e-02,  2.4913e-01, -2.5625e-01,\n",
       "         4.2320e-01,  5.2475e-01,  5.8231e-02,  1.4301e-01,  3.6529e-01,\n",
       "        -2.7306e-01, -2.9760e-02,  1.5675e-01, -7.5312e-01,  2.1615e-01,\n",
       "         6.0249e-02, -6.7500e-01,  3.0463e-01,  1.4588e-01, -3.9077e-01,\n",
       "        -5.7166e-01, -2.7269e-01, -1.2674e-01, -4.5547e-01, -7.6915e-01,\n",
       "         5.9840e-01, -1.2838e-01, -3.4033e-01, -3.5857e-01,  1.6141e-01,\n",
       "        -4.2764e-01,  7.7816e-01, -1.3785e-01, -4.8563e-01, -1.5310e-01,\n",
       "        -1.6884e-01,  3.1606e-01,  1.0611e-01,  7.3503e-02,  4.6374e-01,\n",
       "        -2.5633e-01,  1.3790e-01,  1.1223e-02,  2.8467e-01, -6.5039e-02,\n",
       "         1.3772e-01,  4.5074e-01,  2.6829e-01, -1.2912e-01,  3.2935e-01,\n",
       "        -6.0099e-01, -3.8050e-01, -7.1759e-01, -1.3014e+00,  4.0876e-01,\n",
       "        -8.0233e-01,  3.1905e-01, -4.9321e-02, -1.2864e-02,  7.7933e-01,\n",
       "         6.4684e-01, -2.3083e-01, -5.2476e-01,  1.8617e-01, -2.8306e-01,\n",
       "         3.2636e-01, -3.3167e-01,  1.1762e+00, -2.5423e-01,  2.1503e-01,\n",
       "        -6.0635e-01,  2.1703e-01,  6.2535e-01,  1.6330e-03,  2.8092e-01,\n",
       "        -4.2019e-01, -2.8769e-01, -3.3481e-01, -5.1207e-01,  8.3323e-01,\n",
       "        -5.5492e-02, -3.5159e-01,  1.2919e-01,  4.1824e-01,  1.1877e-01,\n",
       "        -4.7023e-01,  2.7140e-01,  1.1472e+00,  3.7132e-01,  1.6566e-01,\n",
       "        -1.0765e+00, -8.2974e-01,  8.4596e-02,  4.9941e-01,  3.1458e-02,\n",
       "         3.5955e-01,  2.4606e-01, -8.6492e-01, -7.0839e-01,  5.0377e-01,\n",
       "        -5.1736e-01,  8.0205e-01,  3.3786e-01,  1.7480e-01,  7.2108e-01,\n",
       "        -2.6715e-01, -1.7474e-01, -3.8524e-01,  1.0572e-01, -5.5077e-01,\n",
       "         3.5454e-01, -1.2337e-01, -5.7016e-01,  8.6701e-01,  3.2253e-02,\n",
       "         4.5045e-01, -4.3969e-01,  1.3615e-01,  1.8708e-01, -7.7978e-02,\n",
       "        -4.8071e-01, -2.8683e-01,  7.3213e-01, -8.9736e-02, -7.8176e-01,\n",
       "         7.1653e-01,  1.6381e-01, -2.4205e-01, -2.3020e-01, -4.2446e-01,\n",
       "        -6.6397e-01, -7.4411e-01, -8.3112e-02,  1.4098e-01, -6.0038e-01,\n",
       "        -3.5440e-01, -8.3508e-01, -3.4818e-02,  1.0438e-01, -7.5591e-01,\n",
       "         6.5938e-01, -5.3708e-01,  5.0980e-01, -3.2773e-01, -6.5506e-01,\n",
       "         1.6577e-01,  3.3820e-02, -2.9562e-01, -2.9612e-01, -8.0326e-03,\n",
       "        -8.6030e-01,  3.1668e-01,  3.0156e-01, -6.3394e-01,  4.5458e-01,\n",
       "         2.1974e-01,  8.6086e-02, -1.6276e-01,  2.1337e-02,  4.2023e-01,\n",
       "         6.5097e-02,  4.2264e-01,  7.9846e-01,  4.8603e-01, -2.3522e-02,\n",
       "         1.0549e-01,  5.3977e-01,  6.4359e-01,  7.0084e-02, -4.2520e-01,\n",
       "         2.2186e-01, -2.1393e-01,  1.5227e-01,  2.5440e-01, -2.1148e-01,\n",
       "         1.0310e-01,  2.3431e-01,  3.0445e-01, -3.0521e-02, -7.8730e-02,\n",
       "        -6.3459e-01, -2.1583e-01,  6.7312e-02,  6.4451e-01, -3.4419e-01,\n",
       "        -7.9528e-01,  3.0382e-02, -2.1756e-01,  1.1763e-01,  5.1039e-01,\n",
       "         5.1146e-01, -2.0939e-03, -1.9649e-01,  2.7859e-01, -1.9698e-01,\n",
       "         2.7155e-01, -2.7695e-01, -2.9597e-01,  1.2256e-01, -1.5361e-01,\n",
       "        -4.7157e-02,  4.0023e-01,  1.4992e-01, -1.0179e-01,  3.0127e-01,\n",
       "         1.6107e-01,  3.5754e-02, -1.2301e-01,  4.2413e-02, -1.2642e-01,\n",
       "        -3.6995e-01, -7.8801e-02, -7.8517e-01, -3.7482e-02,  3.2583e-01,\n",
       "         7.6156e-01,  1.0411e+00,  1.7310e-02,  4.5859e-02,  5.8586e-01,\n",
       "        -6.4516e-01, -2.0998e-01,  1.3043e-01,  2.3979e-01, -4.5954e-01,\n",
       "        -8.7920e-01,  3.4430e-01, -4.0299e-01, -6.5247e-02,  1.0366e-01,\n",
       "        -4.6520e-01, -2.5796e-01,  1.9493e-01,  3.1664e-01, -2.4256e-01,\n",
       "        -2.8227e-01, -4.9451e-01,  3.1517e-01,  5.8128e-01, -4.8208e-01,\n",
       "        -2.1831e-01,  6.0240e-01, -2.4626e-01, -2.6363e-01,  1.2699e+00,\n",
       "         5.2684e-01, -6.6025e-02,  8.5864e-02, -7.1721e-01,  4.1262e-01,\n",
       "         1.5132e-01,  6.2638e-01,  2.8306e-01,  1.8370e-01,  4.9481e-01,\n",
       "         2.3896e-01, -2.7041e-01, -1.6859e-01,  1.0269e-01, -4.7028e-01,\n",
       "         8.2162e-02, -4.9081e-01, -6.6000e-02,  9.1927e-01, -6.3956e-02,\n",
       "         1.3326e-01,  2.8544e-01,  2.0207e-02,  4.1349e-01,  6.1697e-01,\n",
       "        -1.3040e-02,  1.2146e-01, -3.6975e-01, -8.8958e-02, -8.1528e-01,\n",
       "         1.8789e-01, -5.0190e-01,  3.1310e-01, -2.3573e-01, -1.4247e-01,\n",
       "         9.4499e-02,  1.5169e-03,  3.5691e-02,  2.4868e-01, -1.9822e-01,\n",
       "        -4.8452e-02, -1.5186e-01, -3.7370e-01, -1.2745e-01,  6.6482e-02,\n",
       "        -5.2278e-01,  3.5998e-01,  4.0082e-01,  5.2368e-01,  7.6837e-01,\n",
       "         5.4533e-01, -4.7344e-02,  3.8897e-01,  3.6514e-01, -1.0157e-01,\n",
       "         3.3486e-01, -3.8680e-01,  1.9579e-01, -1.5363e-01, -2.7546e-01,\n",
       "        -4.5021e-01,  6.9436e-01, -5.1970e-02,  8.1568e-02,  3.9183e-01,\n",
       "         8.0657e-02, -5.5117e-01, -2.3690e-01,  4.8057e-02,  2.4318e-01,\n",
       "        -1.0075e-01,  3.5948e-01,  2.6058e-01,  3.2202e-01, -1.6634e-01,\n",
       "         4.6644e-01,  9.2639e-02, -3.5983e-01, -4.0683e-01, -9.0705e-01,\n",
       "        -4.7917e-01, -4.5819e-01,  4.8972e-01, -7.1346e-01, -9.0199e-01,\n",
       "         4.8039e-02,  6.5959e-01, -7.6978e-02,  1.8480e-01,  3.3907e-01,\n",
       "         1.8276e-01,  1.1248e-01,  6.4155e-01, -8.5239e-01,  2.8149e-01,\n",
       "        -3.6389e-02,  8.8086e-01, -5.0560e-01, -2.1512e-01, -1.8702e-01,\n",
       "         3.5198e-02,  3.6312e-01,  2.1758e-01,  8.4411e-03,  1.1463e-01,\n",
       "         1.0579e-01, -1.5167e-01,  7.4874e-01,  6.9199e-01, -5.8130e-01,\n",
       "         2.6752e-01, -3.4532e-01, -1.0393e-01, -1.0234e+00,  4.2780e-01,\n",
       "         9.9189e-02,  2.0671e-01, -7.6644e-01, -3.6664e-02, -2.0854e-01,\n",
       "        -3.7313e-01,  4.9357e-01, -3.4273e-01,  2.8974e-01, -2.4993e-01,\n",
       "         8.5446e-02, -3.5421e-01,  5.5666e-01, -1.7711e-02,  8.0441e-01,\n",
       "         6.9600e-01,  4.8467e-02, -6.2661e-01,  4.6179e-02, -3.8510e-01,\n",
       "        -2.9747e-01,  1.0213e-01, -5.0111e-01,  8.0132e-01, -1.4758e-01,\n",
       "         9.5591e-02, -1.0356e+00,  1.3094e-01, -3.4282e-01,  1.0168e-02,\n",
       "        -8.8702e-01, -7.2518e-01,  6.9984e-01, -1.6563e-01,  3.7153e-01,\n",
       "         8.5097e-01,  1.6437e-01, -3.6039e-01,  8.5383e-02,  1.9285e-01,\n",
       "        -8.7150e-01, -4.5829e-02,  8.5521e-02, -2.1592e-01, -5.5468e-01,\n",
       "         6.7837e-02,  9.0732e-02,  1.7151e-01, -2.6269e-01,  4.5486e-01,\n",
       "         1.2166e-02,  5.0950e-01, -7.4438e-02, -1.8820e-01, -1.0376e-01,\n",
       "         1.6437e-01,  8.8022e-02,  4.5438e-01,  2.8811e-01, -6.0508e-01,\n",
       "        -3.6846e-01, -3.4014e-02,  5.4666e-01,  6.9379e-01, -6.8480e-01,\n",
       "        -1.9539e-01,  6.9781e-01,  6.4755e-01, -4.5852e-01, -6.3061e-02,\n",
       "         3.0760e-01,  3.3159e-01,  7.9327e-01, -2.6680e-01, -4.6377e-02,\n",
       "        -1.9911e-01,  1.8870e-01,  5.0042e-01,  2.9659e-01, -6.7607e-02,\n",
       "        -2.8964e-01, -1.9917e-01,  3.4836e-01, -6.1971e-01,  4.4208e-01,\n",
       "        -7.9688e-02, -2.0122e-02, -2.0628e-01,  1.2755e+00,  1.0319e-01,\n",
       "         4.8150e-01, -5.5765e-01, -4.0742e-01,  3.7812e-01, -3.5123e-01,\n",
       "        -2.8805e-01,  2.7789e-01, -5.5951e-01, -8.2929e-01,  4.4840e-01,\n",
       "        -1.0158e-01, -3.5282e-01, -2.9900e-01, -2.7180e-02, -4.3550e-01,\n",
       "         1.4717e-01, -1.8566e-01, -1.8572e-01, -2.3178e-01,  6.6502e-02,\n",
       "         1.5607e-01, -9.0215e-02, -6.8461e-02,  3.0547e-01,  1.5913e-01,\n",
       "        -1.6626e-01,  1.5902e-01,  3.8262e-01, -2.0447e-01, -2.7803e-02,\n",
       "         3.6980e-01,  7.5538e-01, -1.3526e-01,  1.9844e-02,  5.6756e-01,\n",
       "         6.0574e-01,  9.7036e-01, -5.9437e-01, -1.1204e+00, -2.3058e-01,\n",
       "         1.2720e-01, -6.4578e-02,  7.9169e-01, -8.9998e-01, -6.3506e-02])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1000,5).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9621)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9998)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(768,48)\n",
    "1/a.norm()*(a.numel()**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/wuyaozong/.conda/envs/fwdgrad/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32981/3543848651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/fwdgrad/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1270\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "model.layer[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fwdgrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
