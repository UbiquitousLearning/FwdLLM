{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "from decimal import *\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"20news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label_vocab', 'index_list', 'train_index_list', 'test_index_list'])\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/yahoo_data.h5\", \"r\") as f:\n",
    "    set_y = set()\n",
    "    # for i,key in enumerate(f[\"Y\"].keys()):\n",
    "    #     # print(f[\"Y\"][key][()])\n",
    "    #     set_y.add(f[\"Y\"][key][()])\n",
    "    #     if i > 10000:\n",
    "    #         break\n",
    "    # print(set_y)\n",
    "    # print(f.keys())\n",
    "    print(json.loads(f['attributes'][()]).keys())\n",
    "\n",
    "    # print((json.loads(f[\"attributes\"][()])[\"label_vocab\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/yahoo_data.h5\", \"w\") as f:\n",
    "    set_y = set()\n",
    "    # for i,key in enumerate(f[\"Y\"].keys()):\n",
    "    #     # print(f[\"Y\"][key][()])\n",
    "    #     set_y.add(f[\"Y\"][key][()])\n",
    "    #     if i > 10000:\n",
    "    #         break\n",
    "    # print(set_y)\n",
    "    # print(f.keys())\n",
    "    attributes = {\"label_vocab\": {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4, \"6\": 5, \"7\": 6, \"8\": 7, \"9\": 8, \"10\": 9}}\n",
    "    attributes['index_list'] = list(range(1460000))\n",
    "    attributes['train_index_list'] = list(range(1400000))\n",
    "    attributes['test_index_list'] = list(range(1400000,1460000))\n",
    "    f['attributes'] = json.dumps(attributes)\n",
    "    # print(f['attributes'][()])\n",
    "\n",
    "    # print((json.loads(f[\"attributes\"][()])[\"label_vocab\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'positive': 1}\n",
      "b\"I do n't think most of the people who loved the 1989 Paradiso will prefer this new version .\"\n",
      "b'negative'\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/sst_2_data.h5\", \"w\") as f:\n",
    "    set_y = set()\n",
    "    # for i,key in enumerate(f[\"Y\"].keys()):\n",
    "    #     # print(f[\"Y\"][key][()])\n",
    "    #     set_y.add(f[\"Y\"][key][()])\n",
    "    #     if i > 10000:\n",
    "    #         break\n",
    "    # print(set_y)\n",
    "    print(json.loads(f[\"attributes\"][()])[\"label_vocab\"])\n",
    "    print(f['X']['1000'][()])\n",
    "    print(f['Y']['1000'][()])\n",
    "    # print((json.loads(f[\"attributes\"][()])[\"label_vocab\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/semeval_2010_task8_data.h5\", \"r\") as f:\n",
    "    counter = Counter()\n",
    "    for key in f['X']:\n",
    "        counter[len(f['X'][key][()].decode(\"utf-8\").split(\" \"))] += 1\n",
    "print(max(counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contradiction', 'entailment', 'neutral'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import csv\n",
    "# dataset = \"mnli\"\n",
    "# vocab_set = set()\n",
    "# with h5py.File(\"/data/wyz/fednlp_data/data_files/mnli_data.h5\", \"w\") as wf:\n",
    "#     path=\"/data/wyz/pet_data/data/mnli/train.tsv\"\n",
    "#     with open(path, encoding='utf8') as f:\n",
    "#         for (idx, line) in enumerate(f.readlines()):\n",
    "#             if idx == 0:\n",
    "#                 continue\n",
    "#             line = line.split(\"\\t\")\n",
    "#             text_a = line[8].strip()\n",
    "#             text_b = line[9].strip()\n",
    "#             label = line[-1].strip()\n",
    "\n",
    "#             X = text_a+\"\\1\"+text_b\n",
    "\n",
    "#             X = X.encode(\"utf-8\")\n",
    "#             vocab_set.add(label)\n",
    "#             y = label.encode(\"utf-8\")\n",
    "            \n",
    "#             wf[f\"X/{idx}\"] = X\n",
    "#             wf[f\"Y/{idx}\"] = y\n",
    "#     path=\"/data/wyz/pet_data/data/mnli/dev_matched.tsv\"\n",
    "#     start_id = idx+1\n",
    "#     with open(path, encoding='utf8') as f:\n",
    "#         for (idx, line) in enumerate(f.readlines()):\n",
    "#             if idx == 0:\n",
    "#                 continue\n",
    "#             line = line.split(\"\\t\")\n",
    "#             text_a = line[8].strip()\n",
    "#             text_b = line[9].strip()\n",
    "#             label = line[-1].strip()\n",
    "\n",
    "#             X = text_a+\"\\1\"+text_b\n",
    "\n",
    "#             X = X.encode(\"utf-8\")\n",
    "#             vocab_set.add(label)\n",
    "#             y = label.encode(\"utf-8\")\n",
    "            \n",
    "#             wf[f\"X/{idx+start_id}\"] = X\n",
    "#             wf[f\"Y/{idx+start_id}\"] = y\n",
    "        \n",
    "# with h5py.File(f\"/data/wyz/fednlp_data/data_files/{dataset}_data.h5\", \"a\") as f:\n",
    "#     # del f[\"attributes\"]\n",
    "#     f[\"attributes\"] = json.dumps({\"label_vocab\":{label:i for i,label in enumerate(vocab_set)}})\n",
    "# print(len(vocab_set))\n",
    "# vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/yahoo_data.h5\", \"w\") as wf:\n",
    "    path=\"/data/wyz/pet_data/data/yahoo/train.csv\"\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for idx, row in enumerate(reader):\n",
    "            label, question_title, question_body, answer = row\n",
    "            text_a = ' '.join([question_title.replace('\\\\n', ' ').replace('\\\\', ' '),\n",
    "                                question_body.replace('\\\\n', ' ').replace('\\\\', ' ')])\n",
    "            text_b = answer.replace('\\\\n', ' ').replace('\\\\', ' ')\n",
    "\n",
    "            X = text_a+text_b\n",
    "\n",
    "            X = X.encode(\"utf-8\")\n",
    "            y = str(label).encode(\"utf-8\")\n",
    "            \n",
    "            wf[f\"X/{idx}\"] = X\n",
    "            wf[f\"Y/{idx}\"] = y\n",
    "    path=\"/data/wyz/pet_data/data/yahoo/test.csv\"\n",
    "    start_id = idx+1\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for idx, row in enumerate(reader):\n",
    "            label, question_title, question_body, answer = row\n",
    "            text_a = ' '.join([question_title.replace('\\\\n', ' ').replace('\\\\', ' '),\n",
    "                                question_body.replace('\\\\n', ' ').replace('\\\\', ' ')])\n",
    "            text_b = answer.replace('\\\\n', ' ').replace('\\\\', ' ')\n",
    "\n",
    "            X = text_a+text_b\n",
    "\n",
    "            X = X.encode(\"utf-8\")\n",
    "            y = str(label).encode(\"utf-8\")\n",
    "            \n",
    "            wf[f\"X/{idx+start_id}\"] = X\n",
    "            wf[f\"Y/{idx+start_id}\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train data:650000\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# with h5py.File(\"/data/wyz/fednlp_data/data_files/yelp-full_data.h5\", \"w\") as wf:\n",
    "#     path=\"/data/wyz/pet_data/data/yelp-full/train.csv\"\n",
    "#     with open(path, encoding='utf8') as f:\n",
    "#         reader = csv.reader(f, delimiter=',')\n",
    "#         for idx, row in enumerate(reader):\n",
    "#             label, body = row\n",
    "#             text_a = body.replace('\\\\n', ' ').replace('\\\\', ' ')\n",
    "\n",
    "#             X = text_a\n",
    "\n",
    "#             X = X.encode(\"utf-8\")\n",
    "#             y = str(label).encode(\"utf-8\")\n",
    "            \n",
    "#             wf[f\"X/{idx}\"] = X\n",
    "#             wf[f\"Y/{idx}\"] = y\n",
    "    \n",
    "#     print(f\"len train data:{idx+1}\")\n",
    "#     path=\"/data/wyz/pet_data/data/yelp-full/test.csv\"\n",
    "#     start_id = idx+1\n",
    "#     with open(path, encoding='utf8') as f:\n",
    "#         reader = csv.reader(f, delimiter=',')\n",
    "#         for idx, row in enumerate(reader):\n",
    "#             label, body = row\n",
    "#             text_a = body.replace('\\\\n', ' ').replace('\\\\', ' ')\n",
    "\n",
    "#             X = text_a\n",
    "\n",
    "#             X = X.encode(\"utf-8\")\n",
    "#             y = str(label).encode(\"utf-8\")\n",
    "            \n",
    "#             wf[f\"X/{idx+start_id}\"] = X\n",
    "#             wf[f\"Y/{idx+start_id}\"] = y\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train data:520000\n",
      "560000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/yelp-p_data.h5\", \"w\") as wf:\n",
    "    path=\"/data/wyz/pet_data/data/yelp-full/train.csv\"\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        data_id = 0\n",
    "        for idx, row in enumerate(reader):\n",
    "            label, body = row\n",
    "            text_a = body.replace('\\\\n', ' ').replace('\\\\', ' ')\n",
    "\n",
    "            if label == \"5\":\n",
    "                continue\n",
    "            X = text_a\n",
    "\n",
    "            if label == \"1\" or label == \"2\":\n",
    "                label = \"0\"\n",
    "            if label == \"3\" or label == \"4\":\n",
    "                label = \"1\"\n",
    "\n",
    "            X = X.encode(\"utf-8\")\n",
    "            y = str(label).encode(\"utf-8\")\n",
    "            \n",
    "            wf[f\"X/{data_id}\"] = X\n",
    "            wf[f\"Y/{data_id}\"] = y\n",
    "            data_id += 1\n",
    "    \n",
    "    print(f\"len train data:{data_id}\")\n",
    "    path=\"/data/wyz/pet_data/data/yelp-full/test.csv\"\n",
    "    # start_id = idx+1\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for idx, row in enumerate(reader):\n",
    "            label, body = row\n",
    "            text_a = body.replace('\\\\n', ' ').replace('\\\\', ' ')\n",
    "            if label == \"5\":\n",
    "                continue\n",
    "            X = text_a\n",
    "\n",
    "            if label == \"1\" or label == \"2\":\n",
    "                label = \"0\"\n",
    "            if label == \"3\" or label == \"4\":\n",
    "                label = \"1\"\n",
    "\n",
    "            X = X.encode(\"utf-8\")\n",
    "            y = str(label).encode(\"utf-8\")\n",
    "            \n",
    "            wf[f\"X/{data_id}\"] = X\n",
    "            wf[f\"Y/{data_id}\"] = y\n",
    "            data_id += 1\n",
    "        print(data_id)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392703"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/X\" (18846 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/data/wyz/fednlp_data/data_files/20news_data.h5\", \"r\") as f:\n",
    "    print(f[\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"yelp-p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/data/wyz/fednlp_data/partition_files/yelp-p_partition.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31083/858933052.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/data/wyz/fednlp_data/partition_files/{dataset}_partition.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# for key in f[\"uniform_client_1000\"]['partition_data'].keys():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fwdgrad/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fwdgrad/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/data/wyz/fednlp_data/partition_files/yelp-p_partition.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File(f\"/data/wyz/fednlp_data/partition_files/{dataset}_partition.h5\", \"r\") as f:\n",
    "    for key in f.keys():\n",
    "    # for key in f[\"uniform_client_1000\"]['partition_data'].keys():\n",
    "        print((key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "total = np.array([])\n",
    "total_test = np.array([])\n",
    "with h5py.File(f\"/data/wyz/fednlp_data/partition_files/{dataset}_partition.h5\", \"r\") as f:\n",
    "    for key in f[\"uniform\"]['partition_data'].keys():\n",
    "        total_test = np.append(total_test,f[\"uniform\"]['partition_data'][key][\"test\"][()])\n",
    "        total = np.append(total,f[\"uniform\"]['partition_data'][key][\"train\"][()])\n",
    "        print(len(f[\"uniform\"]['partition_data'][key][\"train\"][()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = list(map(str,range(0,520000)))\n",
    "total_test = list(map(str,range(520000,560000)))\n",
    "dataset = \"yelp-p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np.tile(total,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "client_num = 1000\n",
    "total = np.random.permutation(total).astype(np.int64)\n",
    "total_test = np.random.permutation(total_test).astype(np.int64)\n",
    "total = np.random.choice(total,(len(total)//client_num)*client_num)\n",
    "partition = np.array_split(total, client_num)\n",
    "partition_test = np.array_split(total_test, client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(len(partition[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yelp-p', 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset,client_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/uniform_client_1000\" (2 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(f\"/data/wyz/fednlp_data/partition_files/{dataset}_partition.h5\", \"a\") as f:\n",
    "    # del f[\"uniform_client_100/partition_data\"]\n",
    "    # del f[\"uniform_client_100/n_clients\"]\n",
    "    # del f[f\"uniform_client_{client_num}/n_clients\"]\n",
    "    f[f\"uniform_client_{client_num}/n_clients\"] = client_num\n",
    "    for i in range(client_num):\n",
    "        # del f[f\"uniform_client_{client_num}/partition_data/\"+str(i)+\"/train\"]\n",
    "        # del f[f\"uniform_client_{client_num}/partition_data/\"+str(i)+\"/test\"]\n",
    "        f[f\"uniform_client_{client_num}/partition_data/\"+str(i)+\"/train\"] = partition[i]\n",
    "        f[f\"uniform_client_{client_num}/partition_data/\"+str(i)+\"/test\"] = partition_test[i]\n",
    "    print(f[f'uniform_client_{client_num}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Couldn't delete link (callback link pointer is NULL (specified link may be '.' or not exist))\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/data/wyz/FedNLP/data/advanced_partition/test.ipynb 单元格 25\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250686f656e69783232227d/data/wyz/FedNLP/data/advanced_partition/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39myahoo\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250686f656e69783232227d/data/wyz/FedNLP/data/advanced_partition/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/data/wyz/fednlp_data/data_files/\u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m}\u001b[39;00m\u001b[39m_data.h5\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250686f656e69783232227d/data/wyz/FedNLP/data/advanced_partition/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mdel\u001b[39;00m f[\u001b[39m\"\u001b[39m\u001b[39mattributes\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250686f656e69783232227d/data/wyz/FedNLP/data/advanced_partition/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     f[\u001b[39m\"\u001b[39m\u001b[39mattributes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps({\u001b[39m\"\u001b[39m\u001b[39mlabel_vocab\u001b[39m\u001b[39m\"\u001b[39m:{\u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)}})\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/fwdgrad_py38/lib/python3.8/site-packages/h5py/_hl/group.py:489\u001b[0m, in \u001b[0;36mGroup.__delitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39m@with_phil\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__delitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    488\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Delete (unlink) an item from this group. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid\u001b[39m.\u001b[39;49munlink(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_e(name))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5g.pyx:309\u001b[0m, in \u001b[0;36mh5py.h5g.GroupID.unlink\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Couldn't delete link (callback link pointer is NULL (specified link may be '.' or not exist))\""
     ]
    }
   ],
   "source": [
    "dataset = \"yahoo\"\n",
    "with h5py.File(f\"/data/wyz/fednlp_data/data_files/{dataset}_data.h5\", \"a\") as f:\n",
    "    del f[\"attributes\"]\n",
    "    f[\"attributes\"] = json.dumps({\"label_vocab\":{str(i+1):i for i in range(10)}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/Y\" (1460000 members)>\n"
     ]
    }
   ],
   "source": [
    "dataset = \"yahoo\"\n",
    "with h5py.File(f\"/data/wyz/fednlp_data/data_files/{dataset}_data.h5\", \"r\") as f:\n",
    "    # print(f[\"attributes\"])\n",
    "    print(f['Y'])    # f[\"attributes\"] = json.dumps({\"label_vocab\":{str(i+1):i for i in range(10)}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fwdgrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a00582839fc15773adcf0865feedbb0f5307a5bd14640e7f55c7d88fed64ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
